{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763f0fa0-6584-44d4-b163-a891252769f9",
   "metadata": {},
   "source": [
    "# CS5228 Assignment 2 - DBSCAN, AGNES & Association Rule Mining\n",
    "\n",
    "Hello everyone, this assignment notebook covers DBSCAN, AGNES, Association Rule Mining (ARM). There are some code-completion tasks and question-answering tasks in this answer sheet. For code completion tasks, please write down your answer (i.e., your lines of code) between sentences that \"Your code starts here\" and \"Your code ends here\". The space between these two lines does not reflect the required or expected lines of code. For answers in plain text, you can refer to [this Markdown guide](https://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd) to customize the layout (although it shouldn't be needed).\n",
    "\n",
    "When you work on this notebook, you can insert additional code cells (e.g., for testing) or markdown cells (e.g., to keep track of your thoughts). However, before the submission, please remove all those additional cells again. Thanks!\n",
    "\n",
    "**Important:** \n",
    "* Rename and save this Jupyter notebook as **cs5228_a2_YourName_YourNUSNETID.ipynb** (e.g., **cs5228_a2_BobSmith_e12345678.ipynb**) before submission!\n",
    "* Rename and save the script file *cs5228_a2_script.py* as **cs5228_a2_YourName_YourNUSNETID.py** (e.g., **cs5228_a2_BobSmith_e12345678.py**) before submission!\n",
    "* Submission deadline is Sep 28, 11.59 pm. Late submissions will be penalized by 10% for each additional day. Failure to appropriately rename both files will yield a penalty of 1 Point. There is no need to use your full name if its a rather long; it's just  important to easily identify you in Canvas etc.\n",
    "\n",
    "Please also add your NUSNET and student id in the code cell below. This is just to make any identification of your notebook doubly sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "119a3b10-7afe-4814-9a79-32310842f1c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T03:46:01.447844700Z",
     "start_time": "2023-09-20T03:46:01.424661900Z"
    }
   },
   "outputs": [],
   "source": [
    "student_id = 'A0285647M'\n",
    "nusnet_id = 'e1216292'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#Auto reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T03:46:02.173164700Z",
     "start_time": "2023-09-20T03:46:02.110137100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "f7455d1c-31ad-4620-8d8a-ad2adc363f56",
   "metadata": {},
   "source": [
    "Here is an overview over the tasks to be solved and the points associated with each task. The notebook can appear very long and verbose, but note that a lot of parts provide additional explanations, documentation, or some discussion. The code and markdown cells you are supposed to complete are well, but you can use the overview below to double-check that you covered everything.\n",
    "\n",
    "* **1 DBSCAN (15 Points)**\n",
    "    * 1.1Implementing DBSCAN for Noise Detection (10 Points)\n",
    "        * 1.1 a) Compute Core Points (5 Points)\n",
    "        * 1.2 b) Compute Noise Points (5 Points)\n",
    "    * 1.2 Questions about DBSCAN (5 Points)\n",
    "        * 1.2 a) Basic Data Understanding with DBSCAN (3 Points)\n",
    "        * 1.2 b) Finding all Cluster (2 Points)\n",
    "* **2 AGNES (10 Points)**\n",
    "    * 2.1 Interpreting Dendrograms (6 Points)\n",
    "    * 2.2 Questions about AGNES (4 Points)\n",
    "        * 2.2 a) Picking the Right Linkage Method (2 Points)\n",
    "        * 2.2 b) Outlier Detection with AGNES(2 Points)\n",
    "* **3 Association Rule Mining (25 Points)**\n",
    "    * 3.1 Brute-Forcing Association Rule Mining (5 Points)\n",
    "    * 3.2 Implementing Apriori Algorithm for Finding Frequent Itemsets (10 Points)\n",
    "        * 3.2 a) Generating Candidate Itemsets $L_{k}$ (5 Points)\n",
    "        * 3.2 b) Generating Frequent Itemsets $F_{1}$, $F_{2}$, $F_{3}$, ... (5 Points)\n",
    "    * 3.3 ARM over Real-World Datasets -- Questions (10 Points)\n",
    "        * 3.3 a) (3 Points)\n",
    "        * 3.3 b) (3 Points)\n",
    "        * 3.3 c) (2 Points)\n",
    "        * 3.3 d) (2 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a4e6d6-30f8-4722-9750-4986c33fcccd",
   "metadata": {},
   "source": [
    "## Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ca2bb72-8aa3-4d53-9d2a-e4953aa02592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T02:26:17.818443Z",
     "start_time": "2023-09-21T02:26:14.928468Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from efficient_apriori import apriori\n",
    "import copy\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befecf0d-9ea3-4355-abb3-6cc7b1daa7ec",
   "metadata": {},
   "source": [
    "**Important:** This notebook also requires you to complete in a separate `.py` script file. This keeps this notebook cleaner and simplifies testing your implementations for us. As you need to rename the file `cs5228_a2.py`, you also need to edit the import statement below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c0827e5-fcc6-42d4-93df-96258a07cec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T02:26:18.607230500Z",
     "start_time": "2023-09-21T02:26:18.575532600Z"
    }
   },
   "outputs": [],
   "source": [
    "from cs5228_a2_ParasharaRamesh_e1216292 import *\n",
    "#from cs5228_a2_BobSmith_e12345678 import *  # <-- you will need to rename this accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e65ca2e-36ac-4d5e-aa5d-b5c5e39ba1a4",
   "metadata": {},
   "source": [
    "## 1 DBSCAN\n",
    "\n",
    "The Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm is a clustering algorithm used in data mining and machine learning. It is designed to identify clusters in data that may have arbitrary shapes and handle outliers effectively. Here's a more detailed explanation of how the DBSCAN algorithm works:\n",
    "\n",
    "* **Density-Based Clustering:** DBSCAN identifies clusters based on the density of data points in the feature space rather than assuming a specific cluster shape.\n",
    "\n",
    "* **Core Points:** The algorithm starts by selecting a random data point. It then checks how many data points are within a specified distance (epsilon or Îµ) from this point. If the number of data points within this distance exceeds a predefined threshold called MinPts (including the data point itself), the selected point is classified as a \"core point.\"\n",
    "\n",
    "* **Border Points:** Data points that are within the epsilon distance of a core point but do not meet the MinPts criterion are classified as \"border points.\" Border points can be part of a cluster but are located on the edges of the cluster.\n",
    "\n",
    "* **Noise Points:** Data points that are neither core points nor border points are classified as \"noise points\" or outliers. These are data points that do not belong to any cluster.\n",
    "\n",
    "* **Cluster Expansion:** The algorithm iteratively expands clusters by connecting core points and their density-reachable neighbors. Two core points are considered \"density-reachable\" if there is a path of core points connecting them within the epsilon distance. This process continues until no more core points can be added to a cluster.\n",
    "\n",
    "* **Result:** The output of DBSCAN is a set of clusters, each consisting of core points and potentially including border points. Noise points are not assigned to any cluster.\n",
    "\n",
    "DBSCAN is a valuable algorithm for clustering spatial data when the characteristics of the clusters are not well-known in advance and is commonly used in various applications, including geospatial analysis, image processing, and anomaly detection. In this section, we look into DBSCAN to implement a simplified variant instead of the \"full\" algorithm. For this, let's first load some random data for visualization and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5be27ac-6c0c-4cc8-af75-85c5e90d724b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T03:46:10.371458400Z",
     "start_time": "2023-09-20T03:46:10.308410100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_dbscan_toy is (70, 2)\n"
     ]
    }
   ],
   "source": [
    "X_dbscan_toy = pd.read_csv('data/a2-dbscan-toy-dataset.txt', header=None, sep=' ').to_numpy()\n",
    "\n",
    "print('The shape of X_dbscan_toy is {}'.format(X_dbscan_toy.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d77d1-5f26-4171-a28b-bc11c9a2b637",
   "metadata": {},
   "source": [
    "Now we can run scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) on this dataset. Here we use `eps=0.1` and `min_samples=10` as values for the two main input parameters for DBSCAN that specify the minimum \"density\" of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a95c8274-10f4-4847-8873-d1fb745e387e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T03:46:20.906753900Z",
     "start_time": "2023-09-20T03:46:20.847861600Z"
    }
   },
   "outputs": [],
   "source": [
    "dbscan_clustering = DBSCAN(eps=0.1, min_samples=11).fit(X_dbscan_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e8e42-5e6b-4380-a6c4-e2a0dfe83d2e",
   "metadata": {},
   "source": [
    "The points that are noise points are labeled with `-1`, while all points belonging to clusters are labeled with `0`, `1`, `2`, etc. So we can easily find the indices of all the points labeled as noise as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d35e2a8-c626-4682-a602-86255d06fd84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T03:46:23.491795Z",
     "start_time": "2023-09-20T03:46:23.428770500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The indices of the points labeled as noise are: [ 0  4 27 31 33 38 39 43 46 51 65]\n"
     ]
    }
   ],
   "source": [
    "cluster_point_indices = np.argwhere(dbscan_clustering.labels_ >= 0).squeeze()\n",
    "noise_point_indices = np.argwhere(dbscan_clustering.labels_ < 0).squeeze()\n",
    "\n",
    "print('The indices of the points labeled as noise are: {}'.format(noise_point_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f3cd41-69a7-421a-bd7b-e5d461f98251",
   "metadata": {},
   "source": [
    "Of course, we can also plot the results. Note that the figure below only highlights the points labeled as noise as red triangles; all points belonging to *some* clusters are in grey points (note that we do not care to which exact cluster these points belong)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a22421c9-7321-4cf8-8170-1fc5b8f0e471",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T03:46:26.694091400Z",
     "start_time": "2023-09-20T03:46:26.494691400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_dbscan_toy[cluster_point_indices,0], X_dbscan_toy[cluster_point_indices,1], c='grey')\n",
    "plt.scatter(X_dbscan_toy[noise_point_indices,0], X_dbscan_toy[noise_point_indices,1], c='red', marker='^', s=75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ea655-115e-4eb6-bfd9-bdf320a890e6",
   "metadata": {},
   "source": [
    "Summing up, the red dots in the plots we define as noise or outliers as they are very dissimilar to the other data points. In practice, we would likely remove those noise points, treat them separately, or maybe perform additional preprocessing steps to potentially \"denoise\" the dataset. However, the steps of choice generally depend heavily on the exact data mining task. Here, we focus on the identification of noise points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d510dba9-f28d-4ca9-ab74-14a84cf99378",
   "metadata": {},
   "source": [
    "### 1.1 Implementing DBSCAN for Noise Detection (10 Points)\n",
    "\n",
    "In the lecture, we covered the original algorithm of DBSCAN, which you can also find on [Wikipedia](https://en.wikipedia.org/wiki/DBSCAN). While not difficult to implement, it takes quite a couple of lines of codes to do so. For this assignment, however, we are only interested in the points of a dataset that DBSCAN considers noise (as illustrated above; the red dots in the previous plot). This includes that we do not have to care about\n",
    "\n",
    "* how many clusters there are (the plot above hints at 3 clusters but it does not matter) *and*\n",
    "* which non-noise points (the grey dots in the plot above) belong to which cluster\n",
    "\n",
    "**Your task is to implement a modified/simplified version of DBSCAN to find all noise points in a dataset!** The skeleton of method `get_noise_dbscan()` you need to complete is found in the file `cs5228_a2.py` (before the appropriate renaming). The method takes data matrix `X` as well as the two basic parameters `eps` and `min_samples` as input parameters; we use the same naming as scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html).  The output should be 2 lists of indices: (a) one containing the indices of all *core points* and (b) on containing the indices of all *noise points* in input dataset X.\n",
    "\n",
    "**Important:**\n",
    "* We only split this task into 2.1 a) and 2.1 b) to have intermediate results you can check for correctness (and potentially to better allow for partial marking). Our reference solutions first finds all core points and uses this information to find all noise points; hence the 2 separate code blocks for you to complete.\n",
    "* However, if you have a better/faster/shorter/cooler/etc. solution, you are more than welcome to implement it and ignore the intermediate result of finding all core points. Only the result from 2.1 b) is important. This also means that you can ignore 2.1 a) and still get full marks if you correctly identify all noise points.\n",
    "* If you have an alternative solution, please make sure that the method still returns the 2 output parameters `(core_point_indices, noise_point_indices)`. If you do not need to explicitly identify the core points, you can simply return `None` for `core_point_indices`.\n",
    "* You can import any method `numpy`, `scipy`, `sklearn`, or `pandas` has to offer -- except for any ready-made implementation of DBSCAN, of course :). Please add any imports to the code cell at the top with the other imports. Hint: We already imported [`sklearn.metrics.pairwise.euclidean_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html) for you.\n",
    "\n",
    "We will benchmark your implementation as part of our Little Competitions to see whose solution is the fastest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860dcf0d-5d3e-4261-ba18-48dc4b80d2d7",
   "metadata": {},
   "source": [
    "#### 1.1 a) Compute Core Points (5 Points)\n",
    "\n",
    "As mentioned above, our reference solution first computes all core points. If you follow this approach, complete the respective part in the code of method `get_noise_dbscan()`. Some hints:\n",
    "* Recall that we do not care to which cluster a core point a data sample belongs, only that is a core point in *some* cluster\n",
    "* Have a look at method [`sklearn.metrics.pairwise.euclidean_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html); it might make your life easier.\n",
    "\n",
    "**Important:** Avoid using loops in the parts of the code you have to complete. If you use loops but the results are correct, there will be some minor deduction of points. Once you know what you need to do, it is almost guaranteed that `numpy`, `scipy`, or `sklearn` will provide a useful method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of core points: 50\n",
      "\n",
      "The first 25 indices of the points labeled as core points:\n",
      "[1, 3, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 28, 29, 34]\n",
      "The indices of the points labeled as core points lesser than 25:\n",
      "[1, 3, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24]\n"
     ]
    }
   ],
   "source": [
    "my_core_point_indices, _ = get_noise_dbscan(X_dbscan_toy, eps=0.1, min_samples=10)\n",
    "\n",
    "print('Total number of core points: {}\\n'.format(len(my_core_point_indices)))\n",
    "print('The first 25 indices of the points labeled as core points:\\n{}'.format(sorted(my_core_point_indices)[:25]))\n",
    "\n",
    "#NOTE: Added this!\n",
    "print('The indices of the points labeled as core points lesser than 25:\\n{}'.format(list(filter(lambda x: x<25,sorted(my_core_point_indices)))))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T03:51:16.962911800Z",
     "start_time": "2023-09-20T03:51:16.899769100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**NOTE: In the cell below it is mentioned that the first 25 indices are provided but there are only 20 points shown, in the above code it shows the first 25 points by slicing it.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "f4555514-9b0f-48f8-89d7-a7472da31715",
   "metadata": {},
   "source": [
    "The output of previous code cell should look like:\n",
    "    \n",
    "```\n",
    "Total number of core points: 50\n",
    "\n",
    "The first 25 indices of the points labeled as core points:\n",
    "[1, 3, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24]\n",
    "```\n",
    "\n",
    "Note that `0`, `4`, and `27` are missing from this list since [`sklearn.cluster.DBSCAN`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) told us that these points are noise. Of course, also the border points are missing here, but [`sklearn.cluster.DBSCAN`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) does not return those explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0410e2-e9e4-4e8c-8583-7a35ce2c4e43",
   "metadata": {},
   "source": [
    "#### 1.1 b) Compute Noise Points (5 Points)\n",
    "\n",
    "Knowing the core points is useful but only an intermediate step. Now it is time to complete the method `get_noise_dbscan()` to compute the indices of all noise points in `X`. Again, our reference solution uses `core_point_indices` to accomplish this. If your implementation does not require the information about core points but returns the correct `noise_point_indices` then this is perfectly fine!\n",
    "\n",
    "**Important:** Avoid using loops in the parts of the code you have to complete. If you use loops but the results are correct, there will be some minor deduction of points. Once you know what you need to do, it is almost guaranteed that `numpy`, `scipy`, or `sklearn` will provide a useful method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6ab1310-b286-479b-af36-57397a7e0afe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T03:51:21.455150300Z",
     "start_time": "2023-09-20T03:51:21.393060500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of noise points: 10\n",
      "\n",
      "The indices of all points labeled as noise points:\n",
      "[0, 4, 27, 31, 33, 39, 43, 46, 51, 65]\n"
     ]
    }
   ],
   "source": [
    "_, my_noise_point_indices = get_noise_dbscan(X_dbscan_toy, eps=0.1, min_samples=10)\n",
    "\n",
    "print('Total number of noise points: {}\\n'.format(len(my_noise_point_indices)))\n",
    "print('The indices of all points labeled as noise points:\\n{}'.format(sorted(my_noise_point_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696b674a-ed95-47dd-a8ce-fa9a0bb366f8",
   "metadata": {},
   "source": [
    "The output of previous code cell should look like:\n",
    "\n",
    "```\n",
    "Total number of noise points: 10\n",
    "\n",
    "The indices of all points labeled as noise points:\n",
    "[0, 4, 27, 31, 33, 39, 43, 46, 51, 65]\n",
    "```\n",
    "\n",
    "Since we used the same values for `eps` and `min_samples`, this result matches the output we saw earlier when we used scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) over the toy dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9b11df-e56e-4c67-b25d-7e02e03d9a40",
   "metadata": {},
   "source": [
    "### 1.2 Questions about DBSCAN\n",
    "\n",
    "**1.2 a) Basic Data Understanding with DBSCAN (3 Points)**\n",
    "\n",
    "Assume you have a dataset `X`, run DBSCAN, and get a clustering that contains a set of clusters and some noise points (there's no need to be more precise; it's only important that you don't get just noise). Let's also assume you create a new dataset `X_new` simply by shuffling `X`; no other changes. Now you run DBSCAN with the same parameters as before over `X_new` and get a different clustering, i.e., most of the clusters are not exactly the same as before.\n",
    "\n",
    "**Describe what this information tells about the dataset and clustering!** This may include a brief discussion how changing the parameters of DBSCAN will likely affect the results.\n",
    "\n",
    "**Your Answer:**\n",
    "- Technically speaking even after shuffling we still have the same set of points so plotting each point would still give us the same spatial domain of points.\n",
    "- The only difference however is that DBSCAN always gives a deterministic answer for what are the core points and what are the noise points, however each run of DBSCAN can potentially give us different border points\n",
    "- The reason for this non-determinism of the border points is because the algorithm itself starts in a random manner classifies everything as noise until it finds the first core point , then it tries to build around that to classify all other points in that neighbourhood as subsequent core and border points.\n",
    "- Since the order of finding a core point is random; The \"exploration\" from each randomly selected core point can give rise to some specific set of core points and border points suited as per that choice and whether other core points from other clusters were already found before that\n",
    "- So if the order is different (while the parameters remain the same) the border points would also be different.\n",
    "- Since the border points could be different on each run of DBSCAN, the cluster it traces out could also technically be different thus yielding different clusters in X_new when compared with X even with the same values of the parameters.\n",
    "- That said, if the parameters of epsilon and min_neighbours are different, then the clustering result itself will be completely different as the very definition of core, noise and border points depends on the no of points available in its epsilon neighbourhood and the threshold of the min_neighbours used to classify it as a specific type of point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508f6a67-1e79-4d30-828e-6bd78ad5d7a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f201ef0-2ed3-47e8-8313-bf04aebdca9d",
   "metadata": {},
   "source": [
    "**1.2 b) Finding all Cluster (2 Points)**\n",
    "\n",
    "Your method `get_noise_dbscan()` finds all the noise points in a dataset according to the definition of DBSCAN. Now let's assume you now want to find all clusters, i.e., the number of clusters and which non-noise point belongs to which cluster.\n",
    "\n",
    "**2.2a) Describe how the result of `get_noise_dbscan()` may help you to find all clusters! (2 Points)** There is no need to implement anything; just a brief description how clusters can be found by already knowing all noise points.\n",
    "\n",
    "**Your Answer:**\n",
    "* Using the set of core points we can essentially do something like a DFS to trace out the entire chain of all core points starting from this particular core point. Each such connected component can be assigned to a specific cluster id\n",
    "* Once we have assigned all the core points to specific clusters by finding all the separate connected components, we can also start a similar process by finding the linkage of all the border points in a particular cluster ( i.e pick one border point around any core point, and use DFS again with that starting point to find out the chain of border points around core points belonging to cluster with id say 'x'. Once we find that linkage/connected component we can assign all of those border points also to the same cluster id.)\n",
    "* This way we will have different clusters where each of the core and border point is assigned to a specific cluster id.\n",
    "* All the identified noise points are going to be ignored as noise points technically don't belong to any cluster by definition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b6f28-66cc-432b-b602-28f7b8a987a9",
   "metadata": {},
   "source": [
    "## 2 AGNES\n",
    "\n",
    "AGNES, which stands for Agglomerative Nesting, is an agglomerative hierarchical clustering algorithm used in data analysis and machine learning. It is a bottom-up approach to clustering, meaning it starts with individual data points as separate clusters and iteratively merges them into larger clusters. Here's an overview of how the AGNES clustering algorithm works:\n",
    "\n",
    "* **Initialization:** Initially, each data point is treated as a single cluster, resulting in as many clusters as there are data points.\n",
    "\n",
    "* **Agglomerative Process:** AGNES proceeds by iteratively merging the two closest clusters into a single larger cluster. The distance between clusters is typically determined by a linkage criterion, which can be one of the following commonly used methods: *Single Linkage*, *Complete Linkage*, *Average Linkage*, and more.\n",
    "\n",
    "* **Hierarchy Building:** As clusters are merged, a hierarchy or dendrogram is built to represent the sequence of cluster mergers. This dendrogram can be used to visualize the hierarchical structure of the data and choose the desired number of clusters later.\n",
    "\n",
    "* **Stopping Criterion:** AGNES continues merging clusters until a stopping criterion is met. This criterion could be based on a predetermined number of clusters, a threshold distance, or other criteria. By default, AGNES stops when all data points form a single cluster.\n",
    "\n",
    "* **Result:** The output of AGNES is a hierarchical tree-like structure called a dendrogram, which can be cut at a specific level to obtain a partition of the data into clusters. The choice of where to cut the dendrogram depends on the desired number of clusters.\n",
    "\n",
    "AGNES is a valuable tool for hierarchical clustering and can be useful in various data analysis tasks, including taxonomy construction, gene expression analysis, and image segmentation.\n",
    "\n",
    "### 2.1 Interpreting Dendrograms\n",
    "\n",
    "We saw in the lecture that dendrograms are a meaningful way to visualize the hierarchical relationships between the data points with respect to the clustering using AGNES (or any other hierarchical clustering technique). Properly interpreting is important to get a correct understanding of the underlying data.\n",
    "\n",
    "Below are the plots of 6 different datasets labeled A-F. Each dataset contains 30 data points, each with two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ae180-bb68-41d1-b6d9-9bdb6b955e3f",
   "metadata": {},
   "source": [
    "<img src=\"data/a2-agnes-data-labeled.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8274c5-6643-4365-81fd-127fc7f3144e",
   "metadata": {},
   "source": [
    "Below are 6 dendrograms labeled 1-6. These dendrograms show the clustering using **AGNES with Single Linkage** for the 6 dataset above, but in a random order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f27422f-5c85-4046-9cb9-791ed5e106f5",
   "metadata": {},
   "source": [
    "<img src=\"data/a2-agnes-dendrogram-labeled.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f21fe-47cb-4ef1-8a01-c185ca0ecbcf",
   "metadata": {},
   "source": [
    "**Find the correct combinations of datasets and dendrograms** -- that is, find for each dataset the dendrogram that visualizes the clustering using AGNES with Single Linkage! Give a brief explanation for each decision; max 1-2 sentences! Complete the table below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f666ddb-5b31-45e0-8480-0fcc2986976c",
   "metadata": {},
   "source": [
    "| Dataset | Dendrogram | Brief Explanation |\n",
    "| ---  |------------|---------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **A**    | 6          | Here we can clearly see there is one blob and 2 outlier points. Therefore it follows that the 2 outlier points would be merged in the last two steps which corresponds to this particular dendogram|\n",
    "| **B**    | 1          | If we draw one line conected all points its like an outgrowing spiral starting from the dense region to the not so dense point in the bottom left. So each point can only be merged with one other point at each step which corresponds to this particular dendogram |\n",
    "| **C**    | 5          | Each point in the circumference of this cluster eventually becomess linked as 1 cluster as those points are all close to each other but the order of merging is slightly different due to the change in densities along the circumference. However the center of the cirle is the furthest away from each point on the circumference so that gets merged in the end. |\n",
    "| **D**    | 3          | Each of those half crescents form one cluster and then they are grouped together in the end. Because of the nature of the crescent each point is closer to another point so each point in the crescent gets paired up consecutively|\n",
    "| **E**    | 2          | All points here are randomly distributed so it follows that even the dendogram shows uniform merging at each level|\n",
    "| **F**    | 4          | There are naturally 3 clusters. In each cluster the points get merged together first and then each of those clusters are merged. This dendogram also has 3 groups which are then merged together in order |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bed46c-f6d9-4235-9434-79b85e023040",
   "metadata": {},
   "source": [
    "### 2.2 Questions about AGNES (4 Points)\n",
    "\n",
    "**2.2 a) Picking the Right Linkage Method (2 Points)**\n",
    "\n",
    "Assume your dataset contains the geolocations of traffic accidents on Singapore expressways over the time span of a year. Using AGNES, we want to find sections of the expressways where traffic accidents are particularly common.\n",
    "\n",
    "**Which Linkage Methods covered in the lecture is most suitable for this task?** Briefly explain your choice!\n",
    "\n",
    "\n",
    "**Your Answer:**\n",
    "- Single Linkage is more apt here as accidents which happen on roads typically occur closer together due to the nature of how the road curves/bends etc. So this way by merging together points which are very close to each other it can very clearly define which exact sections the accidents are most likely to happen.\n",
    "- However, if there are outlier datapoints it may consider those points as a part of the cluster as well as single linkage is highly sensitive to noise. Therefore, in such scenarios it's probably better to use other linkage methods using AGNES and then evaluate all of them together as those linkage methods are very robust when dealing with noise (e.g. Complete & Average)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b688c7a1-9f63-465b-b53c-d419ec2244cb",
   "metadata": {},
   "source": [
    "**2.2 b) Outlier Detection with AGNES (2 Points)**\n",
    "\n",
    "In contrast to DBSCAN, AGNES has no explicit concept of noise points for outlier detection. But an AGNES clustering still gives meaningful insights into a dataset.\n",
    "\n",
    "**How can the results of AGNES be used to identify outliers in a data?** Briefly explain your answer!\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "- Similar to question 2.1 , we can draw the dendograms using any linkage method and infer the outliers from that diagram as the outlier points would typically be singleton clusters which would be merged at the very end as other closer points/clusters gets merged together first.\n",
    "- Along similar lines we can consider a dendogram height threshold as the vertical axis represents distance between clusters. By setting a threshold at a specific height, we can say that points of clusters that merge at a height beyond this are considered as outliers.\n",
    "- Another method using the dendogram would be to analyze the number of nodes in a subtree. Outliers may be found in branches that have fewer data points or where clusters merge at later/higher levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c5071-c895-46e6-b8dd-031ccd6a462a",
   "metadata": {},
   "source": [
    "## 3 Association Rule Mining\n",
    "\n",
    "Association rule mining is a data mining technique used to discover interesting relationships, patterns, or associations in large datasets. It primarily focuses on identifying frequent patterns, often in the form of rules, within transactional or relational databases. This technique is commonly applied in various fields, including market basket analysis in retail, web usage mining, healthcare, and more. Here's how association rule mining works:\n",
    "\n",
    "* **Transaction Data:** Association rule mining typically starts with a dataset that represents transactions or events. Each transaction consists of a set of items, and these transactions are often stored in a database.\n",
    "\n",
    "* **Itemsets:** An itemset is a collection of one or more items or attributes from the dataset. For example, in a retail context, an itemset could be a list of products purchased together in a single transaction.\n",
    "\n",
    "* **Support:** Support is a measure of the frequency with which an itemset appears in the dataset. It is defined as the proportion of transactions that contain the itemset. High support indicates that the itemset is a common pattern in the data.\n",
    "\n",
    "* **Confidence:** Confidence is a measure of how often the rule is found to be true. It is defined as the proportion of transactions that contain both the antecedent and the consequent. High confidence suggests a strong association between the antecedent and consequent.\n",
    "\n",
    "* **Lift:** Lift is a measure of how much more likely the consequent is given the antecedent compared to its expected occurrence by chance. A lift value greater than 1 suggests a positive association, while a value less than 1 suggests a negative or weak association.\n",
    "\n",
    "* **Association Rules:** Association rules are typically expressed as \"if-then\" statements. They describe relationships between itemsets. Each rule consists of two parts:\n",
    "    * Antecedent (Left-hand side, LHS): This is the itemset that represents the condition or premise of the rule.\n",
    "    * Consequent (Right-hand side, RHS): This is the itemset that represents the result or conclusion of the rule.\n",
    "\n",
    "The association rule mining process involves finding itemsets that meet a minimum support threshold and then generating rules based on those itemsets. Rules are evaluated based on their confidence and lift, and interesting or meaningful rules are selected for further analysis or action. For example, in a retail context, association rule mining might reveal that customers who purchase items like bread and milk (antecedent) are highly likely to also purchase butter (consequent). This insight can be valuable for marketing and inventory management, as it suggests items that can be promoted together or placed in close proximity in the store. Association rule mining algorithms, such as the Apriori algorithm, are commonly used to efficiently discover and analyze these patterns in large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1929c4b3-489e-4001-87e5-c23cc78d7bb7",
   "metadata": {},
   "source": [
    "#### Toy Dataset\n",
    "\n",
    "The following dataset with 5 transactions and 6 different items is directly taken from the lecture slides. This should make it easier to test your implementation. The format is a list of tuples, where each tuple represents the set of items of an individual transaction. This format can also be used as input for the `efficient-apriori` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a82b32-535f-4df6-89d8-e49b5a6ee20f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T02:26:32.240600200Z",
     "start_time": "2023-09-21T02:26:32.224584900Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_demo = [\n",
    "    ('bread', 'yogurt'),\n",
    "    ('bread', 'milk', 'cereal', 'eggs'),\n",
    "    ('yogurt', 'milk', 'cereal', 'cheese'),\n",
    "    ('bread', 'yogurt', 'milk', 'cereal'),\n",
    "    ('bread', 'yogurt', 'milk', 'cheese')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713910f-65d1-4da4-b049-f75301f4c706",
   "metadata": {},
   "source": [
    "#### Auxiliary Methods\n",
    "\n",
    "We want you to focus on the Apriori algorithm. So we provide you with a set of auxiliary functions. Feel free to look at their implementation in the file `data/utils.py`.\n",
    "\n",
    "Given a set of items, `powerset()` returns all possible subsets of items with a specified minimum and maximum length. For example, you can use this method to generate all itemsets for a transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b5dd054-7bdc-4746-94f0-e94941573fc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T02:26:34.633487400Z",
     "start_time": "2023-09-21T02:26:34.595850800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a',)\n",
      "('b',)\n",
      "('c',)\n",
      "('a', 'b')\n",
      "('a', 'c')\n",
      "('b', 'c')\n",
      "('a', 'b', 'c')\n"
     ]
    }
   ],
   "source": [
    "for subset in powerset(('c', 'b', 'a'), min_len=1, max_len=3):\n",
    "    print(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db4a4a-566a-45e7-ba79-6a478d9632f6",
   "metadata": {},
   "source": [
    "The method `unique_items()` returns all the unique items across all transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "794486a8-894c-47dd-a400-527bb3266d3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T02:26:36.524263100Z",
     "start_time": "2023-09-21T02:26:36.461790100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'bread', 'cereal', 'cheese', 'eggs', 'milk', 'yogurt'}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_items(transactions_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca351d-7126-4a3f-9ca7-34bd788420a9",
   "metadata": {},
   "source": [
    "The method `support()` calculates and returns the support for a given itemset and set of transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "368bf3ed-681c-4669-afa4-cf8737f56ab0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T02:26:39.178316200Z",
     "start_time": "2023-09-21T02:26:39.162659Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.6"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support(transactions_demo, ('bread', 'milk'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8e9eed-b589-46cf-936c-a826b49cdde2",
   "metadata": {},
   "source": [
    "The method `confidence()` calculates and returns the confidence for a given association rules and set of transactions. An association rule is represented by a 2-tuple, where the first element represents itemset X and the second element represents items Y (i.e., $X \\Rightarrow Y$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68d1082a-5224-43e4-bae2-b55aca6466ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T02:26:41.072063100Z",
     "start_time": "2023-09-21T02:26:41.042389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.75"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence(transactions_demo, (('bread',), ('milk',)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1925e37a-a56e-44db-824e-7da4ebb17b4b",
   "metadata": {},
   "source": [
    "The method `generate_association_rules()` calculates and returns all possible association rules given an itemset. The result is a list of association rules, each association rule represented as 2-tuple (see above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e37e7321-0b4a-4b98-a4c6-737c058c5951",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T02:26:43.067143800Z",
     "start_time": "2023-09-21T02:26:43.051497900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[(('bread',), ('cereal', 'milk')),\n (('cereal',), ('bread', 'milk')),\n (('milk',), ('bread', 'cereal')),\n (('bread', 'cereal'), ('milk',)),\n (('bread', 'milk'), ('cereal',)),\n (('cereal', 'milk'), ('bread',))]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_association_rules(('bread', 'milk', 'cereal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94851b47-5af7-4191-9d5c-10e70b27e55f",
   "metadata": {},
   "source": [
    "The method `merge_itemsets()` merges two given itemsets into one itemset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82104d31-cf24-4506-8d50-106f0acea107",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T02:26:44.777876200Z",
     "start_time": "2023-09-21T02:26:44.759825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('bread', 'eggs', 'milk')"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_itemsets(('bread', 'milk'), ('bread', 'eggs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9be2a59-8c36-4377-997d-e46392af6bdd",
   "metadata": {},
   "source": [
    "For your implementation, you can make use of these auxiliary methods wherever you see fit. And that is, of course, strongly recommended, as it makes the programming task much easier. So, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4e4166-a4ba-48f5-a59d-3e68ee37a77b",
   "metadata": {},
   "source": [
    "### 3.1 Brute-Forcing Association Rule Mining (5 Points)\n",
    "\n",
    "The most naive approach for mining association rules would be to generate all possible rules and check if their support and confidence exceed the specified thresholds `minsup` and `minconf`. In the lecture, you have learned that, given $d$ unique items in a dataset of transactions, there are $3^d - 2^{d+1} + 1$ possible rules.\n",
    "\n",
    "**Proof that $d$ unique items result in $3^d - 2^{d+1} + 1$ possible rules!** (Hint: Write out all possible rules for $d = 2, 3, 4, ...$ items; you should quickly spot the pattern that will allow you to validate the formula).\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "**This was covered in the tutorials, so as mentioned by Prof I am attempting the two extra questions in the \"cs5228_a2_extra_ParasharaRamesh_e1216292.ipynb\" instead!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8872fc7-ce0c-4e3c-8414-f53e203c1d9a",
   "metadata": {},
   "source": [
    "### 3.2 Implementing Apriori Algorithm for Finding Frequent Itemsets (10 Points)\n",
    "\n",
    "The underlying goal of the Apriori algorithm is to avoid calculating the support of itemsets for which we can say ahead of time (i.e., \"a-priori\") that they cannot be frequent itemsets. Calculating the support is the most expensive part as it requires going through all transactions in our database.\n",
    "\n",
    "In the following task, you will implement the Apriori algorithm for finding Frequent Itemsets. In more detail, you will implement the $F_{k-1}\\times F_1$ method to compute the $L_k$ candidate itemsets given the $F_{k-1}$ frequent itemsets and the $F_1$ frequent itemsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f8ffd-057b-4954-9905-4a42cb1ffce1",
   "metadata": {},
   "source": [
    "#### 3.2 a) Generating Candidate Itemsets $L_{k}$ (5 Points)\n",
    "\n",
    "The $F_{k-1}\\times F_1$ method used the frequent itemset sets of size ($k-1$) and of size $1$ to derive the set $L_k$ of candidate itemsets. The important part is the pruning, e.g., the removal of all k-itemset for which we can already tell that they won't have sufficient support.\n",
    "\n",
    "**Hint:** Instead of first creating all possible k-itemsets and then performing the pruning to yield the final set $L_k$, check each k-itemset immediately in the loop if it should indeed be added to $L_k$ or not. This will make your code a bit simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bedf1692-a3d5-48ba-b628-3e76afc19fca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T02:27:05.850464900Z",
     "start_time": "2023-09-21T02:27:05.834001700Z"
    }
   },
   "outputs": [],
   "source": [
    "#NOTE: Didn't use any of the existing helper functions\n",
    "def generate_Lk(k_minus_one_itemsets, one_itemsets):\n",
    "    # Initialize as set to avoid duplicates\n",
    "    Lk = set()\n",
    "\n",
    "    #NOTE: Typecasting each itemset to a list of sets instead as we can use the \"in\" operator to search in a list of sets as opposed to a set of tuples with any ordering of the set we are searching for\n",
    "    k_minus_one_itemsets = list(map(lambda item_tuple: set(item_tuple), k_minus_one_itemsets))\n",
    "    one_itemsets = list(map(lambda item_tuple: set(item_tuple), one_itemsets))\n",
    "\n",
    "    # Nested loop over both itemsets to find all combinations\n",
    "    for k_minus_one_itemset in k_minus_one_itemsets:\n",
    "        for one_itemset in one_itemsets:\n",
    "\n",
    "            ##########################################################################################\n",
    "            ### Your code starts here ###############################################################\n",
    "\n",
    "            #1. converting it into a list as each itemset is currently a set ( this is only so that I can do step #3)\n",
    "            k_minus_one_itemset = list(k_minus_one_itemset)\n",
    "            one_itemset = list(one_itemset)\n",
    "\n",
    "            #2. make a candidate lk which can be potentially added to Lk\n",
    "            #NOTE. only a frozen set can be added to a set, as only a frozenset is hashable!\n",
    "            candidate_lk = frozenset(k_minus_one_itemset + one_itemset)\n",
    "\n",
    "            if len(candidate_lk) == len(k_minus_one_itemset) + 1:\n",
    "                #2.a continue only if the set you formed actually has k elements in it\n",
    "                can_add_candidate_to_lk = True\n",
    "\n",
    "                for i in range(len(k_minus_one_itemset)):\n",
    "                    #3. consider each k-1 combo\n",
    "                    possible_k_minus_one_combo_to_check = frozenset(\n",
    "                        k_minus_one_itemset[:i] + k_minus_one_itemset[i + 1:] + one_itemset\n",
    "                    )\n",
    "\n",
    "                    #4. check if each k-1 combo is already present in k-1 frequent itemsets\n",
    "                    #NOTE: in operator works only because k-1 frequent itemsets is a list of sets and searching\n",
    "                    if possible_k_minus_one_combo_to_check not in k_minus_one_itemsets:\n",
    "                        #5. if it is not present make the flag false , break and don't consider this candidate_lk itemset\n",
    "                        can_add_candidate_to_lk = False\n",
    "                        break\n",
    "\n",
    "                if can_add_candidate_to_lk:\n",
    "                    #6. add candidate only if all of its k-1 combos are present in the frequent itemset\n",
    "                    #NOTE. add works here because we are adding a frozenset which ensures that other combinations cant be added\n",
    "                    Lk.add(candidate_lk)\n",
    "\n",
    "    #7. typecast all frozensets back to tuple in a set comprehension\n",
    "    Lk = {tuple(lk) for lk in Lk}\n",
    "\n",
    "            ### Your code ends here #################################################################\n",
    "            #########################################################################################                  \n",
    "\n",
    "    return Lk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3c22fe-f6d6-47eb-a461-dce5f2f717fc",
   "metadata": {},
   "source": [
    "For testing your implementation, we manually crafted a k-itemset ($k=2$) and 1-itemset in line with the lecture example, meaning that the result should match the example on the lecture slides as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00b81039-8f0d-4be3-9e1e-5ec4e2e22e33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T02:27:08.135745500Z",
     "start_time": "2023-09-21T02:27:08.120836400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('milk', 'yogurt', 'cheese')\n",
      "('milk', 'bread', 'yogurt')\n",
      "('milk', 'cereal', 'yogurt')\n",
      "('milk', 'cereal', 'bread')\n",
      "('cereal', 'bread', 'yogurt')\n"
     ]
    }
   ],
   "source": [
    "k_minus_one_itemsets = {\n",
    "    ('bread', 'cereal'), ('bread', 'milk'), ('bread', 'yogurt'), ('cereal', 'milk'),\n",
    "    ('cereal', 'yogurt'), ('cheese', 'milk'), ('cheese', 'yogurt'), ('milk', 'yogurt')\n",
    "}\n",
    "\n",
    "one_itemsets = {('bread',), ('cereal',), ('cheese',), ('milk',), ('yogurt',)}\n",
    "\n",
    "Lk = generate_Lk(k_minus_one_itemsets, one_itemsets)\n",
    "\n",
    "for itemset in Lk:\n",
    "    print(itemset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6598c719-5bf9-45f5-96c2-4d06ef21348a",
   "metadata": {},
   "source": [
    "#### 3.2 b) Generating Frequent Itemsets $F_{1}$, $F_{2}$, $F_{3}$, ... (5 Points)\n",
    "\n",
    "Once we have a candidate set L of a given iteration, we can directly derive the frequent itemsets by removing all candidates with insufficient support. As this is a trivial filter step, we give you this method `generate_F` for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a893e64-a4cf-40cc-9ee8-07d01d1a2140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T02:27:11.556401500Z",
     "start_time": "2023-09-21T02:27:11.525222300Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_Fk(Lk, transactions, minsup):\n",
    "    return set([ s for s in Lk if support(transactions, s) >= minsup])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc25fe75-0963-43b4-b260-dc95fef9e6c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T02:27:12.278342900Z",
     "start_time": "2023-09-21T02:27:12.246965900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('milk', 'yogurt', 'cheese')\n",
      "('milk', 'bread', 'yogurt')\n",
      "('milk', 'cereal', 'yogurt')\n",
      "('milk', 'cereal', 'bread')\n"
     ]
    }
   ],
   "source": [
    "Fk = generate_Fk(Lk, transactions_demo, 0.4)\n",
    "\n",
    "for itemset in Fk:\n",
    "    print(itemset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad35a9a9-4538-4005-b9d4-c60ac949314b",
   "metadata": {},
   "source": [
    "**Note:** Just as a quick reminder, this filter step is actually the most costly part of the algorithms since we have to calculate the support for each candidate itemset which requires us to go through each transaction. For our toy dataset, this doesn't really matter, but the whole goal of the Apriori algorithm is to minimize the number of access to the datasets.\n",
    "\n",
    "With the 2 methods `generate_L_kplus1` and `generate_F`, you now have everything to find all frequent itemsets using the Apriori algorithm. To this end, complete the method `find_frequent_itemsets_apriori` below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1d58578-4ca6-4497-9d47-eaecaafc94fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T02:37:20.788482300Z",
     "start_time": "2023-09-21T02:37:20.757091100Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_frequent_itemsets_apriori(transactions, minsup):\n",
    "    # The 1-itemsets are just all unique items across all transactions\n",
    "    # NOTE: Renaming this variable for my readability\n",
    "    all_one_itemsets = unique_items(transactions)\n",
    "\n",
    "    ############################################################################################\n",
    "    ### Your code starts here ##################################################################\n",
    "\n",
    "    # Initialize dictionary with all current frequent itemsets for each size k\n",
    "    # Example format: { 1: {(a), (b), (c)}, 2: {(a, c), ...} }\n",
    "\n",
    "    # initializing with F1\n",
    "    all_one_itemsets = {(item,) for item in all_one_itemsets} #making it a set of tuples so that it can be fed into the next function!\n",
    "    Fk = generate_Fk(all_one_itemsets, transactions, minsup)\n",
    "    F = {1: copy.deepcopy(Fk)}\n",
    "\n",
    "    ### Your code ends here ####################################################################\n",
    "    ############################################################################################\n",
    "\n",
    "    # NOTE: changed the for loop upper bound to len(all_one_itemsets) instead of len(one_itemsets) so that there are no side effects when using the function!\n",
    "    for k in range(2, len(all_one_itemsets) + 1):\n",
    "        # NOTE: Commenting this out as I just use the previous iteration's Fk as the new Fk for this iteration\n",
    "        # Fk = set()\n",
    "\n",
    "        ########################################################################################\n",
    "        ### Your code starts here ##############################################################\n",
    "\n",
    "        Lk = generate_Lk(Fk, F[1])\n",
    "        Fk = generate_Fk(Lk, transactions, minsup)\n",
    "\n",
    "        if len(Fk) == 0:\n",
    "            # we can stop here\n",
    "            break\n",
    "        ### Your code ends here ################################################################\n",
    "        ########################################################################################\n",
    "\n",
    "        F[k] = Fk\n",
    "\n",
    "    #NOTE: printing F as that is easier for debugging\n",
    "    # print(F)\n",
    "\n",
    "    # Merge the dictionary of k-itemsets to a single set and return it\n",
    "    # Example: {1: {(a), (b), (c)}, 2: (a, c)} => {(a), (b), (c), (a, c)}\n",
    "    return set.union(*[itemsets for k, itemsets in F.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e6af7f-5e82-472b-91d8-f3bc08496589",
   "metadata": {},
   "source": [
    "Let's test your implementation using the code cell below. Of course, the result should match the brute force approach (assuming the same value for input parameter `minsup`; default: `0.4` to match the example in the lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "890706d3-048a-43a0-bda1-cc234890be6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T02:35:59.406395900Z",
     "start_time": "2023-09-21T02:35:59.390883800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {('yogurt',), ('cheese',), ('bread',), ('cereal',), ('milk',)}, 2: {('milk', 'yogurt'), ('milk', 'cheese'), ('bread', 'yogurt'), ('milk', 'bread'), ('yogurt', 'cheese'), ('cereal', 'yogurt'), ('milk', 'cereal'), ('cereal', 'bread')}, 3: {('milk', 'yogurt', 'cheese'), ('milk', 'bread', 'yogurt'), ('milk', 'cereal', 'yogurt'), ('milk', 'cereal', 'bread')}}\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Itemsets are\n",
      "('milk', 'yogurt', 'cheese')\n",
      "('milk', 'bread', 'yogurt')\n",
      "('milk', 'cereal', 'yogurt')\n",
      "('yogurt',)\n",
      "('cheese',)\n",
      "('milk', 'yogurt')\n",
      "('milk', 'cheese')\n",
      "('milk', 'cereal', 'bread')\n",
      "('bread', 'yogurt')\n",
      "('bread',)\n",
      "('milk', 'bread')\n",
      "('yogurt', 'cheese')\n",
      "('cereal', 'yogurt')\n",
      "('cereal',)\n",
      "('milk', 'cereal')\n",
      "('milk',)\n",
      "('cereal', 'bread')\n"
     ]
    }
   ],
   "source": [
    "F = find_frequent_itemsets_apriori(transactions_demo, 0.4)\n",
    "\n",
    "print(\"-\"*150)\n",
    "print(\"Itemsets are\")\n",
    "for itemset in F:\n",
    "    print(itemset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b31eb5-ae84-41b7-ba5c-daac2d9d7a0d",
   "metadata": {},
   "source": [
    "#### From Frequent Itemsets to Association Rules\n",
    "\n",
    "Your implementation so far gives you the Frequent Itemsets in a list of transactions using the Apriori method. This step is typically the most time-consuming one in Association Rule Mining. However, we still have to do the second step and find all Association Rules given the Frequent Itemsets. We saw in the lecture that this can also be done in an efficient manner using the Apriori method to avoid checking all rules.\n",
    "\n",
    "Since this step is typically less computationally expensive, we simply do it the naive way -- that is, we go over all Frequent Itemsets, and check for Frequent Itemset and check which of the Association Rules that can be generated from it has a sufficiently high confidence. With all the auxiliary methods we provide, this becomes trivial to implement, so we simply give you the method `find_association_rules()` below. Note how it uses your implementation of `frequent_itemsets_apriori()`."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "394667f5-fedd-4800-9d78-a4390b062d2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T02:39:06.850308800Z",
     "start_time": "2023-09-21T02:39:06.819056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[(('cereal',), ('milk',))]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_association_rules(transactions, min_support, min_confidence):\n",
    "    # Initialize empty list of association rules\n",
    "    association_rules = []\n",
    "    \n",
    "    # Find and loop over all frequent itemsets\n",
    "    for itemset in find_frequent_itemsets_apriori(transactions, min_support):\n",
    "        if len(itemset) == 1:\n",
    "            continue\n",
    "\n",
    "        # Find and loop over all association rules that can be generated from the itemset\n",
    "        for r in generate_association_rules(itemset):\n",
    "            # Check if the association rule fulfils the confidence requriement\n",
    "            if confidence(transactions, r) >= min_confidence:\n",
    "                association_rules.append(r)\n",
    "                \n",
    "    # Return final list of association rules\n",
    "    return association_rules\n",
    "\n",
    "find_association_rules(transactions_demo, 0.6, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ba75c3-dfb0-4c3e-93b3-8ccdbd6c8b8c",
   "metadata": {},
   "source": [
    "If everything is correct, for the default values for `min_support` and `min_confidence`, the one Association Rules that should be returned is $\\{cereal\\}\\Rightarrow \\{milk\\}$ (in Python represented as a tuple of 2 tuples, left-hand side and right-hand side)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d69694-1260-4e26-8374-79cf830ec11a",
   "metadata": {},
   "source": [
    "#### Comparison with `efficient-apriori` package\n",
    "\n",
    "You can run the apriori algorithm over the demo data to check if your implementation is correct. Try different values for the parameters `min_support` and `min_confidence` and compare the results. Note that the order of the returned association rules might differ between your implementation and the apriori one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "428c6cd8-1d5d-4024-baed-24eeba3a2174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T02:40:16.527775Z",
     "start_time": "2023-09-21T02:40:16.511688800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itemsets are:\n",
      "{1: {('bread',): 4, ('yogurt',): 4, ('milk',): 4, ('cereal',): 3}, 2: {('bread', 'milk'): 3, ('bread', 'yogurt'): 3, ('cereal', 'milk'): 3, ('milk', 'yogurt'): 3}}\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Rules are:\n",
      "Rule [('cereal',) => ('milk',)] (support: 0.6, confidence: 1.0, lift: 1.25)\n"
     ]
    }
   ],
   "source": [
    "#NOTE: also capturing itemsets for comparision\n",
    "itemsets, rules = apriori(transactions_demo, min_support=0.6, min_confidence=1.0, max_length=4)\n",
    "\n",
    "#NOTE: printing itemsets as well\n",
    "print(\"Itemsets are:\")\n",
    "print(itemsets)\n",
    "print(\"-\"* 160)\n",
    "print(\"Rules are:\")\n",
    "for r in rules:\n",
    "    print('Rule [{} => {}] (support: {}, confidence: {}, lift: {})'.format(r.lhs, r.rhs, r.support, r.confidence, r.lift))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60528c4a-2237-49be-8e73-b6097f414761",
   "metadata": {},
   "source": [
    "### 3.3 Association Rule Mining over Real-World Datasets\n",
    "\n",
    "For the last task, we look into real-world datasets to find interesting Association Rules. As those datasets are already quite large -- although not really large -- we only rely only on the `efficient-apriori` package for performance reasons. To make things even easier for you, we provide the method `show_top_rules` which computes the Association Rules using the `efficient-apriori` package, but (a) filters the rules w.r.t. to the right-hand side (optional), (b) sorts the rules w.r.t. the specified metric, and (c) shows only the top-k rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fffaea-8098-4043-b2c1-14d8967ba785",
   "metadata": {},
   "source": [
    "#### Loading the Data\n",
    "\n",
    "For the first dataset, we consider a typical [Market Basket Analysis](https://www.kaggle.com/yugagrawal95/market-basket-analysis-apriori-in-python/data?select=Market_Basket_Optimisation.csv) dataset containing the set of items people have purchased in a supermarket. This dataset is very similar to the running example used throughout the lecture -- well, just larger :), with ~7,500 transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0a764b32-02a1-4708-8703-ef087e7cd8db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T02:46:00.321033500Z",
     "start_time": "2023-09-22T02:46:00.258468100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transactions overall: 7501\n",
      "\n",
      "Some example transactions:\n",
      "['shrimp', 'almonds', 'avocado', 'vegetables mix', 'green grapes', 'whole weat flour', 'yams', 'cottage cheese', 'energy drink', 'tomato juice', 'low fat yogurt', 'green tea', 'honey', 'salad', 'mineral water', 'salmon', 'antioxydant juice', 'frozen smoothie', 'spinach', 'olive oil']\n",
      "['burgers', 'meatballs', 'eggs']\n",
      "['chutney']\n",
      "['turkey', 'avocado']\n",
      "['mineral water', 'milk', 'energy bar', 'whole wheat rice', 'green tea']\n"
     ]
    }
   ],
   "source": [
    "transactions_groceries = []\n",
    "\n",
    "with open('data/a2-groceries.csv') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        transactions_groceries.append(line.split(','))\n",
    "\n",
    "print('Number of transactions overall: {}'.format(len(transactions_groceries)))\n",
    "print()\n",
    "print('Some example transactions:')\n",
    "for t in transactions_groceries[:5]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9956e-dcb7-4bb9-85ef-4b318f74924e",
   "metadata": {},
   "source": [
    "As the second dataset, use the [Coronavirus Disease 2019 (COVID-19) Clinical Data Repository](https://covidclinicaldata.org/) to find Association Rules that might tell as, which symptoms are most indicative of a COVID-19 infections. We already downloaded, cleaned, and prepared the dataset for you. The dataset file `data/a1-covid-symptoms-result.csv` contains over 710k transactions. Each transaction is a set of $0..n$ symptoms and $1$ test result label (\"POSITIVE\" and \"NEGATIVE\"). For example a line in the file can look like `runny_nose sore_throat fatigue POSITIVE`. Note that a line might also be just `NEGATIVE` or `POSITIVE` in case a person was tested without any symptoms. Feel free to take a look at the file -- looking at the raw data is always a good first step when it comes to data mining.\n",
    "\n",
    "Note that we generate 2 lists of transactions which we both use later:\n",
    "* `transactions_covid_all` contains all 710k+ transactions in the dataset\n",
    "* `transactions_covid_pos` contains all 11k+ transactions with a \"POSITIVE\" test result label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4f24726e-7359-48ea-9479-555212b361a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T02:46:04.889882100Z",
     "start_time": "2023-09-22T02:46:03.839018600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transactions overall: 710350\n",
      "Number of \"POSITIVE\" transactions: 11060\n",
      "\n",
      "Some example transactions:\n",
      "['muscle_sore', 'fever', 'fatigue', 'POSITIVE']\n",
      "['loss_of_smell', 'loss_of_taste', 'muscle_sore', 'fever', 'fatigue', 'headache', 'POSITIVE']\n",
      "['cough', 'sore_throat', 'fever', 'sob', 'fatigue', 'POSITIVE']\n",
      "['POSITIVE']\n",
      "['POSITIVE']\n"
     ]
    }
   ],
   "source": [
    "transactions_covid_all = []\n",
    "transactions_covid_pos = []\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "with open('data/a2-covid-symptoms-result.csv') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if 'POSITIVE' in line:\n",
    "            transactions_covid_pos.append(line.split(' '))\n",
    "        \n",
    "        transactions_covid_all.append(line.split(' '))\n",
    "\n",
    "print('Number of transactions overall: {}'.format(len(transactions_covid_all)))\n",
    "print('Number of \"POSITIVE\" transactions: {}'.format(len(transactions_covid_pos)))\n",
    "print()\n",
    "print('Some example transactions:')\n",
    "for t in transactions_covid_pos[:5]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315b9174-f19c-416e-b3db-7da8524b52bf",
   "metadata": {},
   "source": [
    "#### Finding Association Rules\n",
    "\n",
    "Now that we have the data, we can try to find interesting Association Rules in both datasets. To this end, we have prepared a series of code cells to run the Apriori Algorithm implemented by the `efficient-apriori` package over both datasets with different parameters. Each run is labeled A-D; the question at the may make references to some of these individual runs.\n",
    "\n",
    "**Note:** There is nothing for you to implement here, you can just run the code cells! However, for the questions below, you can always change the different parameters (e.g., `min_support`, `min_confidence`) to see their effects in the result as well as the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ef6b703-370b-4cbe-872c-4988d1f6aaf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T02:48:57.597575100Z",
     "start_time": "2023-09-21T02:48:52.388088200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total Number of Rules: 8861 | Number of rules with matching RHS: 4 ===\n",
      "('loss_of_smell', 'loss_of_taste') => ('POSITIVE',): supp: 0.001, conf: 0.296, lift: 19.016\n",
      "('loss_of_smell',) => ('POSITIVE',): supp: 0.002, conf: 0.224, lift: 14.389\n",
      "('cough', 'fever', 'headache') => ('POSITIVE',): supp: 0.001, conf: 0.205, lift: 13.149\n",
      "('loss_of_taste',) => ('POSITIVE',): supp: 0.002, conf: 0.200, lift: 12.866\n",
      "\n",
      "CPU times: total: 1.2 s\n",
      "Wall time: 5.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run A\n",
    "show_top_rules(transactions_covid_all, min_support=0.001, min_confidence=0.2, k=5, sort='lift', rhs='POSITIVE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "377cc9aa-19ee-46d1-b0d2-e65bca2e4c3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T02:48:57.676218100Z",
     "start_time": "2023-09-21T02:48:57.597575100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total Number of Rules: 6 | Number of rules with matching RHS: 6 ===\n",
      "('cough',) => ('POSITIVE',): supp: 0.323, conf: 1.000, lift: 1.000\n",
      "('fatigue',) => ('POSITIVE',): supp: 0.195, conf: 1.000, lift: 1.000\n",
      "('fever',) => ('POSITIVE',): supp: 0.200, conf: 1.000, lift: 1.000\n",
      "('headache',) => ('POSITIVE',): supp: 0.240, conf: 1.000, lift: 1.000\n",
      "('muscle_sore',) => ('POSITIVE',): supp: 0.193, conf: 1.000, lift: 1.000\n",
      "('sore_throat',) => ('POSITIVE',): supp: 0.174, conf: 1.000, lift: 1.000\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 15.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run B\n",
    "show_top_rules(transactions_covid_pos, min_support=0.15, min_confidence=0.8, k=5, sort='lift', rhs='POSITIVE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "608cd9c5-c04f-4f08-ad9c-ed0daef988a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T02:48:58.386462600Z",
     "start_time": "2023-09-21T02:48:57.660007300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total Number of Rules: 9434 | Number of rules with matching RHS: 9434 ===\n",
      "('french fries', 'pasta') => ('escalope', 'mushroom cream sauce'): supp: 0.001, conf: 0.348, lift: 60.675\n",
      "('mineral water', 'pasta') => ('eggs', 'shrimp'): supp: 0.001, conf: 0.625, lift: 44.228\n",
      "('french fries', 'mushroom cream sauce') => ('escalope', 'pasta'): supp: 0.001, conf: 0.229, lift: 38.966\n",
      "('escalope', 'french fries', 'pasta') => ('mushroom cream sauce',): supp: 0.001, conf: 0.667, lift: 34.970\n",
      "('escalope', 'french fries', 'mushroom cream sauce') => ('pasta',): supp: 0.001, conf: 0.533, lift: 33.903\n",
      "('fresh tuna', 'honey') => ('fromage blanc',): supp: 0.002, conf: 0.400, lift: 29.416\n",
      "\n",
      "CPU times: total: 188 ms\n",
      "Wall time: 741 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run C\n",
    "show_top_rules(transactions_groceries, min_support=0.001, min_confidence=0.2, k=5, sort='lift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56bb777c-01fa-43de-bb55-3d6ed0428c37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T02:48:58.448973300Z",
     "start_time": "2023-09-21T02:48:58.386462600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total Number of Rules: 163 | Number of rules with matching RHS: 163 ===\n",
      "('herb & pepper',) => ('ground beef',): supp: 0.016, conf: 0.323, lift: 3.292\n",
      "('mineral water', 'spaghetti') => ('ground beef',): supp: 0.017, conf: 0.286, lift: 2.908\n",
      "('tomatoes',) => ('frozen vegetables',): supp: 0.016, conf: 0.236, lift: 2.474\n",
      "('shrimp',) => ('frozen vegetables',): supp: 0.017, conf: 0.233, lift: 2.447\n",
      "('milk', 'mineral water') => ('frozen vegetables',): supp: 0.011, conf: 0.231, lift: 2.419\n",
      "('ground beef', 'mineral water') => ('spaghetti',): supp: 0.017, conf: 0.417, lift: 2.395\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 46.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run D\n",
    "show_top_rules(transactions_groceries, min_support=0.01, min_confidence=0.2, k=5, sort='lift')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### My Experiments tweaking the parameters a bit\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total Number of Rules: 0 | Number of rules with matching RHS: 0 ===\n",
      "\n",
      "CPU times: total: 93.8 ms\n",
      "Wall time: 221 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Run E\n",
    "# inc min support only\n",
    "show_top_rules(transactions_covid_all, min_support=0.1, min_confidence=0.2, k=5, sort='lift')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-22T02:46:17.470752800Z",
     "start_time": "2023-09-22T02:46:17.234391500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total Number of Rules: 0 | Number of rules with matching RHS: 0 ===\n",
      "\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 195 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Run F\n",
    "# inc min support only with rhs as positive with rhs as positive\n",
    "show_top_rules(transactions_covid_all, min_support=0.1, min_confidence=0.2, k=5, sort='lift', rhs='POSITIVE')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-22T02:46:18.842103200Z",
     "start_time": "2023-09-22T02:46:18.699418200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total Number of Rules: 1941 | Number of rules with matching RHS: 1941 ===\n",
      "('POSITIVE', 'loss_of_taste') => ('loss_of_smell',): supp: 0.001, conf: 0.772, lift: 88.984\n",
      "('POSITIVE', 'loss_of_smell') => ('loss_of_taste',): supp: 0.001, conf: 0.710, lift: 79.566\n",
      "('cough', 'headache', 'loss_of_smell') => ('loss_of_taste',): supp: 0.001, conf: 0.648, lift: 72.635\n",
      "('fatigue', 'headache', 'loss_of_smell', 'muscle_sore') => ('loss_of_taste',): supp: 0.001, conf: 0.636, lift: 71.214\n",
      "('loss_of_taste', 'runny_nose') => ('loss_of_smell',): supp: 0.001, conf: 0.608, lift: 70.162\n",
      "('fatigue', 'loss_of_smell', 'sore_throat') => ('loss_of_taste',): supp: 0.001, conf: 0.623, lift: 69.811\n",
      "\n",
      "CPU times: total: 1.7 s\n",
      "Wall time: 5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Run G\n",
    "# inc min confidence only\n",
    "show_top_rules(transactions_covid_all, min_support=0.001, min_confidence=0.6, k=5, sort='lift')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-22T02:45:19.393191200Z",
     "start_time": "2023-09-22T02:45:14.388762Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total Number of Rules: 1941 | Number of rules with matching RHS: 0 ===\n",
      "\n",
      "CPU times: total: 1.77 s\n",
      "Wall time: 4.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Run H\n",
    "# inc min confidence only with rhs as positive\n",
    "show_top_rules(transactions_covid_all, min_support=0.001, min_confidence=0.6, k=5, sort='lift',rhs = 'POSITIVE')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-22T02:46:29.231133900Z",
     "start_time": "2023-09-22T02:46:24.706423200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total Number of Rules: 0 | Number of rules with matching RHS: 0 ===\n",
      "\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 204 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Run I\n",
    "# inc both\n",
    "show_top_rules(transactions_covid_all, min_support=0.1, min_confidence=0.6, k=5, sort='lift')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-22T02:46:42.638175800Z",
     "start_time": "2023-09-22T02:46:42.418188600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total Number of Rules: 0 | Number of rules with matching RHS: 0 ===\n",
      "\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 196 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Run J\n",
    "# inc both with rhs as POSITIVE\n",
    "show_top_rules(transactions_covid_all, min_support=0.1, min_confidence=0.6, k=5, sort='lift', rhs='POSITIVE')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-22T02:46:47.097798200Z",
     "start_time": "2023-09-22T02:46:46.885641500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total Number of Rules: 174357 | Number of rules with matching RHS: 1190 ===\n",
      "('cough', 'fever', 'headache', 'loss_of_smell', 'loss_of_taste', 'runny_nose') => ('POSITIVE',): supp: 0.000, conf: 0.526, lift: 33.804\n",
      "('cough', 'fever', 'loss_of_smell', 'loss_of_taste', 'sore_throat') => ('POSITIVE',): supp: 0.000, conf: 0.500, lift: 32.113\n",
      "('cough', 'fever', 'headache', 'loss_of_smell', 'loss_of_taste', 'sore_throat') => ('POSITIVE',): supp: 0.000, conf: 0.481, lift: 30.924\n",
      "('cough', 'fever', 'loss_of_smell', 'loss_of_taste', 'runny_nose') => ('POSITIVE',): supp: 0.000, conf: 0.478, lift: 30.717\n",
      "('cough', 'fever', 'loss_of_smell', 'loss_of_taste', 'muscle_sore', 'runny_nose') => ('POSITIVE',): supp: 0.000, conf: 0.474, lift: 30.423\n",
      "('cough', 'fever', 'loss_of_smell', 'loss_of_taste', 'runny_nose', 'sore_throat') => ('POSITIVE',): supp: 0.000, conf: 0.474, lift: 30.423\n",
      "\n",
      "CPU times: total: 8.42 s\n",
      "Wall time: 18.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Run K\n",
    "# dec min support with rhs as positive\n",
    "show_top_rules(transactions_covid_all, min_support=0.000001, min_confidence=0.2, k=5, sort='lift', rhs='POSITIVE')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-22T02:47:07.879346Z",
     "start_time": "2023-09-22T02:46:49.136975800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total Number of Rules: 18359 | Number of rules with matching RHS: 35 ===\n",
      "('loss_of_smell', 'loss_of_taste') => ('POSITIVE',): supp: 0.001, conf: 0.296, lift: 19.016\n",
      "('loss_of_smell',) => ('POSITIVE',): supp: 0.002, conf: 0.224, lift: 14.389\n",
      "('cough', 'fever', 'headache') => ('POSITIVE',): supp: 0.001, conf: 0.205, lift: 13.149\n",
      "('loss_of_taste',) => ('POSITIVE',): supp: 0.002, conf: 0.200, lift: 12.866\n",
      "('cough', 'fever') => ('POSITIVE',): supp: 0.002, conf: 0.171, lift: 10.968\n",
      "('fever', 'muscle_sore') => ('POSITIVE',): supp: 0.001, conf: 0.130, lift: 8.332\n",
      "\n",
      "CPU times: total: 1.75 s\n",
      "Wall time: 4.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Run L\n",
    "# dec min confidence with rhs as positive\n",
    "show_top_rules(transactions_covid_all, min_support=0.001, min_confidence=0.02, k=5, sort='lift', rhs='POSITIVE')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-22T02:47:12.304610700Z",
     "start_time": "2023-09-22T02:47:07.882344400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "e09b2e16-5ab2-45dd-8cde-9c428036ac4f",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d0fad-1624-408b-b764-77c3dfe505c5",
   "metadata": {},
   "source": [
    "**Question 3.3 a) (3 Points)** From your observation, what are the effects of increasing/reducing `min_support` and `min_confidence`? Support your answer with evidence. You can perform more runs of efficient-apriori with different parameter settings, if needed.\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "- The runs from E -> L are custom runs which I used to verify my hypothesis on what happens when changing the min_support and min_confidence values\n",
    "1. **Min Support:**\n",
    "- **Increasing min_support:**\n",
    "    + On increasing the min_support threshold, only the itemsets which occur more frequently in the dataset are to be considered for association rules. As a result, fewer rules are obtained as only the most frequent itemsets will meet this criterion.\n",
    "    + e.g. Runs E & F actually show no results as none of the itemsets even meet this min support threshold\n",
    "    + Generally speaking, the rules which have higher support are more reliable as we are considering only the most frequently occuring itemsets as some of the infrequent itemsets could also be just noise.\n",
    "- **Decreasing min_support:**\n",
    "    + Along similar lines, decreasing the min_support allows for the discovery of a large number of rules.\n",
    "    + e.g. Run K shows that the total of number of rules has considerably increased when compared with Run A\n",
    "    + Generally speaking, some of the rules having low support may indicate that they are based on infrequent patterns and might not be as reliable.\n",
    "\n",
    "2. **Min Confidence:**\n",
    "- **Increasing min_confidence:**\n",
    "    + Increasing min_confidence filters out rules with lower confidence, ensuring that only strong associations are considered. This results in fewer rules, but they are more trustworthy. (e.g. Runs G & H)\n",
    "    + Rules with higher confidence are more predictive, indicating a stronger relationship between the antecedent (LHS) and consequent (RHS) items.\n",
    "    + Higher min_confidence reduces the likelihood of including spurious rules that might occur due to random chance\n",
    "\n",
    "- **Decreasing min_confidence:**\n",
    "    + Lowering min_confidence allows for the discovery of a larger number of rules, even if they have lower confidence. This can be useful for exploring weaker but potentially interesting associations.\n",
    "    + e.g. Run L\n",
    "\n",
    "- **Conclusion:**\n",
    "    + In summary, adjusting min_support and min_confidence allows you to control the number and quality of the discovered association rules. Increasing these thresholds results in fewer but more reliable rules, while reducing them produces more \"interesting\" rules, including potentially weaker associations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c4cf70-2ff1-44d8-b171-2210395bfc6d",
   "metadata": {},
   "source": [
    "**Question 3.3 b) (3 Points)** Runs A and B return association rules with symptoms on the left-hand side and a POSITIVE test result on the right-hand side. As such both runs find rules which (combination of) symptoms are most indicative of a positive test result. However, the results of Run A and B are rather different. Explain the differences and discuss which result provides more reliable insights!\n",
    "\n",
    "**Your Answer:**\n",
    "- Runs A & B both use different datasets and also have different thresholds for min_support and min_confidence. Let us look at the differences below\n",
    "- **Run A:**\n",
    "    + Uses the larger dataset with both positive and negative cases. (transactions_covid_all)\n",
    "    + Lower minimum support (0.001) and lower minimum confidence (0.2) thresholds.\n",
    "    + Generates more rules, including potentially weaker associations.\n",
    "    + Insights can be broader but potentially less reliable due to lower confidence thresholds.\n",
    "    + However, the high value for lift indicates that there is a strong association even though the confidence might be lower.\n",
    "    + Suitable for exploratory analysis and capturing a wide range of associations if we are not restricting ourselves only to the RHS of 'POSITIVE'\n",
    "- **Run B:**\n",
    "    + Smaller dataset, focusing exclusively on positive cases. (transactions_covid_pos)\n",
    "    + Higher minimum support (0.15) and higher minimum confidence (0.8) thresholds.\n",
    "    + Generates fewer rules. Out of the rules printed all of them had perfect confidence (1.000). Which means that both the antecedent and the consequent itemsets always occur together\n",
    "    + But the lift value of 1 indicates that there is a random association. Which means that we cannot really gain any new insights from this despite the fact that we are dealing only with positive cases.\n",
    "- **Summary:**\n",
    "    +  In conclusion, Run A is more reliable for gaining new insights due to its higher lift values when compared with Run B\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f772c97-1c63-4899-8057-761390eaacbd",
   "metadata": {},
   "source": [
    "**Question 3.3 c) (2 Points)** Runs A and C roughly return the same number of rules in roughly the similar amount of time. However, there are almost 10 times more transactions in the COVID dataset (Run A) compared to the groceries dataset (Run C). Explain why the result and performance are very similar despite the very different number of transactions.\n",
    "\n",
    "**Important:** The exact runtimes might vary noticeable depending on your PC/laptop. So if the runtimes of both runs are not similar for you, please simply assume that the two runtimes are similar!\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "Both runs have the same values for min_support(0.001) and min_confidence(0.2) {which are relatively low} which means we are trying to mine out more interesting rule associations where we don't bother for frequent items or for items which occur together more often.\n",
    "\n",
    "That said, there are several reasons why the performance and results are very similar, they are listed below:\n",
    "1. Even though the size of the datasets are different, the apriori (anti-monotonicity) principle ensures that we don't need to consider itemsets which have a support lesser than the threshold and association rules which have lesser confidence than the threshold. This essentially removes out entire subtrees in the tree of all possible itemsets and rule-associations thereby making computation much faster. It might be the case that for both of these datasets the final effective tree size we need for computation of support and confidence happens to be the same after filtering which results in similar performance.\n",
    "2. Other reason for why both operations take similar times might be related to the very implementation of the efficient-apriori algorithm itself. The implementation of the algorithm could be so optimal due to its memory management and use of datastructures for efficient pruning of the search space that the performance results might be similar in the end.\n",
    "3. Another reason which could explain this observation is the size of each transaction. If the transactions are relatively sparse, meaning that each transaction contains only a small subset of items from a large itemset, the algorithm can work efficiently which might be the case in both of these runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67e2ad9-2525-4cba-a8fa-5c012f14adee",
   "metadata": {},
   "source": [
    "**Question 3.3 d) (2 Points)** Have a look at the confidence and lift of the top rules for Run A and B. What noticeable differences can you observe and how can you explain these differences?\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "- In terms of probabilities (not in terms of support / confidence) the formula for lift can be written as\n",
    "\n",
    "$$Lift(A,B) = \\frac{P(A \\cap B)}{P(A) * P(B)}$$\n",
    "\n",
    "- Generally speaking here is how we interpret different lift values:\n",
    "    + lift = 1 (Indicates no association between A and B , as it would be one only when both A and B are independent from each other. i.e. no intersection in their venn diagram)\n",
    "    + lift > 1 (Indicates that there is a strong association between A and B, as in this case their intersection  in the venn diagram happens to be really big)\n",
    "    + lift < 1 (Indicates that there is a weak association between A and B, as in this case their intersection in the venn diagram happens to be small)\n",
    "- There are also cases where we can have a high confidence but a low lift value because each of those itemsets by themselves might be popular ( i.e. more frequent) but the only reason they occur together is because of their individual popularity and not because of their association strength.\n",
    "- Using this knowledge, here are my observations for the confidence and lift values for the top rules of Run A & B:\n",
    "    + Run A summary:\n",
    "        + Uses transactions_covid_all, larger dataset\n",
    "        + Minimum Support: 0.001\n",
    "        + Minimum Confidence: 0.2\n",
    "    + Run B summary:\n",
    "        + Uses transactions_covid_pos, smaller dataset\n",
    "        + Minimum Support: 0.15 (Higher than Run A)\n",
    "        + Minimum Confidence: 0.8 (Higher than Run A)\n",
    "    + In Run A's top rules we can see that the confidence values are lower but the lift values are high whereas in Run B's top rules we can see that it has a perfect confidence but the lift values are exactly equal to 1\n",
    "    + What this implies is that in Run A, despite the low confidence & low support there is a reasonable lift because when the association does occur it is note-worthy and non-random.\n",
    "        + This implies that while the symptoms being analyzed may not always lead to a positive test result (low confidence), when they do, it's a strong and meaningful signal (high lift). This could be due to the co-occurrence of symptoms being significant within the subset of the dataset containing positive cases, even though they are not exclusive to COVID-19.\n",
    "    + Similarly, in Run B despite the perfect confidence values ( meaning they always occur together) there is a lift of 1 meaning that the association between both are due to randomness as both the antecedent and consequent are actually independent of each other but just happen to co-occur many times together\n",
    "        + This is because, in this specific dataset of positive cases, the presence of symptoms and a positive test result are closely tied together, but there is no actual cause-and-effect relationship. Therefore, the lift value of 1 suggests that the co-occurrence of symptoms and positive test results is expected and doesn't provide additional information beyond what would be random chance within this subset of positive cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-22T06:09:55.971326700Z",
     "start_time": "2023-09-22T06:09:55.939968700Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
