{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763f0fa0-6584-44d4-b163-a891252769f9",
   "metadata": {},
   "source": [
    "# CS5228 Assignment 2 Extra\n",
    "\n",
    "Hello everyone, this assignment notebook covers Clustering using question-answering tasks. For the answers, you can refer to [this Markdown guide](https://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd) to customize the layout (although it shouldn't be needed).\n",
    "\n",
    "**Important:** \n",
    "* Rename and save this Jupyter notebook as **cs5228_a2_extra_YourName_YourNUSNETID.ipynb** (e.g., **cs5228_a2_BobSmith_e12345678.ipynb**) before submission!\n",
    "* Submission deadline is Sep 28, 11.59 pm. Late submissions will be penalized by 10% for each additional day. Failure to appropriately rename both files will yield a penalty of 1 Point. There is no need to use your full name if its a rather long; it's just  important to easily identify you in Canvas etc.\n",
    "\n",
    "Please also add your NUSNET and student id in the code cell below. This is just to make any identification of your notebook doubly sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "119a3b10-7afe-4814-9a79-32310842f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_id = 'A0285647M'\n",
    "nusnet_id = 'e1216292'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7455d1c-31ad-4620-8d8a-ad2adc363f56",
   "metadata": {},
   "source": [
    "Here is an overview over the tasks to be solved and the points associated with each task. The notebook can appear very long and verbose, but note that a lot of parts provide additional explanations, documentation, or some discussion. The code and markdown cells you are supposed to complete are well, but you can use the overview below to double-check that you covered everything.\n",
    "\n",
    "* **4 Questions about Clustering (5 Points)**\n",
    "    * 4.1 DBSCAN on Scaled Data (2 Points)\n",
    "    * 4.2 K-Means++ with Deterministic Result (3 Points)Â¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ccaa8-0df8-4b2c-9676-0f872ec645d8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e65ca2e-36ac-4d5e-aa5d-b5c5e39ba1a4",
   "metadata": {},
   "source": [
    "## 4 Questions about Clustering (5 Points)\n",
    "\n",
    "### 4.1 DBSCAN on Scaled Data (2 Points)\n",
    "\n",
    "Assume you have a $d$-dimensional dataset `X` in the Euclidean space, i.e., each data point as $d$ numerical features (with each feature value in the interval $[0, 1]$). After running DBSCAN over `X`, you get some clustering (again, we only assume it's not only noise). Now you create a new dataset `X_new` by multiplying all data points by 10 afterwards adding 100 to all data points (in Python, assuming X is a NumPy array this can simply be done by `X_new = X * 10 + 100`). Now you can run DBSCAN over `X_new`.\n",
    "\n",
    "**Explain how you have to change the parameters of DBSCAN for `X_new` to get the same clusters as for `X`!**. You can ignore any corner cases like duplicate data points or the case where all the data points are noise points.\n",
    "\n",
    "**Your Answer:**'\n",
    "\n",
    "- Since we are scaling each datapoint , each feature will now lie in the range of [100,110] ( which is like a linear transformation as we are stretching every dimension in the \"d\" dimensions by a factor of 10)\n",
    "- The first obvious thing is that adding 100 ( or say another number ) only shifts the datapoints around and does not contribute to the stretching or shrinking of this linear space. Therefore, it makes sense that amongst the parameters 'epsilon' and 'min_neighbours' only epsilon is scaled up as min_neighbours stays as is.\n",
    "- With this scaled up epsilon value and the same min_neighbours we should be able to get the same clustering irrespective of what bias we are adding ( i.e. in this specific case it was a bias of 100 added to each datapoint, it could have as well been 50 or 77 but in all cases epsilon only needs to be scaled up)\n",
    "- Now the question is how to scale up epsilon when staying in a particular dimension \"d\". In 2 dimensions it is a circle of influence, in 3 dimensions the epsilon neighbourhood is a sphere of influence. Therefore, in general in 'd' dimensions, the epsilon neighbourhood is a d-dimensional hypersphere.\n",
    "- Since we are scaling it by a factor of 10;  in 2 dimensions, one square grid between (0,0) -> (1,1) gets stretched out to a bigger area in the range (100,100) -> (110,110). Essentially in d-dimensions, each one of those dimensions is going to be stretched by 10.\n",
    "- Since we are dealing with d-dimensional hypersphere now, its radius (which is epsilon) also has to be stretched by a factor of 10 (as the radius vector is also like dimensional vector)\n",
    "\n",
    "$$\\epsilon_{new} = 10*\\epsilon_{old}$$\n",
    "\n",
    "- Therefore, in essence the dimensionality does not matter. i.e. \"d\" could be anything ; but epsilon should only be scaled by a factor of 10.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c4b827",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2 K-Means++ with Deterministic Result (3 Points)\n",
    "\n",
    "Assume you have a $d$-dimensional dataset `X` in the Euclidean space, i.e., each data point as $d$ numerical features (with each feature value in the interval $[0, 1]$). You have 1,000 data points in total. Now you run K-Means with K-Means++ initialization and a value of K=20, yielding a clustering of 20 non-empty clusters.\n",
    "\n",
    "**Describe a data distribution of `X` where you will always get the same clustering when running K-Means++ with K=20!**. In other words, you run K-Means++ with K=20 again and again over X, and you will always get the same clustering. How must `X` \"look\" like to guarantee that?\n",
    "\n",
    "**Your answer:**\n",
    "\n",
    "- The only way to achieve this would be to have 20 coordinates in the d dimensional space where each coordinate has 50 duplicate points in them.\n",
    "- In this particular case, when a particular point is chosen to be a centroid, the next centroid picked would correspond to some other coordinate in space which also has 50 duplicate points inside it.\n",
    "- This way, the picking of 20 centroids would correspond exactly to those 20 coordinate locations in space (each having 50 points), and each run of k-means++ would only pick the next centroid in a random manner. But at the end we will exactly have the same set of 20 centroids being picked.\n",
    "- Now when we run the regular k-means, each of the 49 other duplicate points would get assigned to the cluster corresponding to the other duplicate centroid point which was found earlier.\n",
    "- This way in the end, the 20 clusters determined will always be the same even though the order of initialization of centroids was different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
