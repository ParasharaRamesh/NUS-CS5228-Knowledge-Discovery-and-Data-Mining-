{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763f0fa0-6584-44d4-b163-a891252769f9",
   "metadata": {},
   "source": [
    "# CS5228 Assignment 2 - DBSCAN, AGNES & Association Rule Mining\n",
    "\n",
    "Hello everyone, this assignment notebook covers DBSCAN, AGNES, Association Rule Mining (ARM). There are some code-completion tasks and question-answering tasks in this answer sheet. For code completion tasks, please write down your answer (i.e., your lines of code) between sentences that \"Your code starts here\" and \"Your code ends here\". The space between these two lines does not reflect the required or expected lines of code. For answers in plain text, you can refer to [this Markdown guide](https://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd) to customize the layout (although it shouldn't be needed).\n",
    "\n",
    "When you work on this notebook, you can insert additional code cells (e.g., for testing) or markdown cells (e.g., to keep track of your thoughts). However, before the submission, please remove all those additional cells again. Thanks!\n",
    "\n",
    "**Important:** \n",
    "* Rename and save this Jupyter notebook as **cs5228_a2_YourName_YourNUSNETID.ipynb** (e.g., **cs5228_a2_BobSmith_e12345678.ipynb**) before submission!\n",
    "* Rename and save the script file *cs5228_a2_script.py* as **cs5228_a2_YourName_YourNUSNETID.py** (e.g., **cs5228_a2_BobSmith_e12345678.py**) before submission!\n",
    "* Submission deadline is Sep 28, 11.59 pm. Late submissions will be penalized by 10% for each additional day. Failure to appropriately rename both files will yield a penalty of 1 Point. There is no need to use your full name if its a rather long; it's just  important to easily identify you in Canvas etc.\n",
    "\n",
    "Please also add your NUSNET and student id in the code cell below. This is just to make any identification of your notebook doubly sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119a3b10-7afe-4814-9a79-32310842f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_id = ''\n",
    "nusnet_id = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7455d1c-31ad-4620-8d8a-ad2adc363f56",
   "metadata": {},
   "source": [
    "Here is an overview over the tasks to be solved and the points associated with each task. The notebook can appear very long and verbose, but note that a lot of parts provide additional explanations, documentation, or some discussion. The code and markdown cells you are supposed to complete are well, but you can use the overview below to double-check that you covered everything.\n",
    "\n",
    "* **1 DBSCAN (15 Points)**\n",
    "    * 1.1Implementing DBSCAN for Noise Detection (10 Points)\n",
    "        * 1.1 a) Compute Core Points (5 Points)\n",
    "        * 1.2 b) Compute Noise Points (5 Points)\n",
    "    * 1.2 Questions about DBSCAN (5 Points)\n",
    "        * 1.2 a) Basic Data Understanding with DBSCAN (3 Points)\n",
    "        * 1.2 b) Finding all Cluster (2 Points)\n",
    "* **2 AGNES (10 Points)**\n",
    "    * 2.1 Interpreting Dendrograms (6 Points)\n",
    "    * 2.2 Questions about AGNES (4 Points)\n",
    "        * 2.2 a) Picking the Right Linkage Method (2 Points)\n",
    "        * 2.2 b) Outlier Detection with AGNES(2 Points)\n",
    "* **3 Association Rule Mining (25 Points)**\n",
    "    * 3.1 Brute-Forcing Association Rule Mining (5 Points)\n",
    "    * 3.2 Implementing Apriori Algorithm for Finding Frequent Itemsets (10 Points)\n",
    "        * 3.2 a) Generating Candidate Itemsets $L_{k}$ (5 Points)\n",
    "        * 3.2 b) Generating Frequent Itemsets $F_{1}$, $F_{2}$, $F_{3}$, ... (5 Points)\n",
    "    * 3.3 ARM over Real-World Datasets -- Questions (10 Points)\n",
    "        * 3.3 a) (3 Points)\n",
    "        * 3.3 b) (3 Points)\n",
    "        * 3.3 c) (2 Points)\n",
    "        * 3.3 d) (2 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a4e6d6-30f8-4722-9750-4986c33fcccd",
   "metadata": {},
   "source": [
    "## Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca2bb72-8aa3-4d53-9d2a-e4953aa02592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from efficient_apriori import apriori\n",
    "\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befecf0d-9ea3-4355-abb3-6cc7b1daa7ec",
   "metadata": {},
   "source": [
    "**Important:** This notebook also requires you to complete in a separate `.py` script file. This keeps this notebook cleaner and simplifies testing your implementations for us. As you need to rename the file `cs5228_a2.py`, you also need to edit the import statement below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0827e5-fcc6-42d4-93df-96258a07cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs5228_a2 import *\n",
    "#from cs5228_a2_BobSmith_e12345678 import *  # <-- you will need to rename this accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ccaa8-0df8-4b2c-9676-0f872ec645d8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e65ca2e-36ac-4d5e-aa5d-b5c5e39ba1a4",
   "metadata": {},
   "source": [
    "## 1 DBSCAN\n",
    "\n",
    "The Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm is a clustering algorithm used in data mining and machine learning. It is designed to identify clusters in data that may have arbitrary shapes and handle outliers effectively. Here's a more detailed explanation of how the DBSCAN algorithm works:\n",
    "\n",
    "* **Density-Based Clustering:** DBSCAN identifies clusters based on the density of data points in the feature space rather than assuming a specific cluster shape.\n",
    "\n",
    "* **Core Points:** The algorithm starts by selecting a random data point. It then checks how many data points are within a specified distance (epsilon or Îµ) from this point. If the number of data points within this distance exceeds a predefined threshold called MinPts (including the data point itself), the selected point is classified as a \"core point.\"\n",
    "\n",
    "* **Border Points:** Data points that are within the epsilon distance of a core point but do not meet the MinPts criterion are classified as \"border points.\" Border points can be part of a cluster but are located on the edges of the cluster.\n",
    "\n",
    "* **Noise Points:** Data points that are neither core points nor border points are classified as \"noise points\" or outliers. These are data points that do not belong to any cluster.\n",
    "\n",
    "* **Cluster Expansion:** The algorithm iteratively expands clusters by connecting core points and their density-reachable neighbors. Two core points are considered \"density-reachable\" if there is a path of core points connecting them within the epsilon distance. This process continues until no more core points can be added to a cluster.\n",
    "\n",
    "* **Result:** The output of DBSCAN is a set of clusters, each consisting of core points and potentially including border points. Noise points are not assigned to any cluster.\n",
    "\n",
    "DBSCAN is a valuable algorithm for clustering spatial data when the characteristics of the clusters are not well-known in advance and is commonly used in various applications, including geospatial analysis, image processing, and anomaly detection. In this section, we look into DBSCAN to implement a simplified variant instead of the \"full\" algorithm. For this, let's first load some random data for visualization and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5be27ac-6c0c-4cc8-af75-85c5e90d724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dbscan_toy = pd.read_csv('data/a2-dbscan-toy-dataset.txt', header=None, sep=' ').to_numpy()\n",
    "\n",
    "print('The shape of X_dbscan_toy is {}'.format(X_dbscan_toy.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d77d1-5f26-4171-a28b-bc11c9a2b637",
   "metadata": {},
   "source": [
    "Now we can run scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) on this dataset. Here we use `eps=0.1` and `min_samples=10` as values for the two main input parameters for DBSCAN that specify the minimum \"density\" of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95c8274-10f4-4847-8873-d1fb745e387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_clustering = DBSCAN(eps=0.1, min_samples=11).fit(X_dbscan_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e8e42-5e6b-4380-a6c4-e2a0dfe83d2e",
   "metadata": {},
   "source": [
    "The points that are noise points are labeled with `-1`, while all points belonging to clusters are labeled with `0`, `1`, `2`, etc. So we can easily find the indices of all the points labeled as noise as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d35e2a8-c626-4682-a602-86255d06fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_point_indices = np.argwhere(dbscan_clustering.labels_ >= 0).squeeze()\n",
    "noise_point_indices = np.argwhere(dbscan_clustering.labels_ < 0).squeeze()\n",
    "\n",
    "print('The indices of the points labeled as noise are: {}'.format(noise_point_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f3cd41-69a7-421a-bd7b-e5d461f98251",
   "metadata": {},
   "source": [
    "Of course, we can also plot the results. Note that the figure below only highlights the points labeled as noise as red triangles; all points belonging to *some* clusters are in grey points (note that we do not care to which exact cluster these points belong)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22421c9-7321-4cf8-8170-1fc5b8f0e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_dbscan_toy[cluster_point_indices,0], X_dbscan_toy[cluster_point_indices,1], c='grey')\n",
    "plt.scatter(X_dbscan_toy[noise_point_indices,0], X_dbscan_toy[noise_point_indices,1], c='red', marker='^', s=75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ea655-115e-4eb6-bfd9-bdf320a890e6",
   "metadata": {},
   "source": [
    "Summing up, the red dots in the plots we define as noise or outliers as they are very dissimilar to the other data points. In practice, we would likely remove those noise points, treat them separately, or maybe perform additional preprocessing steps to potentially \"denoise\" the dataset. However, the steps of choice generally depend heavily on the exact data mining task. Here, we focus on the identification of noise points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d510dba9-f28d-4ca9-ab74-14a84cf99378",
   "metadata": {},
   "source": [
    "### 1.1 Implementing DBSCAN for Noise Detection (10 Points)\n",
    "\n",
    "In the lecture, we covered the original algorithm of DBSCAN, which you can also find on [Wikipedia](https://en.wikipedia.org/wiki/DBSCAN). While not difficult to implement, it takes quite a couple of lines of codes to do so. For this assignment, however, we are only interested in the points of a dataset that DBSCAN considers noise (as illustrated above; the red dots in the previous plot). This includes that we do not have to care about\n",
    "\n",
    "* how many clusters there are (the plot above hints at 3 clusters but it does not matter) *and*\n",
    "* which non-noise points (the grey dots in the plot above) belong to which cluster\n",
    "\n",
    "**Your task is to implement a modified/simplified version of DBSCAN to find all noise points in a dataset!** The skeleton of method `get_noise_dbscan()` you need to complete is found in the file `cs5228_a2.py` (before the appropriate renaming). The method takes data matrix `X` as well as the two basic parameters `eps` and `min_samples` as input parameters; we use the same naming as scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html).  The output should be 2 lists of indices: (a) one containing the indices of all *core points* and (b) on containing the indices of all *noise points* in input dataset X.\n",
    "\n",
    "**Important:**\n",
    "* We only split this task into 2.1 a) and 2.1 b) to have intermediate results you can check for correctness (and potentially to better allow for partial marking). Our reference solutions first finds all core points and uses this information to find all noise points; hence the 2 separate code blocks for you to complete.\n",
    "* However, if you have a better/faster/shorter/cooler/etc. solution, you are more than welcome to implement it and ignore the intermediate result of finding all core points. Only the result from 2.1 b) is important. This also means that you can ignore 2.1 a) and still get full marks if you correctly identify all noise points.\n",
    "* If you have an alternative solution, please make sure that the method still returns the 2 output parameters `(core_point_indices, noise_point_indices)`. If you do not need to explicitly identify the core points, you can simply return `None` for `core_point_indices`.\n",
    "* You can import any method `numpy`, `scipy`, `sklearn`, or `pandas` has to offer -- except for any ready-made implementation of DBSCAN, of course :). Please add any imports to the code cell at the top with the other imports. Hint: We already imported [`sklearn.metrics.pairwise.euclidean_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html) for you.\n",
    "\n",
    "We will benchmark your implementation as part of our Little Competitions to see whose solution is the fastest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860dcf0d-5d3e-4261-ba18-48dc4b80d2d7",
   "metadata": {},
   "source": [
    "#### 1.1 a) Compute Core Points (5 Points)\n",
    "\n",
    "As mentioned above, our reference solution first computes all core points. If you follow this approach, complete the respective part in the code of method `get_noise_dbscan()`. Some hints:\n",
    "* Recall that we do not care to which cluster a core point a data sample belongs, only that is a core point in *some* cluster\n",
    "* Have a look at method [`sklearn.metrics.pairwise.euclidean_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html); it might make your life easier.\n",
    "\n",
    "**Important:** Avoid using loops in the parts of the code you have to complete. If you use loops but the results are correct, there will be some minor deduction of points. Once you know what you need to do, it is almost guaranteed that `numpy`, `scipy`, or `sklearn` will provide a useful method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2772f0a2-52e4-4f50-a148-763bfb040374",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_core_point_indices, _ = get_noise_dbscan(X_dbscan_toy, eps=0.1, min_samples=10)\n",
    "\n",
    "print('Total number of core points: {}\\n'.format(len(my_core_point_indices)))\n",
    "print('The first 25 indices of the points labeled as core points:\\n{}'.format(sorted(my_core_point_indices)[:25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4555514-9b0f-48f8-89d7-a7472da31715",
   "metadata": {},
   "source": [
    "The output of previous code cell should look like:\n",
    "    \n",
    "```\n",
    "Total number of core points: 50\n",
    "\n",
    "The first 25 indices of the points labeled as core points:\n",
    "[1, 3, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24]\n",
    "```\n",
    "\n",
    "Note that `0`, `4`, and `27` are missing from this list since [`sklearn.cluster.DBSCAN`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) told us that these points are noise. Of course, also the border points are missing here, but [`sklearn.cluster.DBSCAN`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) does not return those explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0410e2-e9e4-4e8c-8583-7a35ce2c4e43",
   "metadata": {},
   "source": [
    "#### 1.1 b) Compute Noise Points (5 Points)\n",
    "\n",
    "Knowing the core points is useful but only an intermediate step. Now it is time to complete the method `get_noise_dbscan()` to compute the indices of all noise points in `X`. Again, our reference solution uses `core_point_indices` to accomplish this. If your implementation does not require the information about core points but returns the correct `noise_point_indices` then this is perfectly fine!\n",
    "\n",
    "**Important:** Avoid using loops in the parts of the code you have to complete. If you use loops but the results are correct, there will be some minor deduction of points. Once you know what you need to do, it is almost guaranteed that `numpy`, `scipy`, or `sklearn` will provide a useful method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ab1310-b286-479b-af36-57397a7e0afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, my_noise_point_indices = get_noise_dbscan(X_dbscan_toy, eps=0.1, min_samples=10)\n",
    "\n",
    "print('Total number of noise points: {}\\n'.format(len(my_noise_point_indices)))\n",
    "print('The indices of all points labeled as noise points:\\n{}'.format(sorted(my_noise_point_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696b674a-ed95-47dd-a8ce-fa9a0bb366f8",
   "metadata": {},
   "source": [
    "The output of previous code cell should look like:\n",
    "\n",
    "```\n",
    "Total number of noise points: 10\n",
    "\n",
    "The indices of all points labeled as noise points:\n",
    "[0, 4, 27, 31, 33, 39, 43, 46, 51, 65]\n",
    "```\n",
    "\n",
    "Since we used the same values for `eps` and `min_samples`, this result matches the output we saw earlier when we used scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) over the toy dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9b11df-e56e-4c67-b25d-7e02e03d9a40",
   "metadata": {},
   "source": [
    "### 1.2 Questions about DBSCAN\n",
    "\n",
    "**1.2 a) Basic Data Understanding with DBSCAN (3 Points)**\n",
    "\n",
    "Assume you have a dataset `X`, run DBSCAN, and get a clustering that contains a set of clusters and some noise points (there's no need to be more precise; it's only important that you don't get just noise). Let's also assume you create a new dataset `X_new` simply by shuffling `X`; no other changes. Now you run DBSCAN with the same parameters as before over `X_new` and get a different clustering, i.e., most of the clusters are not exactly the same as before.\n",
    "\n",
    "**Describe what this information tells about the dataset and clustering!** This may include a brief discussion how changing the parameters of DBSCAN will likely affect the results.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508f6a67-1e79-4d30-828e-6bd78ad5d7a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f201ef0-2ed3-47e8-8313-bf04aebdca9d",
   "metadata": {},
   "source": [
    "**1.2 b) Finding all Cluster (2 Points)**\n",
    "\n",
    "Your method `get_noise_dbscan()` finds all the noise points in a dataset according to the definition of DBSCAN. Now let's assume you now want to find all clusters, i.e., the number of clusters and which noise point belongs to which cluster.\n",
    "\n",
    "**2.2a) Describe how the result of `get_noise_dbscan()` may help you to find all clusters! (2 Points)** There is no need to implement anything; just a brief description how clusters can be found by already knowing all noise points.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b65c0bf-de0b-4f1f-8e1d-005c3ae150ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8725ae50-60bd-4706-89b2-b48a03566084",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b6f28-66cc-432b-b602-28f7b8a987a9",
   "metadata": {},
   "source": [
    "## 2 AGNES\n",
    "\n",
    "AGNES, which stands for Agglomerative Nesting, is an agglomerative hierarchical clustering algorithm used in data analysis and machine learning. It is a bottom-up approach to clustering, meaning it starts with individual data points as separate clusters and iteratively merges them into larger clusters. Here's an overview of how the AGNES clustering algorithm works:\n",
    "\n",
    "* **Initialization:** Initially, each data point is treated as a single cluster, resulting in as many clusters as there are data points.\n",
    "\n",
    "* **Agglomerative Process:** AGNES proceeds by iteratively merging the two closest clusters into a single larger cluster. The distance between clusters is typically determined by a linkage criterion, which can be one of the following commonly used methods: *Single Linkage*, *Complete Linkage*, *Average Linkage*, and more.\n",
    "\n",
    "* **Hierarchy Building:** As clusters are merged, a hierarchy or dendrogram is built to represent the sequence of cluster mergers. This dendrogram can be used to visualize the hierarchical structure of the data and choose the desired number of clusters later.\n",
    "\n",
    "* **Stopping Criterion:** AGNES continues merging clusters until a stopping criterion is met. This criterion could be based on a predetermined number of clusters, a threshold distance, or other criteria. By default, AGNES stops when all data points form a single cluster.\n",
    "\n",
    "* **Result:** The output of AGNES is a hierarchical tree-like structure called a dendrogram, which can be cut at a specific level to obtain a partition of the data into clusters. The choice of where to cut the dendrogram depends on the desired number of clusters.\n",
    "\n",
    "AGNES is a valuable tool for hierarchical clustering and can be useful in various data analysis tasks, including taxonomy construction, gene expression analysis, and image segmentation.\n",
    "\n",
    "### 2.1 Interpreting Dendrograms\n",
    "\n",
    "We saw in the lecture that dendrograms are a meaningful way to visualize the hierarchical relationships between the data points with respect to the clustering using AGNES (or any other hierarchical clustering technique). Properly interpreting is important to get a correct understanding of the underlying data.\n",
    "\n",
    "Below are the plots of 6 different datasets labeled A-F. Each dataset contains 30 data points, each with two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ae180-bb68-41d1-b6d9-9bdb6b955e3f",
   "metadata": {},
   "source": [
    "<img src=\"data/a2-agnes-data-labeled.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8274c5-6643-4365-81fd-127fc7f3144e",
   "metadata": {},
   "source": [
    "Below are 6 dendrograms labeled 1-6. These dendrograms show the clustering using **AGNES with Single Linkage** for the 6 dataset above, but in a random order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f27422f-5c85-4046-9cb9-791ed5e106f5",
   "metadata": {},
   "source": [
    "<img src=\"data/a2-agnes-dendrogram-labeled.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f21fe-47cb-4ef1-8a01-c185ca0ecbcf",
   "metadata": {},
   "source": [
    "**Find the correct combinations of datasets and dendrograms** -- that is, find for each dataset the dendrogram that visualizes the clustering using AGNES with Single Linkage! Give a brief explanation for each decision; max 1-2 sentences! Complete the table below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f666ddb-5b31-45e0-8480-0fcc2986976c",
   "metadata": {},
   "source": [
    "| Dataset | Dendrogram | Brief Explanation |\n",
    "| ---  | ---   | ---                  |\n",
    "| **A**    | ??? | ??? |\n",
    "| **B**    | ??? | ??? |\n",
    "| **C**    | ??? | ??? |\n",
    "| **D**    | ??? | ??? |\n",
    "| **E**    | ??? | ??? |\n",
    "| **F**    | ??? | ??? |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bed46c-f6d9-4235-9434-79b85e023040",
   "metadata": {},
   "source": [
    "### 2.2 Questions about AGNES (4 Points)\n",
    "\n",
    "**2.2 a) Picking the Right Linkage Method (2 Points)**\n",
    "\n",
    "Assume your dataset contains the geolocations of traffic accidents on Singapore expressways over the time span of a year. Using AGNES, we want to find sections of the expressways where traffic accidents are particularly common.\n",
    "\n",
    "**Which Linkage Methods covered in the lecture is most suitable for this task?** Briefly explain your choice!\n",
    "\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393fc684-9220-47c5-a652-f136d4e14ad1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b688c7a1-9f63-465b-b53c-d419ec2244cb",
   "metadata": {},
   "source": [
    "**2.2 b) Outlier Detection with AGNES (2 Points)**\n",
    "\n",
    "In contrast to DBSCAN, AGNES has no explicit concept of noise points for outlier detection. But an AGNES clustering still gives meaningful insights into a dataset.\n",
    "\n",
    "**How can the results of AGNES be used to identify outliers in a data?** Briefly explain your answer!\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e8e87-af1f-47a0-8906-cc8aad541243",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a95c9d3e-b19c-47a8-ad5c-48415a4043f2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c5071-c895-46e6-b8dd-031ccd6a462a",
   "metadata": {},
   "source": [
    "## 3 Association Rule Mining\n",
    "\n",
    "Association rule mining is a data mining technique used to discover interesting relationships, patterns, or associations in large datasets. It primarily focuses on identifying frequent patterns, often in the form of rules, within transactional or relational databases. This technique is commonly applied in various fields, including market basket analysis in retail, web usage mining, healthcare, and more. Here's how association rule mining works:\n",
    "\n",
    "* **Transaction Data:** Association rule mining typically starts with a dataset that represents transactions or events. Each transaction consists of a set of items, and these transactions are often stored in a database.\n",
    "\n",
    "* **Itemsets:** An itemset is a collection of one or more items or attributes from the dataset. For example, in a retail context, an itemset could be a list of products purchased together in a single transaction.\n",
    "\n",
    "* **Support:** Support is a measure of the frequency with which an itemset appears in the dataset. It is defined as the proportion of transactions that contain the itemset. High support indicates that the itemset is a common pattern in the data.\n",
    "\n",
    "* **Confidence:** Confidence is a measure of how often the rule is found to be true. It is defined as the proportion of transactions that contain both the antecedent and the consequent. High confidence suggests a strong association between the antecedent and consequent.\n",
    "\n",
    "* **Lift:** Lift is a measure of how much more likely the consequent is given the antecedent compared to its expected occurrence by chance. A lift value greater than 1 suggests a positive association, while a value less than 1 suggests a negative or weak association.\n",
    "\n",
    "* **Association Rules:** Association rules are typically expressed as \"if-then\" statements. They describe relationships between itemsets. Each rule consists of two parts:\n",
    "    * Antecedent (Left-hand side, LHS): This is the itemset that represents the condition or premise of the rule.\n",
    "    * Consequent (Right-hand side, RHS): This is the itemset that represents the result or conclusion of the rule.\n",
    "\n",
    "The association rule mining process involves finding itemsets that meet a minimum support threshold and then generating rules based on those itemsets. Rules are evaluated based on their confidence and lift, and interesting or meaningful rules are selected for further analysis or action. For example, in a retail context, association rule mining might reveal that customers who purchase items like bread and milk (antecedent) are highly likely to also purchase butter (consequent). This insight can be valuable for marketing and inventory management, as it suggests items that can be promoted together or placed in close proximity in the store. Association rule mining algorithms, such as the Apriori algorithm, are commonly used to efficiently discover and analyze these patterns in large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1929c4b3-489e-4001-87e5-c23cc78d7bb7",
   "metadata": {},
   "source": [
    "#### Toy Dataset\n",
    "\n",
    "The following dataset with 5 transactions and 6 different items is directly taken from the lecture slides. This should make it easier to test your implementation. The format is a list of tuples, where each tuple represents the set of items of an individual transaction. This format can also be used as input for the `efficient-apriori` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a82b32-535f-4df6-89d8-e49b5a6ee20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_demo = [\n",
    "    ('bread', 'yogurt'),\n",
    "    ('bread', 'milk', 'cereal', 'eggs'),\n",
    "    ('yogurt', 'milk', 'cereal', 'cheese'),\n",
    "    ('bread', 'yogurt', 'milk', 'cereal'),\n",
    "    ('bread', 'yogurt', 'milk', 'cheese')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713910f-65d1-4da4-b049-f75301f4c706",
   "metadata": {},
   "source": [
    "#### Auxiliary Methods\n",
    "\n",
    "We want you to focus on the Apriori algorithm. So we provide you with a set of auxiliary functions. Feel free to look at their implementation in the file `data/utils.py`.\n",
    "\n",
    "Given a set of items, `powerset()` returns all possible subsets of items with a specified minimum and maximum length. For example, you can use this method to generate all itemsets for a transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5dd054-7bdc-4746-94f0-e94941573fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset in powerset(('c', 'b', 'a'), min_len=1, max_len=3):\n",
    "    print(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db4a4a-566a-45e7-ba79-6a478d9632f6",
   "metadata": {},
   "source": [
    "The method `unique_items()` returns all the unique items across all transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794486a8-894c-47dd-a400-527bb3266d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_items(transactions_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca351d-7126-4a3f-9ca7-34bd788420a9",
   "metadata": {},
   "source": [
    "The method `support()` calculates and returns the support for a given itemset and set of transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368bf3ed-681c-4669-afa4-cf8737f56ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "support(transactions_demo, ('bread', 'milk'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8e9eed-b589-46cf-936c-a826b49cdde2",
   "metadata": {},
   "source": [
    "The method `confidence()` calculates and returns the confidence for a given association rules and set of transactions. An association rule is represented by a 2-tuple, where the first element represents itemset X and the second element represents items Y (i.e., $X \\Rightarrow Y$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1082a-5224-43e4-bae2-b55aca6466ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence(transactions_demo, (('bread',), ('milk',)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1925e37a-a56e-44db-824e-7da4ebb17b4b",
   "metadata": {},
   "source": [
    "The method `generate_association_rules()` calculates and returns all possible association rules given an itemset. The result is a list of association rules, each association rule represented as 2-tuple (see above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e7321-0b4a-4b98-a4c6-737c058c5951",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_association_rules(('bread', 'milk', 'cereal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94851b47-5af7-4191-9d5c-10e70b27e55f",
   "metadata": {},
   "source": [
    "The method `merge_itemsets()` merges two given itemsets into one itemset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82104d31-cf24-4506-8d50-106f0acea107",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_itemsets(('bread', 'milk'), ('bread', 'eggs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9be2a59-8c36-4377-997d-e46392af6bdd",
   "metadata": {},
   "source": [
    "For your implementation, you can make use of these auxiliary methods wherever you see fit. And that is, of course, strongly recommended, as it makes the programming task much easier. So, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4e4166-a4ba-48f5-a59d-3e68ee37a77b",
   "metadata": {},
   "source": [
    "### 3.1 Brute-Forcing Association Rule Mining (5 Points)\n",
    "\n",
    "The most naive approach for mining association rules would be to generate all possible rules and check if their support and confidence exceed the specified thresholds `minsup` and `minconf`. In the lecture, you have learned that, given $d$ unique items in a dataset of transactions, there are $3^d - 2^{d+1} + 1$ possible rules.\n",
    "\n",
    "**Proof that $d$ unique items result in $3^d - 2^{d+1} + 1$ possible rules!** (Hint: Write out all possible rules for $d = 2, 3, 4, ...$ items; you should quickly spot the pattern that will allow you to validate the formula).\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf453112-c20d-407a-bdf9-381597c40dc8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8872fc7-ce0c-4e3c-8414-f53e203c1d9a",
   "metadata": {},
   "source": [
    "### 3.2 Implementing Apriori Algorithm for Finding Frequent Itemsets (10 Points)\n",
    "\n",
    "The underlying goal of the Apriori algorithm is to avoid calculating the support of itemsets for which we can say ahead of time (i.e., \"a-priori\") that they cannot be frequent itemsets. Calculating the support is the most expensive part as it requires going through all transactions in our database.\n",
    "\n",
    "In the following task, you will implement the Apriori algorithm for finding Frequent Itemsets. In more detail, you will implement the $F_{k-1}\\times F_1$ method to compute the $L_k$ candidate itemsets given the $F_{k-1}$ frequent itemsets and the $F_1$ frequent itemsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f8ffd-057b-4954-9905-4a42cb1ffce1",
   "metadata": {},
   "source": [
    "#### 3.2 a) Generating Candidate Itemsets $L_{k}$ (5 Points)\n",
    "\n",
    "The $F_{k-1}\\times F_1$ method used the frequent itemset sets of size ($k-1$) and of size $1$ to derive the set $L_k$ of candidate itemsets. The important part is the pruning, e.g., the removal of all k-itemset for which we can already tell that they won't have sufficient support.\n",
    "\n",
    "**Hint:** Instead of first creating all possible k-itemsets and then performing the pruning to yield the final set $L_k$, check each k-itemset immediately in the loop if it should indeed be added to $L_k$ or not. This will make your code a bit simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedf1692-a3d5-48ba-b628-3e76afc19fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Lk(k_minus_one_itemsets, one_itemsets):\n",
    "    \n",
    "    # Initialize as set to avoid duplicates\n",
    "    Lk = set()\n",
    "    \n",
    "    # Nested loop over both itemsets to find all combinations\n",
    "    for k_minus_one_itemset in k_minus_one_itemsets:\n",
    "        for one_itemset in one_itemsets:\n",
    "            \n",
    "            #########################################################################################\n",
    "            ### Your code starts here ###############################################################              \n",
    "            \n",
    "\n",
    "                \n",
    "            ### Your code ends here #################################################################\n",
    "            #########################################################################################                  \n",
    "                \n",
    "    return Lk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3c22fe-f6d6-47eb-a461-dce5f2f717fc",
   "metadata": {},
   "source": [
    "For testing your implementation, we manually crafted a k-itemset ($k=2$) and 1-itemset in line with the lecture example, meaning that the result should match the example on the lecture slides as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b81039-8f0d-4be3-9e1e-5ec4e2e22e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_minus_one_itemsets = {\n",
    "    ('bread', 'cereal'), ('bread', 'milk'), ('bread', 'yogurt'), ('cereal', 'milk'),\n",
    "    ('cereal', 'yogurt'), ('cheese', 'milk'), ('cheese', 'yogurt'), ('milk', 'yogurt')\n",
    "}\n",
    "\n",
    "one_itemsets = {('bread',), ('cereal',), ('cheese',), ('milk',), ('yogurt',)}\n",
    "\n",
    "Lk = generate_Lk(k_minus_one_itemsets, one_itemsets)\n",
    "\n",
    "for itemset in Lk:\n",
    "    print(itemset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6598c719-5bf9-45f5-96c2-4d06ef21348a",
   "metadata": {},
   "source": [
    "#### 3.2 b) Generating Frequent Itemsets $F_{1}$, $F_{2}$, $F_{3}$, ... (5 Points)\n",
    "\n",
    "Once we have a candidate set L of a given iteration, we can directly derive the frequent itemsets by removing all candidates with insufficient support. As this is a trivial filter step, we give you this method `generate_F` for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a893e64-a4cf-40cc-9ee8-07d01d1a2140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Fk(Lk, transactions, minsup):\n",
    "    return set([ s for s in Lk if support(transactions, s) >= minsup])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc25fe75-0963-43b4-b260-dc95fef9e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fk = generate_Fk(Lk, transactions_demo, 0.4)\n",
    "\n",
    "for itemset in Fk:\n",
    "    print(itemset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad35a9a9-4538-4005-b9d4-c60ac949314b",
   "metadata": {},
   "source": [
    "**Note:** Just as a quick reminder, this filter step is actually the most costly part of the algorithms since we have to calculate the support for each candidate itemset which requires us to go through each transaction. For our toy dataset, this doesn't really matter, but the whole goal of the Apriori algorithm is to minimize the number of access to the datasets.\n",
    "\n",
    "With the 2 methods `generate_L_kplus1` and `generate_F`, you now have everything to find all frequent itemsets using the Apriori algorithm. To this end, complete the method `find_frequent_itemsets_apriori` below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d58578-4ca6-4497-9d47-eaecaafc94fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_frequent_itemsets_apriori(transactions, minsup):\n",
    "    # The 1-itemsets are just all unique items across all transactions\n",
    "    items = unique_items(transactions)\n",
    "    \n",
    "    ############################################################################################\n",
    "    ### Your code starts here ##################################################################\n",
    "    \n",
    "\n",
    "    \n",
    "    ### Your code ends here ####################################################################\n",
    "    ############################################################################################\n",
    "    \n",
    "    # Initialize dictionary with all current frequent itemsets for each size k\n",
    "    # Example format: { 1: {(a), (b), (c)}, 2: {(a, c), ...} }\n",
    "    F = { 1: F_1 }\n",
    "    \n",
    "    for k in range(2, len(one_itemsets)+1):\n",
    "\n",
    "        Fk = set()\n",
    "        \n",
    "        ########################################################################################\n",
    "        ### Your code starts here ##############################################################\n",
    "\n",
    "\n",
    "                \n",
    "        ### Your code ends here ################################################################\n",
    "        ########################################################################################\n",
    "                \n",
    "        F[k] = Fk    \n",
    "\n",
    "    # Merge the dictionary of k-itemsets to a single set and return it\n",
    "    # Example: {1: {(a), (b), (c)}, 2: (a, c)} => {(a), (b), (c), (a, c)}\n",
    "    return set.union(*[ itemsets for k, itemsets in F.items() ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e6af7f-5e82-472b-91d8-f3bc08496589",
   "metadata": {},
   "source": [
    "Let's test your implementation using the code cell below. Of course, the result should match the brute force approach (assuming the same value for input parameter `minsup`; default: `0.4` to match the example in the lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890706d3-048a-43a0-bda1-cc234890be6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = find_frequent_itemsets_apriori(transactions_demo, 0.4)\n",
    "\n",
    "for itemset in F:\n",
    "    print(itemset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b31eb5-ae84-41b7-ba5c-daac2d9d7a0d",
   "metadata": {},
   "source": [
    "#### From Frequent Itemsets to Association Rules\n",
    "\n",
    "Your implementation so far gives you the Frequent Itemsets in a list of transactions using the Apriori method. This step is typically the most time-consuming one in Association Rule Mining. However, we still have to do the second step and find all Association Rules given the Frequent Itemsets. We saw in the lecture that this can also be done in an efficient manner using the Apriori method to avoid checking all rules.\n",
    "\n",
    "Since this step is typically less computationally expensive, we simply do it the naive way -- that is, we go over all Frequent Itemsets, and check for Frequent Itemset and check which of the Association Rules that can be generated from it has a sufficiently high confidence. With all the auxiliary methods we provide, this becomes trivial to implement, so we simply give you the method `find_association_rules()` below. Note how it uses your implementation of `frequent_itemsets_apriori()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394667f5-fedd-4800-9d78-a4390b062d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_association_rules(transactions, min_support, min_confidence):\n",
    "    # Initialize empty list of association rules\n",
    "    association_rules = []\n",
    "    \n",
    "    # Find and loop over all frequent itemsets\n",
    "    for itemset in find_frequent_itemsets_apriori(transactions, min_support):\n",
    "        if len(itemset) == 1:\n",
    "            continue\n",
    "\n",
    "        # Find and loop over all association rules that can be generated from the itemset\n",
    "        for r in generate_association_rules(itemset):\n",
    "            # Check if the association rule fulfils the confidence requriement\n",
    "            if confidence(transactions, r) >= min_confidence:\n",
    "                association_rules.append(r)\n",
    "                \n",
    "    # Return final list of association rules\n",
    "    return association_rules\n",
    "\n",
    "find_association_rules(transactions_demo, 0.6, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ba75c3-dfb0-4c3e-93b3-8ccdbd6c8b8c",
   "metadata": {},
   "source": [
    "If everything is correct, for the default values for `min_support` and `min_confidence`, the one Association Rules that should be returned is $\\{cereal\\}\\Rightarrow \\{milk\\}$ (in Python represented as a tuple of 2 tuples, left-hand side and right-hand side)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d69694-1260-4e26-8374-79cf830ec11a",
   "metadata": {},
   "source": [
    "#### Comparison with `efficient-apriori` package\n",
    "\n",
    "You can run the apriori algorithm over the demo data to check if your implementation is correct. Try different values for the parameters `min_support` and `min_confidence` and compare the results. Note that the order of the returned association rules might differ between your implementation and the apriori one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c6cd8-1d5d-4024-baed-24eeba3a2174",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, rules = apriori(transactions_demo, min_support=0.6, min_confidence=1.0, max_length=4)\n",
    "\n",
    "for r in rules:\n",
    "    print('Rule [{} => {}] (support: {}, confidence: {}, lift: {})'.format(r.lhs, r.rhs, r.support, r.confidence, r.lift))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60528c4a-2237-49be-8e73-b6097f414761",
   "metadata": {},
   "source": [
    "### 3.3 Association Rule Mining over Real-World Datasets\n",
    "\n",
    "For the last task, we look into real-world datasets to find interesting Association Rules. As those datasets are already quite large -- although not really large -- we only rely only on the `efficient-apriori` package for performance reasons. To make things even easier for you, we provide the method `show_top_rules` which computes the Association Rules using the `efficient-apriori` package, but (a) filters the rules w.r.t. to the right-hand side (optional), (b) sorts the rules w.r.t. the specified metric, and (c) shows only the top-k rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fffaea-8098-4043-b2c1-14d8967ba785",
   "metadata": {},
   "source": [
    "#### Loading the Data\n",
    "\n",
    "For the first dataset, we consider a typical [Market Basket Analysis](https://www.kaggle.com/yugagrawal95/market-basket-analysis-apriori-in-python/data?select=Market_Basket_Optimisation.csv) dataset containing the set of items people have purchased in a supermarket. This dataset is very similar to the running example used throughout the lecture -- well, just larger :), with ~7,500 transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a764b32-02a1-4708-8703-ef087e7cd8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_groceries = []\n",
    "\n",
    "with open('data/a2-groceries.csv') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        transactions_groceries.append(line.split(','))\n",
    "\n",
    "print('Number of transactions overall: {}'.format(len(transactions_groceries)))\n",
    "print()\n",
    "print('Some example transactions:')\n",
    "for t in transactions_groceries[:5]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9956e-dcb7-4bb9-85ef-4b318f74924e",
   "metadata": {},
   "source": [
    "As the second dataset, use the [Coronavirus Disease 2019 (COVID-19) Clinical Data Repository](https://covidclinicaldata.org/) to find Association Rules that might tell as, which symptoms are most indicative of a COVID-19 infections. We already downloaded, cleaned, and prepared the dataset for you. The dataset file `data/a1-covid-symptoms-result.csv` contains over 710k transactions. Each transaction is a set of $0..n$ symptoms and $1$ test result label (\"POSITIVE\" and \"NEGATIVE\"). For example a line in the file can look like `runny_nose sore_throat fatigue POSITIVE`. Note that a line might also be just `NEGATIVE` or `POSITIVE` in case a person was tested without any symptoms. Feel free to take a look at the file -- looking at the raw data is always a good first step when it comes to data mining.\n",
    "\n",
    "Note that we generate 2 lists of transactions which we both use later:\n",
    "* `transactions_covid_all` contains all 710k+ transactions in the dataset\n",
    "* `transactions_covid_pos` contains all 11k+ transactions with a \"POSITIVE\" test result label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f24726e-7359-48ea-9479-555212b361a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_covid_all = []\n",
    "transactions_covid_pos = []\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "with open('data/a2-covid-symptoms-result.csv') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if 'POSITIVE' in line:\n",
    "            transactions_covid_pos.append(line.split(' '))\n",
    "        \n",
    "        transactions_covid_all.append(line.split(' '))\n",
    "\n",
    "print('Number of transactions overall: {}'.format(len(transactions_covid_all)))\n",
    "print('Number of \"POSITIVE\" transactions: {}'.format(len(transactions_covid_pos)))\n",
    "print()\n",
    "print('Some example transactions:')\n",
    "for t in transactions_covid_pos[:5]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315b9174-f19c-416e-b3db-7da8524b52bf",
   "metadata": {},
   "source": [
    "#### Finding Association Rules\n",
    "\n",
    "Now that we have the data, we can try to find interesting Association Rules in both datasets. To this end, we have prepared a series of code cells to run the Apriori Algorithm implemented by the `efficient-apriori` package over both datasets with different parameters. Each run is labeled A-D; the question at the may make references to some of these individual runs.\n",
    "\n",
    "**Note:** There is nothing for you to implement here, you can just run the code cells! However, for the questions below, you can always change the different parameters (e.g., `min_support`, `min_confidence`) to see their effects in the result as well as the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef6b703-370b-4cbe-872c-4988d1f6aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run A\n",
    "show_top_rules(transactions_covid_all, min_support=0.001, min_confidence=0.2, k=5, sort='lift', rhs='POSITIVE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377cc9aa-19ee-46d1-b0d2-e65bca2e4c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run B\n",
    "show_top_rules(transactions_covid_pos, min_support=0.15, min_confidence=0.8, k=5, sort='lift', rhs='POSITIVE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608cd9c5-c04f-4f08-ad9c-ed0daef988a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run C\n",
    "show_top_rules(transactions_groceries, min_support=0.001, min_confidence=0.2, k=5, sort='lift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb777c-01fa-43de-bb55-3d6ed0428c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run D\n",
    "show_top_rules(transactions_groceries, min_support=0.01, min_confidence=0.2, k=5, sort='lift')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09b2e16-5ab2-45dd-8cde-9c428036ac4f",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d0fad-1624-408b-b764-77c3dfe505c5",
   "metadata": {},
   "source": [
    "**Question 3.3 a) (3 Points)** From your observation, what are the effects of increasing/reducing `min_support` and `min_confidence`? Support your answer with evidence. You can perform more runs of efficient-apriori with different parameter settings, if needed.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4999cc3-f962-4cdf-8460-5b6c895ab3e1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7c4cf70-2ff1-44d8-b171-2210395bfc6d",
   "metadata": {},
   "source": [
    "**Question 3.3 b) (3 Points)** Runs A and B return association rules with symptoms on the left-hand side and a POSITIVE test result on the right-hand side. As such both runs find rules which (combination of) symptoms are most indicative of a positive test result. However, the results of Run A and B are rather different. Explain the differences and discuss which result provides more reliable insights!\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70021f6-ee28-4b8d-be61-0e0cc612c089",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f772c97-1c63-4899-8057-761390eaacbd",
   "metadata": {},
   "source": [
    "**Question 3.3 c) (2 Points)** Runs A and C roughly return the same number of rules in roughly the similar amount of time. However, there are almost 10 times more transactions in the COVID dataset (Run A) compared to the groceries dataset (Run C). Explain why the result and performance are very similar despite the very different number of transactions.\n",
    "\n",
    "**Important:** The exact runtimes might vary noticeable depending on your PC/laptop. So if the runtimes of both runs are not similar for you, please simply assume that the two runtimes are similar!\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8deddc-a43c-4b19-8d33-b633524bd5a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a67e2ad9-2525-4cba-a8fa-5c012f14adee",
   "metadata": {},
   "source": [
    "**Question 3.3 d) (2 Points)** Have a look at the confidence and lift of the top rules for Run A and B. What noticeable differences can you observe and how can you explain these differences?\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45047a8-929e-430a-9eb0-dd5ed9e11af9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a94584c-9786-461d-8a35-290468e44fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
