{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763f0fa0-6584-44d4-b163-a891252769f9",
   "metadata": {},
   "source": [
    "# CS5228 Assignment 3 - Tree-Based Models\n",
    "\n",
    "Hello everyone, this assignment notebook covers Tree-Based Models. There are some code-completion tasks and question-answering tasks in this answer sheet. For code completion tasks, please write down your answer (i.e., your lines of code) between sentences that \"Your code starts here\" and \"Your code ends here\". The space between these two lines does not reflect the required or expected lines of code. For answers in plain text, you can refer to [this Markdown guide](https://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd) to customize the layout (although it shouldn't be needed).\n",
    "\n",
    "When you work on this notebook, you can insert additional code cells (e.g., for testing) or markdown cells (e.g., to keep track of your thoughts). However, before the submission, please remove all those additional cells again. Thanks!\n",
    "\n",
    "**Important:** \n",
    "* Rename and save this Jupyter notebook as **cs5228_a3_YourName_YourNUSNETID.ipynb** (e.g., **cs5228_a3_BobSmith_e12345678.ipynb**) before submission!\n",
    "* Rename and save the script file *cs5228_a3_script.py* as **cs5228_a3_YourName_YourNUSNETID.py** (e.g., **cs5228_a3_BobSmith_e12345678.py**) before submission!\n",
    "* Submission deadline is Oct 26, 11.59 pm. Late submissions will be penalized by 10% for each additional day. Failure to appropriately rename both files will yield a penalty of 1 Point. There is no need to use you full name if its a rather long; it's just  important to easily identify you in Canvas etc.\n",
    "\n",
    "Please also add your NUSNET and student id in the code cell below. This is just to make any identification of your notebook doubly sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "119a3b10-7afe-4814-9a79-32310842f1c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T03:27:02.373918300Z",
     "start_time": "2023-10-23T03:27:02.313764600Z"
    }
   },
   "outputs": [],
   "source": [
    "student_id = 'A0285647M'\n",
    "nusnet_id = 'e1216292'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7455d1c-31ad-4620-8d8a-ad2adc363f56",
   "metadata": {},
   "source": [
    "Here is an overview over the tasks to be solved and the points associated with each task. The notebook can appear very long and verbose, but note that a lot of parts provide additional explanations, documentation, or some discussion. The code and markdown cells you are supposed to complete are well, but you can use the overview below to double-check that you covered everything.\n",
    "\n",
    "* **1 Bagging and Feature Sampling (10 Points)**\n",
    "    * 1.1 Implement Bootstrapping (2 Points)\n",
    "    * 1.2 Implement Feature Sampling (2 Points)\n",
    "    * 1.3 Comparing Bagging and Bagging+FeatureSampling (6 Points)\n",
    "* **2 Implementing AdaBoost with Decision Stumps (30 Points)**\n",
    "    * 2.1 Implementing a Decision Stump Classifier (12 Points)\n",
    "        * 2.1 a) Calculating the Gini Score a Single Node (2 Points)\n",
    "        * 2.1 b) Calculating the Gini Score for a Split (2 Points)\n",
    "        * 2.1 c) Training the Decision Stump: Finding the Best Split (5 Points)\n",
    "        * 2.1 d) Predicting the Classes (3 Points)\n",
    "    * 2.2 Implementing AdaBoost (12 Points)\n",
    "        * 2.2 a) Training the Gradient-Boosted Regressor (8 Points)\n",
    "        * 2.2 b) Predicting Output Values (4 Points)\n",
    "    * 2.3 Questions about AdaBoost (6 Points)\n",
    "        * 2.3 a) Question 1 (2 Points)\n",
    "        * 2.3 b) Question 2 (4 Points)\n",
    "* **3 Evaluation of Tree-Based Models (10 Points)**\n",
    "    * 3.1 Data Preprocessing (2 Points)\n",
    "    * 3.2 Basic K-Fold Cross Validation (8 Points)\n",
    "        * 3.2a) Comparing Tree-Based Regression Models (5 Points)\n",
    "        * 3.2b) Assessing the Evaluation (3 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a4e6d6-30f8-4722-9750-4986c33fcccd",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Enable Auto-Reload\n",
    "\n",
    "This ensures that any saved changes to your `.py` file gets automatically reloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65641534-c0db-491c-8f9a-475ba94a5c6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T03:27:02.373918300Z",
     "start_time": "2023-10-23T03:27:02.334031100Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad660cb-6a44-4906-8756-d18ec238bf09",
   "metadata": {},
   "source": [
    "### Enable \"Inline Plotting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8067a9c2-d8d6-4cb5-b994-07fdc2c51b17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T03:27:03.028570400Z",
     "start_time": "2023-10-23T03:27:02.363654500Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bdd634-a20d-491b-9339-49eab3ab7205",
   "metadata": {},
   "source": [
    "### Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ca2bb72-8aa3-4d53-9d2a-e4953aa02592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T03:27:05.033942300Z",
     "start_time": "2023-10-23T03:27:03.028570400Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score, mean_squared_error\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befecf0d-9ea3-4355-abb3-6cc7b1daa7ec",
   "metadata": {},
   "source": [
    "**Important:** This notebook also requires you to complete in a separate `.py` script file. This keeps this notebook cleaner and simplifies testing your implementations for us. As you need to rename the file `cs5228_a3.py`, you also need to edit the import statement below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c0827e5-fcc6-42d4-93df-96258a07cec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T03:27:05.097578Z",
     "start_time": "2023-10-23T03:27:05.033942300Z"
    }
   },
   "outputs": [],
   "source": [
    "from cs5228_a3_ParasharaRamesh_e1216292 import *\n",
    "#from cs5228_a3_BobSmith_e12345678 import get_noise_dbscan # <-- you will need to rename this accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ccaa8-0df8-4b2c-9676-0f872ec645d8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e65ca2e-36ac-4d5e-aa5d-b5c5e39ba1a4",
   "metadata": {},
   "source": [
    "## 1 Bagging and Feature Sampling (10 Points)\n",
    "\n",
    "In the lecture, we discussed the limitations of individual Decision Trees, which motivated the notion of Tree Ensembles. In a nutshell, a Tree Ensemble trains multiple Decision Trees within the same classifier and regressor to reduce variance and improve accuracy. The first approach towards creating Tree Ensembles was to train multiple Decision Trees over different samples of the data:\n",
    "\n",
    "* **Bagging (Bootstrap Aggregation):** Sample a new dataset $D_i$ sampled from $D$ uniformly and with replacement ($|D_i| = |D|$)\n",
    "* **Feature Sampling:** For a given dataset $D$ with $d$ features, consider only a random subset of features of size $m$ with $m<d$.\n",
    "\n",
    "Combining Bagging and Feature Sampling is the underlying idea of *Random Forests*. In this task, you will explore the effects of Bagging and Bagging+FeatureSampling\n",
    "\n",
    "We use the very basic [IRIS](https://archive.ics.uci.edu/ml/datasets/iris) dataset: it's small and clean, and has only numerical features. The dataset contains 3 classes of 50 instances each, where each class refers to a type of iris plant.\n",
    "\n",
    "### Prepare Example Data\n",
    "\n",
    "#### Load Data from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a94584c-9786-461d-8a35-290468e44fdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T03:27:05.184054100Z",
     "start_time": "2023-10-23T03:27:05.097578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   sepal_length  sepal_width  petal_length  petal_width  species\n0           5.1          3.5           1.4          0.2        0\n1           4.9          3.0           1.4          0.2        0\n2           4.7          3.2           1.3          0.2        0\n3           4.6          3.1           1.5          0.2        0\n4           5.0          3.6           1.4          0.2        0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal_length</th>\n      <th>sepal_width</th>\n      <th>petal_length</th>\n      <th>petal_width</th>\n      <th>species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/a3-iris.csv')\n",
    "\n",
    "# Convert the species name to numerical categories 0, 1, 2\n",
    "df['species'] = pd.factorize(df['species'])[0]\n",
    "\n",
    "# Show the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5be1d5-ec05-4775-b65b-49072c231f5e",
   "metadata": {},
   "source": [
    "#### Convert to NumPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69cfc1fb-102b-4daf-be5e-39c5d8196d35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T03:27:05.304176700Z",
     "start_time": "2023-10-23T03:27:05.176911500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (150, 4)\n",
      "Shape of y: (150,)\n"
     ]
    }
   ],
   "source": [
    "data = df.to_numpy()\n",
    "\n",
    "X = data[:,0:4]\n",
    "y = data[:,4].astype(int)\n",
    "\n",
    "print('Shape of X: {}'.format(X.shape))\n",
    "print('Shape of y: {}'.format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3644e33f-34d9-4925-9955-66fb240ce08e",
   "metadata": {},
   "source": [
    "### 1.1 Implement Bootstrapping (2 Points)\n",
    "\n",
    "Implement method `create_boostrap_sample()` to generate a bootstrap sample for a given dataset! The input dataset is represented by feature array `X` and array `y` containing the class labels (classification) or output values (regression). Hint: numpy provides some convenient methods to make this a very simple task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61eaec19-c816-44a7-af9c-3f50d8661b4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T03:27:05.304176700Z",
     "start_time": "2023-10-23T03:27:05.239864700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_bootstrap: (150, 4)\n",
      "Shape of y_bootstrap: (150,)\n"
     ]
    }
   ],
   "source": [
    "def create_bootstrap_sample(X, y):\n",
    "    N, d = X.shape\n",
    "    \n",
    "    X_bootstrap, y_bootstrap = None, None\n",
    "    \n",
    "    #########################################################################################\n",
    "    ### Your code starts here ###############################################################\n",
    "\n",
    "    sampled_indices = np.random.choice(range(N), size=N, replace=True)\n",
    "    num_unique_elements = len(np.unique(sampled_indices))\n",
    "    # print(f\"No of unique samples is {num_unique_elements}\")\n",
    "    X_bootstrap = X[sampled_indices]\n",
    "    y_bootstrap = y[sampled_indices]\n",
    "\n",
    "    ### Your code ends here #################################################################\n",
    "    #########################################################################################\n",
    "    \n",
    "    return X_bootstrap, y_bootstrap\n",
    "\n",
    "\n",
    "X_bootstrap, y_bootstrap = create_bootstrap_sample(X, y)\n",
    "\n",
    "print('Shape of X_bootstrap: {}'.format(X_bootstrap.shape))\n",
    "print('Shape of y_bootstrap: {}'.format(y_bootstrap.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e2b42-0489-4e61-93da-ba054196ab83",
   "metadata": {},
   "source": [
    "The shapes of `X_bootstrap` and `y_bootstrap` should of course be the same as the shapes of `X` and `y`, but containing randomly selected samples. If you need to convince yourself, you can also print some elements of `X` to see if they are different between runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a2a40-da63-47ff-b950-739a2314dd00",
   "metadata": {},
   "source": [
    "### 1.2 Implement Feature Sampling (2 Points)\n",
    "\n",
    "Implement the method `perform_feature_sampling()`! The input is feature array `X`; use the common approach introduced in the lecture for calculating the number of sampled features -- that is, the number of sample features $m = \\lceil\\sqrt{d}\\rceil$. Apart from the new dataset `X_sample` the method also returns the *indices* of the selected features; we need those for the next task. Hint: Again, numpy should be your best friend here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5742cc84-b8c4-4d83-a75f-20e798a52b19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T03:27:05.415381600Z",
     "start_time": "2023-10-23T03:27:05.304176700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_sampled: (150, 2)\n",
      "Selected indices: [3 1]\n"
     ]
    }
   ],
   "source": [
    "def perform_feature_sampling(X):\n",
    "    N, d = X.shape\n",
    "    \n",
    "    X_feature_sampled, selected_indices = None, None\n",
    "    \n",
    "    #########################################################################################\n",
    "    ### Your code starts here ###############################################################    \n",
    "    m = np.ceil(np.sqrt(d)).astype(int)\n",
    "    # print(f\"m is {m}\")\n",
    "\n",
    "    selected_indices = np.random.choice(d, size=m, replace=False)\n",
    "\n",
    "    X_feature_sampled = X[:, selected_indices]\n",
    "\n",
    "    ### Your code ends here #################################################################\n",
    "    #########################################################################################    \n",
    "    \n",
    "    return X_feature_sampled, selected_indices\n",
    "    \n",
    "X_sampled, selected_indices = perform_feature_sampling(X)\n",
    "\n",
    "print('Shape of X_sampled: {}'.format(X_sampled.shape))\n",
    "print('Selected indices: {}'.format(selected_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2311868-b59e-4de0-8114-e3563b0146b9",
   "metadata": {},
   "source": [
    "`X_sampled` has to contain the same number of data samples as `X`, but with less features than `X`. The number of selected indices should of course be reflected in the shape of `X`. For example, if the shape of `X` is $(n, m)$, then there should be $m$ selected indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f61fc-378a-4792-86ac-32765abe5857",
   "metadata": {},
   "source": [
    "### 1.3 Comparing Bagging and Bagging+FeatureSampling (6 Points)\n",
    "\n",
    "Intuitively, different sampled dataset will yield different Decision Trees, not only regarding the accuracy, but also how the Decision Trees will \"look like\". In the following, we train a set of Decision Trees (using the Decision Tree implementation from `sklearn`) based on different dataset samples.\n",
    "\n",
    "In the code cell below, we use our implementations of the auxiliary methods `create_boostrap_sample()` and `perform_feature_sampling()` to train a series of Decision Trees (i.e., a Tree Ensemble) using only Bagging as well as using Bagging + Feature Sampling. In the output, *root index* is the index of the feature used for the very first split, and *#nodes* reflects the total number of nodes in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df219876-bcc2-4ccb-ba3f-6c06603b4f9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T03:27:09.914006500Z",
     "start_time": "2023-10-23T03:27:08.969777700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging only\t\t\t\t\tBagging + Feature Sampling\n",
      "root index: 3,  #nodes: 15\t\troot index: 2,  #nodes: 15\n",
      "root index: 2,  #nodes: 15\t\troot index: 0,  #nodes: 57\n",
      "root index: 2,  #nodes: 15\t\troot index: 3,  #nodes: 29\n",
      "root index: 2,  #nodes: 15\t\troot index: 3,  #nodes: 25\n",
      "root index: 2,  #nodes: 15\t\troot index: 2,  #nodes: 21\n",
      "root index: 3,  #nodes: 15\t\troot index: 3,  #nodes: 27\n",
      "root index: 2,  #nodes: 9\t\troot index: 3,  #nodes: 17\n",
      "root index: 3,  #nodes: 15\t\troot index: 3,  #nodes: 21\n",
      "root index: 3,  #nodes: 13\t\troot index: 2,  #nodes: 21\n",
      "root index: 3,  #nodes: 7\t\troot index: 3,  #nodes: 17\n",
      "root index: 2,  #nodes: 15\t\troot index: 3,  #nodes: 19\n",
      "root index: 3,  #nodes: 15\t\troot index: 0,  #nodes: 61\n",
      "root index: 2,  #nodes: 9\t\troot index: 2,  #nodes: 15\n",
      "root index: 2,  #nodes: 13\t\troot index: 3,  #nodes: 19\n",
      "root index: 2,  #nodes: 13\t\troot index: 3,  #nodes: 15\n",
      "root index: 3,  #nodes: 13\t\troot index: 2,  #nodes: 17\n",
      "root index: 2,  #nodes: 19\t\troot index: 3,  #nodes: 17\n",
      "root index: 2,  #nodes: 11\t\troot index: 3,  #nodes: 17\n",
      "root index: 3,  #nodes: 13\t\troot index: 2,  #nodes: 29\n",
      "root index: 3,  #nodes: 15\t\troot index: 3,  #nodes: 13\n"
     ]
    }
   ],
   "source": [
    "# We need to set the seed as the sampling is random, and we want to ensure consistent results\n",
    "np.random.seed(0)\n",
    "\n",
    "print(\"Bagging only\\t\\t\\t\\t\\tBagging + Feature Sampling\")\n",
    "\n",
    "for _ in range(20):\n",
    "    # Create a new bootstrap sample (we can use the same for both ensembles)\n",
    "    X_t, y_t = create_bootstrap_sample(X, y)\n",
    "    classifier_bagging = DecisionTreeClassifier().fit(X_t, y_t)\n",
    "        \n",
    "    # Perform feature sampling on bootstrap sample\n",
    "    X_t_fs, selected_indices = perform_feature_sampling(X_t)\n",
    "    classifier_sampling = DecisionTreeClassifier().fit(X_t_fs, y_t)\n",
    "    \n",
    "    # Print core features of trained Decision Tree\n",
    "    # (feature index of root node, total of number in Decision Trr)\n",
    "    print(f\"root index: {classifier_bagging.tree_.feature[0]},  #nodes: {classifier_bagging.tree_.node_count}\\t\\t\"\n",
    "          f\"root index: {selected_indices[classifier_sampling.tree_.feature[0]]},  #nodes: {classifier_sampling.tree_.node_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645db041-fd10-4816-b656-edc571968081",
   "metadata": {},
   "source": [
    "**Interpret the result!** When comparing the resulting Decision Trees when using only **Bagging** and **Bagging+FeatureSampling** you must have observed several differences. List all your observations together with a brief explanation for the observed difference. What insights into the dataset can you gain from your observations?\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "* In Bagging the root indices are typically only either 2 or 3, whereas in the Bagging + feature sampling case the indices vary between 0,2 & 3.\n",
    "* The number of nodes in Bagging is most often around 15 , but takes values 7,9,11,13,19 occasionally. Whereas the number of nodes in the Bagging + feature sampling case shows a lot of variety & complexity (sometimes being as high as 61)\n",
    "* What this means is that Bagging generally tends to construct decision trees with uniform tree structures and similar node counts. Whereas, Bagging along with feature sampling gives rise to more complex, diverse decision trees resulting in varying root indices and structures\n",
    "* From the number of nodes present it is clear that bagging with feature sampling can handle more complex, high dimensional data with diverse feature importance due to the different root indices being chosen, whereas there is not much variation with just bagging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5413b8a-3fef-4cb6-99ff-f88e6dcee636",
   "metadata": {},
   "source": [
    "## 2 Implementing a AdaBoost with Decision Stumps (30 Points)\n",
    "\n",
    "AdaBoost (Adaptive Boosting) is an ensemble learning method that combines the predictions of multiple base learners to improve overall classification performance. In AdaBoost, the base learners are typically weak learners, which are models that perform slightly better than random guessing (e.g., Decision Stumps). Here's a brief overview of how AdaBoost works using Decision Trees as base learners for classification tasks:\n",
    "\n",
    "* **Initialization:** Assign equal weights to all training samples. These weights determine the importance of each sample in the training process.\n",
    "\n",
    "* **Iterative Phase:**\n",
    "\n",
    "    * *Base Learner Training:* Train a base learner (usually a Decision Stump) on the training data. It tries to minimize the weighted classification error, giving more weight to misclassified samples.\n",
    "\n",
    "    * *Weighted Error Calculation:* Calculate the weighted classification error of the base learner. This error is the sum of the weights of misclassified samples.\n",
    "\n",
    "    * *Classifier Weight Calculation:* Assign a weight to the base learner based on its performance. The better the performance, the higher the weight. This weight is used to determine the contribution of the base learner's prediction in the final ensemble.\n",
    "\n",
    "    * *Update Sample Weights:* Increase the weights of misclassified samples so that they become more important in the next iteration. This focuses the subsequent base learners on the samples that are harder to classify correctly.\n",
    "\n",
    "    * *Ensemble Building:* Combine the predictions of all base learners, weighted by their individual classifier weights, to obtain the final ensemble prediction.\n",
    "\n",
    "* **Final Prediction:** The final prediction is made by aggregating the weighted predictions of all base learners.\n",
    "\n",
    "AdaBoost's strength lies in its ability to focus on the difficult-to-classify examples, allowing it to improve performance even with weak base learners. This makes it particularly effective in situations where a single base learner might struggle. Keep in mind that while AdaBoost is powerful, it's important to be cautious about overfitting. AdaBoost can overfit if the base learners are too complex or if the number of iterations is too high. Therefore, it's advisable to monitor the performance on a validation set and potentially use techniques like early stopping or limiting the complexity of base learners.\n",
    "\n",
    "Your last task will be to implement an AdaBoost Classifier using Decision Stumps as covered in the lecture. But not to worry, this may only sound more difficult than it actually is, and we will guide you through this process step by step. We also keep things simple by assuming that all input features are numerical values.\n",
    "\n",
    "Fundamentally, we can split the implementation into 2 subtasks.\n",
    "\n",
    "* **Weak Learner:** You first implement the simplest \"Decision Stump Classifier\", i.e., a Decision Tree with only one split and therefore a height of 1. This means we do not have to care about the recursive splitting of nodes; it's not complicated but would only add tedious coding.\n",
    "* **AdaBoost:** With the Decision Stump Classifier in place, you can implement AdaBoost as shown in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12989e24-9943-4799-9d0e-0cd36c21448e",
   "metadata": {},
   "source": [
    "### Prepare Dataset\n",
    "\n",
    "Again, we use the [IRIS](https://archive.ics.uci.edu/ml/datasets/iris) dataset here.\n",
    "\n",
    "#### Load Dataset from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca04abbf-91c6-4eb3-b06d-75e882e35ad2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T03:27:10.443979800Z",
     "start_time": "2023-10-23T03:27:10.379139200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   sepal_length  sepal_width  petal_length  petal_width  species\n0           5.8          2.8           5.1          2.4        2\n1           6.0          2.2           4.0          1.0        1\n2           5.5          4.2           1.4          0.2        0\n3           7.3          2.9           6.3          1.8        2\n4           5.0          3.4           1.5          0.2        0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal_length</th>\n      <th>sepal_width</th>\n      <th>petal_length</th>\n      <th>petal_width</th>\n      <th>species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.8</td>\n      <td>2.8</td>\n      <td>5.1</td>\n      <td>2.4</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6.0</td>\n      <td>2.2</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5.5</td>\n      <td>4.2</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7.3</td>\n      <td>2.9</td>\n      <td>6.3</td>\n      <td>1.8</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.4</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "df_iris = pd.read_csv('data/a3-iris.csv')\n",
    "# Convert the 3 string class labels to 0, 1, and 2\n",
    "df_iris.species = df_iris.species.factorize()[0]\n",
    "df_iris = df_iris.sample(frac=1).reset_index(drop=True)\n",
    "# Show sample of dataset\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea35f1-368b-445f-9615-d4a55bdd633f",
   "metadata": {},
   "source": [
    "#### Convert Dataframe to NumPy arrays + Split into Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bda2a2fe-25e2-45bc-997d-270bafb821bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T03:27:10.554071700Z",
     "start_time": "2023-10-23T03:27:10.443979800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (30, 4)\n"
     ]
    }
   ],
   "source": [
    "X = df_iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].to_numpy()\n",
    "y = df_iris['species'].to_numpy().squeeze()\n",
    "\n",
    "training_size = int(0.8 * X.shape[0])\n",
    "\n",
    "X_train, y_train = X[:training_size], y[:training_size]\n",
    "X_test, y_test = X[training_size:], y[training_size:]\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be8cea0-007b-471d-b04d-9ab4eab864cd",
   "metadata": {},
   "source": [
    "### 2.1 Implementing a Decision Stump Classifier (12 Points)\n",
    "\n",
    "A Decision Stump is nothing else but a Decision Tree with typically only very few splits -- well, more generally, a Decision Tree with a (very) small height, but we keep it simple here. This means that we consider the smallest Decision Stump consisting of only a single split. This means that there is no need to continue recursively splitting child nodes like for a (full) Decision Tree trained to be a Strong Learner.\n",
    "\n",
    "For finding the best split, we need two main things\n",
    "* A scoring method to quantify how good a split is.\n",
    "* A method to actually find the best split (using the scoring method).\n",
    "\n",
    "You can find the skeleton code for the class `DecisionStumpClassifier` implementing the Decision Stump Classifier in the imported `py` file. You will need to complete this code step by step along with the subtask 2.1 a-d)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a64eae8-efa2-4921-813e-6ac1f9ebce35",
   "metadata": {},
   "source": [
    "#### 2.1 a) Calculating the Gini Score of a Single Node (2 Points)\n",
    "\n",
    "Recall from the lecture, that the Gini score of a node $t$ is defined as:\n",
    "\n",
    "$$Gini(t) = 1 - \\sum_{c\\in C} P(c|t)^2$$\n",
    "\n",
    "where $C$ is the set of classes, and $P(c|t)$ is the relative frequency of class $c$ in node $t$.\n",
    "\n",
    "**Implement this formula in the method `calc_gini_score_node()`!** Hint: Have a look at [`np.unique`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) and basic `numpy` methods such as [`np.sum`](https://numpy.org/doc/stable/reference/generated/numpy.sum.html) and [`np.square`](https://numpy.org/doc/stable/reference/generated/numpy.square.html) to make your life easier. You can use the example calls below to test your implementation of the method. The comments indicate the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52d1dc5d-f32f-4bda-9b8a-d284b7aaa52e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T03:27:11.058777900Z",
     "start_time": "2023-10-23T03:27:10.995362700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.5\n",
      "0.625\n"
     ]
    }
   ],
   "source": [
    "stump = DecisionStumpClassifier()\n",
    "\n",
    "y1 = np.array([1, 1, 1, 1, 1, 1, 1, 1])\n",
    "y2 = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "y3 = np.array([2, 0, 1, 1, 2, 2, 0, 2])\n",
    "\n",
    "print(stump.calc_gini_score_node(y1)) # 0.0\n",
    "print(stump.calc_gini_score_node(y2)) # 0.5\n",
    "print(stump.calc_gini_score_node(y3)) # 0.625"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d5dbb-393a-4d50-8283-65e070e01732",
   "metadata": {},
   "source": [
    "#### 2.1 b) Calculating the Gini Score for a Split (2 Points)\n",
    "\n",
    "In the lecture, we defined the impurity of a split as the average of the impurities of the child nodes, weighted by their size (in terms of the number of samples in each child node). Since we only consider binary splits (2 child nodes) and consider only the Gini score to measure impurity, the Gini score of a split simplifies to:\n",
    "\n",
    "$$Gini(t_{left}, t_{right}) = \\frac{n_{left}}{n}Gini(t_{left}) + \\frac{n_{right}}{n}Gini(t_{right})$$\n",
    "\n",
    "where $n_{left}$ ($n_{right}$) is the number of samples in the left (right) child node; and $n = n_{left} + n_{right}$\n",
    "\n",
    "**Implement this formula in the method `calc_gini_score_split`!** You obviously can and should use the existing method `calc_gini_score_node()`. You can use the example calls below to test your implementation of the method. The comments indicate the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec2c5d09-e78e-458e-ab5f-2ad413eb38fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T03:27:11.455846500Z",
     "start_time": "2023-10-23T03:27:11.393868700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.25\n",
      "0.3125\n"
     ]
    }
   ],
   "source": [
    "stump = DecisionStumpClassifier()\n",
    "\n",
    "print(stump.calc_gini_score_split(y1, y1))  # 0.0\n",
    "print(stump.calc_gini_score_split(y1, y2))  # 0.25\n",
    "print(stump.calc_gini_score_split(y1, y3))  # 0.3125"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c975fd-2552-4071-b96f-73b0a69c44d0",
   "metadata": {},
   "source": [
    "#### 2.1 c) Training the Decision Stump: Finding the Best Split (5 Points)\n",
    "\n",
    "With the means to calculate the Gini score for an arbitrary split, we can now train our Decision Stump Classifier. Recall, that our Decision Tree will only have a height of 1 as we only need to make 1 split.\n",
    "\n",
    "**Implement method `fit()`** to find the best split with respect to all features and corresponding thresholds. You obviously can and should use of the existing method `calc_gini_score_split()`. The skeleton code of method `fit()` already provides with the nested loop that goes through all features and the respective thresholds. Note that we keep it simple here as we use all unique values of a features as candidate thresholds.\n",
    "\n",
    "You can use the example calls below to test your implementation of the method. The comments indicate the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "385322ba-a8ac-4a48-84c9-4194b38a15e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T03:27:11.874130600Z",
     "start_time": "2023-10-23T03:27:11.794095100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of best feature: 2\n",
      "Best threshold: 2.45\n"
     ]
    }
   ],
   "source": [
    "stump = DecisionStumpClassifier().fit(X_train, y_train)\n",
    "\n",
    "print(\"Index of best feature:\", stump.feature_idx)\n",
    "print(\"Best threshold:\", stump.threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5cd9b-d17e-4fe5-be7c-56626a9da316",
   "metadata": {},
   "source": [
    "The output of previous cell should be:\n",
    "\n",
    "```\n",
    "Index of best feature: 2\n",
    "Best threshold: 2.45\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d231267-03e5-48da-bbf7-ce8f77e4796b",
   "metadata": {},
   "source": [
    "#### 2.1 d) Predicting the Class Labels (3 Points)\n",
    "\n",
    "After training our Decision Stump Classifier, we now only need to implement the last step -- that is, the prediction of the class labels for new data samples. Again, since we only have 1 split, this step is also rather easy to implement.\n",
    "\n",
    "**Implement method `predict()`** to predict the class labels for a given set of data samples. Hint: Have again a look at [`np.unique`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) and and [`np.argmax`](https://numpy.org/doc/stable/reference/generated/numpy.square.html) to make your life easier. You can use the example calls below to test your implementation of the method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e44f9595-a004-447c-9324-4cc93a07069a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T03:27:12.458836Z",
     "start_time": "2023-10-23T03:27:12.383910100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 2. 0. 0. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 0. 2. 2. 2. 2. 0.\n",
      " 0. 0. 2. 2. 2. 0.]\n"
     ]
    }
   ],
   "source": [
    "stump = DecisionStumpClassifier().fit(X_train, y_train)\n",
    "\n",
    "print(stump.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c39a12-e5bf-4305-8979-b6ce9e3ca3ed",
   "metadata": {},
   "source": [
    "The output of previous cell should be:\n",
    "\n",
    "```\n",
    "[0. 2. 0. 0. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 0. 2. 2. 2. 2. 0.\n",
    " 0. 0. 2. 2. 2. 0.]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8748b62f-e16f-4698-a558-f82f0a2bad01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T03:27:13.665417800Z",
     "start_time": "2023-10-23T03:27:13.596019600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score of your Decision Tree implementation on the toy dataset is 0.540\n"
     ]
    }
   ],
   "source": [
    "y_pred = stump.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('The f1 score of your Decision Tree implementation on the toy dataset is {:.3f}'.format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d52a989-c417-4e66-95b0-eb46f2737911",
   "metadata": {},
   "source": [
    "The resulting f1 score should be **0.540**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e6c48-4509-4fee-bf3e-c6248323a0de",
   "metadata": {},
   "source": [
    "**Testing your Implementation on the IRIS Dataset.** This part is only for you to test your implementation on a real-world dataset (IRIS) since the toy dataset might not reveal all bugs in your code. You can also directly compare the result your Decision Stump implementation with the results from scikit-learn's [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html); Of course, we need to set `max_depth=1` to make it a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eb8a27b-29d9-4a8e-94c7-e8c7fbcb45b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T03:27:14.533738900Z",
     "start_time": "2023-10-23T03:27:14.469978500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score of your Decision Tree implementation on the IRIS dataset is 0.540\n",
      "The f1 score of sklearn Decision Tree implementation on the IRIS dataset is 0.540\n"
     ]
    }
   ],
   "source": [
    "my_stump = DecisionStumpClassifier().fit(X_train, y_train)\n",
    "sk_stump = DecisionTreeClassifier(max_depth=1).fit(X_train, y_train)\n",
    "\n",
    "my_y_pred = my_stump.predict(X_test)\n",
    "sk_y_pred = sk_stump.predict(X_test)\n",
    "\n",
    "my_f1 = f1_score(y_test, my_y_pred, average='macro')\n",
    "sk_f1 = f1_score(y_test, sk_y_pred, average='macro')\n",
    "\n",
    "print('The f1 score of your Decision Tree implementation on the IRIS dataset is {:.3f}'.format(my_f1))\n",
    "print('The f1 score of sklearn Decision Tree implementation on the IRIS dataset is {:.3f}'.format(sk_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c61b97-eac9-47b2-81d9-3f8082572c0d",
   "metadata": {},
   "source": [
    "You should see an f1 score of **0.540** using both your implementation as well as the one from scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6b0839-d1df-40a5-8e15-fa5dfe538d5f",
   "metadata": {},
   "source": [
    "## 2.2 Implementing AdaBoost (12 Points)\n",
    "\n",
    "AdaBoost is a very popular ensemble technique that trains a series of *Weak Learners* to make predictions. Although AdaBoost is a generic technique, it is very commonly used with Decision Stumps as Weak Learners, since Decision Trees with a limited maximum height make naturally good Weak Learners.\n",
    "\n",
    "Again, we provide you with a skeleton code for the class implementing the AdaBoost Classifier. We call it `AdaBoostTreeClassifier` to avoid naming conflicts with `AdaBoostClassifier` of scikit-learn, and because we limit ourselves to Decision Trees (well, Stumps) as the estimators (i.e., the Weak Learners)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9409b98-9379-4b5a-bd39-c1fc38a4dae0",
   "metadata": {},
   "source": [
    "#### 2.1 a) Training the AdaBoost Classifier (8 Points)\n",
    "\n",
    "In the lecture, we went step by step through the training process of an AdaBoost classifier. We saw that this process comprises multiple but rather straightforward steps.\n",
    "\n",
    "\n",
    "**Implement method `fit()`** to train your `AdaBoostTreeClassifier`. The skeleton code of method `fit()` allows you to focus on the core steps within each iteration for training the next Weak Learner (here, our Decision Stump). To help you a little bit, we list the main 4 steps and give you the first step -- training the next estimator using the current dataset sample -- for free.\n",
    "\n",
    "**Important:** By default, `AdaBoostTreeClassifier` uses your implementation of `DecisionStumpClassifier` as its Weak Learner. In case you had problems implementing `DecisionStumpClassifier` or you simply want to test the results, you can also use scikit-learn's `DecisionTreeClassifier`. To make this change, just use the commented line under Step 1 to train the Weak Learner.\n",
    "\n",
    "You can use the example calls below to test your implementation of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32a73cd4-2e7a-40d2-be68-3b76bef889ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T03:27:15.917198Z",
     "start_time": "2023-10-23T03:27:15.819504700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alpha values -- i.e., the amount-of-says -- are:\n",
      "[0.36544375 0.54110924 0.61476166 0.64369347 0.50502969]\n"
     ]
    }
   ],
   "source": [
    "# We need to set the seed as the sampling is random\n",
    "np.random.seed(0)\n",
    "\n",
    "adaboost = AdaBoostTreeClassifier(n_estimators=5).fit(X_train, y_train)\n",
    "\n",
    "# adaboost_from_skl = AdaBoostClassifier(n_estimators=5).fit(X_train,y_train)\n",
    "\n",
    "print(\"The alpha values -- i.e., the amount-of-says -- are:\")\n",
    "print(adaboost.alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56104f15-a1bb-4775-b178-559df6fad71c",
   "metadata": {},
   "source": [
    "The output of previous cell should be:\n",
    "\n",
    "```\n",
    "The alpha values -- i.e., the amount-of-says -- are:\n",
    "[0.36544375 0.54110924 0.65028309 0.65364937 0.53673646]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31473312-a14c-4ad4-b7a5-40914965af8d",
   "metadata": {},
   "source": [
    "#### 2.2 b) Predicting the Class Labels (5 Points)\n",
    "\n",
    "As the last step, we now only need our AdaBoost classifier to predict the class labels for unseen data samples. Again, we saw in the lecture how this works: For each data sample, we check which of the `n_estimators` estimators predicts a certain class label, and the sum of all the alphas (i.e., the amounts of say) of the estimators of the same class.\n",
    "\n",
    "The skeleton code of the `AdaBoostTreeClassifier` already provides a method `predict()` which takes a list of data samples as input and calls the method `predict_sample()` to predict the class label for each data sample individually. It's not that difficult to do this completely vectorized without the loop over the data samples, but here we want to focus on the basic algorithm and not worry about performance.\n",
    "\n",
    "**Implement method predict_sample()** to predict the class label for a given data sample. Hint: Have again a look at [np.argwhere](https://numpy.org/doc/stable/reference/generated/numpy.argwhere.html) to maybe make your life easier. You can use the example calls below to test your implementation of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f037f2b0-2dbb-444a-9064-e9bf287e443c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T03:27:17.020476200Z",
     "start_time": "2023-10-23T03:27:16.924119700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 0 0 2 0 2 1 1 1 2 2 1 1 0 1 2 2 0 1 1 2 1 0 0 0 2 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "# We need to set the seed as the sampling is random\n",
    "np.random.seed(0)\n",
    "\n",
    "adaboost = AdaBoostTreeClassifier(n_estimators=5).fit(X_train, y_train)\n",
    "\n",
    "print(adaboost.predict(X_test))\n",
    "\n",
    "#TODO.x the results are wrong here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5f3b08-9358-475b-819a-fd54330b45bb",
   "metadata": {},
   "source": [
    "The output of previous cell should be:\n",
    "\n",
    "```\n",
    "[0 2 0 0 2 0 2 1 1 1 2 2 1 2 0 1 2 2 0 1 1 2 1 0 0 0 2 1 2 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bdb6f2-f184-419c-a271-f307da7f6fbd",
   "metadata": {},
   "source": [
    "**Testing your Implementation on the IRIS Dataset.** We the code cell below, you can again directly compare the result your AdaBoost implementation with the scikit-learn [`AdaBoostClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html). Two things to note:\n",
    "\n",
    "* By default, `AdaBoostClassifier` uses the `DecisionTreeClassifier` class as the estimators (i.e., the Weak Learners) with `max_depth=1`\n",
    "\n",
    "* By default, `AdaBoostClassifier` sets `n_estimators=50`; we therefore choose the same default value for `AdaBoostTreeClassifier`\n",
    "\n",
    "As a result, we do not have to set any parameters for `AdaBoostClassifier` and can use the default ones to allow for a fair comparison with your implementation of `AdaBoostTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b52fc85-d2cf-4a3f-9b8b-11a25034bc87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T03:27:18.316123400Z",
     "start_time": "2023-10-23T03:27:17.853988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score of your AdaBoost implementation on the IRIS dataset is 0.933\n",
      "The f1 score of the sklearn AdaBoost implementation on the IRIS dataset is 0.933\n"
     ]
    }
   ],
   "source": [
    "my_adaboost = AdaBoostTreeClassifier().fit(X_train, y_train)\n",
    "sk_adaboost = AdaBoostClassifier().fit(X_train, y_train)\n",
    "\n",
    "my_y_pred = my_adaboost.predict(X_test)\n",
    "sk_y_pred = sk_adaboost.predict(X_test)\n",
    "\n",
    "my_f1 = f1_score(y_test, my_y_pred, average='macro')\n",
    "sk_f1 = f1_score(y_test, sk_y_pred, average='macro')\n",
    "\n",
    "print('The f1 score of your AdaBoost implementation on the IRIS dataset is {:.3f}'.format(my_f1))\n",
    "print('The f1 score of the sklearn AdaBoost implementation on the IRIS dataset is {:.3f}'.format(sk_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f14028b-e090-47cf-870a-2b11e3645b8e",
   "metadata": {},
   "source": [
    "You should see an f1 score of **0.933** using both your implementation as well as the one from scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6649193e-eb67-4fac-88d0-b7450aff1a17",
   "metadata": {},
   "source": [
    "### 2.3 Questions about AdaBoost (6 Points)\n",
    "\n",
    "Assume you use your implementation of `AdaBoostTreeClassifier` to train a binary classifier of the dataset shown in 1.1 b).\n",
    "\n",
    "![](./data/2.3a.png)\n",
    "\n",
    "**2.3 a) Question (2 Points):** Will the binary classifier be able to achieve a training error (not test error!) of 0? Explain your answer! (Your explanation is more important than a simple Yes/No answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a596b-add5-4acf-aa83-3755f6c4583c",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "In the case of a regular decision tree we have seen that we need at-most only 8 splits to perfectly separate out each point ( i.e. training error of 0). In the case of my implementation AdaBoostTreeClassification, it has around 50 decision stump estimators each of which has a maximum depth of 1.\n",
    "\n",
    "Since my Adaboost internally uses my DecisionStump implementation which tries to find out all possible thresholds across all features (which are just the x & y coordinate points in this case), it is fair to say that each decision stump given some data it might either choose to split this dataset either using a single \"horizontal line separator\" or single \"vertical line seperator\".\n",
    "\n",
    "Adaboost basically tries to re-classify misclassified entries of the previous estimator by ensuring that those datapoints are picked again (with replacement) in a probabilistic manner. This approach for this particular dataset has a high chance of giving a training error of 0 because we will keep trying to correctly classify all points once the previous estimator(s) chooses a particular split.\n",
    "\n",
    "Since each decision stump by itself cannot overfit the training data to give a training error of 0, the hope is that an ensemble of them together can correctly classify each point correctly\n",
    "\n",
    "For this particular dataset, here is how a sample adaboost fitting might work.\n",
    "\n",
    "Estimator 1: gives a vertical split between column 5 & 6 , classifying all points to the left as \"blue\" due to majority points being blue, and assigning all points to the right as \"red\" ( even though there are equal red and blue points)\n",
    "Estimator 2: It samples majority of the datapoints which were misclassified and comes up with another single split to try and reduce that error ( say a horizontal line in between row 3&4). Now the misclassified points from this are weighed again\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "Estimator 50: Yet another single horizontal or vertical line is predicted based on the sampled datapoints using the weight of the previous estimator's misclassification weights.\n",
    "\n",
    "Once we find all the splits, each of the 50 estimators have their own \"say\" values which can influence the final classification prediction.\n",
    "\n",
    "Since 50 is high enough number ( much greater than 7 or 8 for example which were the number of splits needed in a regular decision tree), it seems likely that the combination of all of these estimators should be able to correctly classify all points. Even in case 50 estimators does not do the trick, increasing the number estimators even further (say to 100) will definitely ensure that the training error can go to 0.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678d899b-c099-4f05-b769-2d3d5c66fd59",
   "metadata": {},
   "source": [
    "**2.3 b) True/False Questions (4 Points):** In the table below are 4 statement that are either *True* or *False*. Complete the table to specify whether a statement is *True* or *False*, and provide a brief explanation for your answer (Your explanation is more important than a simple True/False answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b080cf-91f9-477c-95cf-3827fbf96294",
   "metadata": {},
   "source": [
    "This is a markdown cell. Please fill in your answers for (1)~(4).\n",
    "\n",
    "| No. | Statement                                                                                               \t| True or False?   \t | Brief Explanation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
    "|-----|------------------------------------------------------------------------------------------------------------|--------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| 1   | AdaBoost usually performs better than Random Forests when the dataset contains mislabeled data points | False              | Even though both are ensemble models, adaboost is mainly a boosting algorithm trying to correctly classify misclassified points therefore it is very sensitive to noise and outliers. Random forests however is not as sensitive to noise due to the higher reduction in variance achieved across its various decision stumps                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
    "| 2   | The error rate $\\epsilon_m$ of the Adaboost classifier always decreases from one iteration to the next. | False              | Even though Adaboost aims to \"boost\" the misclassified points, it may not always decrease the error rate from iteration to iteration. Since each estimator is a decision stump it might so be the case that the error rate increases slightly for this particular estimator even though we sample the datapoints from the weights of the previous estimator. Since this is also very sensitive to noise, it might stumble upon a particular outlier data point during one of the iterations with one of its estimators causing the error value to increase. Therefore, we cannot guarantee that the error rate always decreases.                                                                                                                                                                                                                  |\n",
    "| 3   | Assume an error rate of $\\epsilon_m \\leq 0.2$ in iteration $m$. This means that up to 20% of the data samples have been misclassified | False              | Assuming we have 100 datapoints, in the first iteration all weights will be 1. Since error is calculated as the $\\Sigma_i^Nw_i.indicator_function(x_i)$ after 'm' iterations it might be the case that weights are a lot different where some points have more weight and other points have lesser weight. Therefore getting a value 0.2 with these new weights does not imply that upto 20% of the samples have been misclassified. If a point is repeatedly being misclassified it might have a high weight which contributes more to the error value of 0.2 so if he have such 'high' error datapoints we can get a value of 0.2 with a lot less number of misclassfied samples. A similar argument can be made on the other end of the spectrum as well as a lot of datapoints together could also contribute to the value of 0.2 in totality |\n",
    "| 4   | If after running AdaBoost the last Weak Learner does not misclassify and training samples, additional iterations could still help reduce errors on unseen data  | False              | If the last 'weak' learning is able to perfectly classify all of the training samples it indicates that the model has 'overfit' on the training data. This does not mean that the model will perform better on unseen data and is not indicative of its generalizability. It could also be the case that the model has learnt to predict even outliers correctly as the adaboost algorithm is sensitive to noise. This would mean that testing on unseen data actually leads to a worse performance.                                                                                                                                                                                                                                                                                                                                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2ac1ea-25f6-4d20-84d3-b3cec2f15d23",
   "metadata": {},
   "source": [
    "## 3 Evaluating Tree-Based Models\n",
    "\n",
    "In this last part, we look into evaluation different tree-based models using k-fold cross validationa. K-fold cross-validation is a technique used in machine learning to assess the performance and generalization ability of a model. It involves dividing the dataset into K subsets (or \"folds\") of equal size. The model is trained on K-1 of these folds and tested on the remaining one. This process is repeated K times, with each fold used as the test set exactly once. The final performance metric is computed by averaging the results from each iteration. K-fold cross-validation helps ensure that the model's performance is consistent across different subsets of the data, reducing the risk of overfitting or underfitting.\n",
    "\n",
    "### Prepare Dataset\n",
    "\n",
    "#### Load Dataset from File\n",
    "\n",
    "We use a [WHO Life Expectancy](https://www.kaggle.com/kumarajarshi/life-expectancy-who) dataset for this task. Note that we cleaned the dataset for you (i.e., there are no dirty records in there)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "643c9a75-171d-40e5-b060-eb6da2a5e5cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T05:38:08.992627400Z",
     "start_time": "2023-10-23T05:38:08.843056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Year      Status  Adult Mortality  infant deaths  Alcohol  \\\n0  2015  Developing            263.0             62     0.01   \n1  2014  Developing            271.0             64     0.01   \n2  2013  Developing            268.0             66     0.01   \n3  2012  Developing            272.0             69     0.01   \n4  2011  Developing            275.0             71     0.01   \n\n   percentage expenditure  Hepatitis B  Measles   BMI  under-five deaths  ...  \\\n0               71.279624         65.0     1154  19.1                 83  ...   \n1               73.523582         62.0      492  18.6                 86  ...   \n2               73.219243         64.0      430  18.1                 89  ...   \n3               78.184215         67.0     2787  17.6                 93  ...   \n4                7.097109         68.0     3013  17.2                 97  ...   \n\n   Total expenditure  Diphtheria  HIV/AIDS         GDP  Population  \\\n0               8.16        65.0       0.1  584.259210  33736494.0   \n1               8.18        62.0       0.1  612.696514    327582.0   \n2               8.13        64.0       0.1  631.744976  31731688.0   \n3               8.52        67.0       0.1  669.959000   3696958.0   \n4               7.87        68.0       0.1   63.537231   2978599.0   \n\n   thinness  1-19 years  thinness 5-9 years  Income composition of resources  \\\n0                  17.2                17.3                            0.479   \n1                  17.5                17.5                            0.476   \n2                  17.7                17.7                            0.470   \n3                  17.9                18.0                            0.463   \n4                  18.2                18.2                            0.454   \n\n   Schooling  Life expectancy  \n0       10.1             65.0  \n1       10.0             59.9  \n2        9.9             59.9  \n3        9.8             59.5  \n4        9.5             59.2  \n\n[5 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Year</th>\n      <th>Status</th>\n      <th>Adult Mortality</th>\n      <th>infant deaths</th>\n      <th>Alcohol</th>\n      <th>percentage expenditure</th>\n      <th>Hepatitis B</th>\n      <th>Measles</th>\n      <th>BMI</th>\n      <th>under-five deaths</th>\n      <th>...</th>\n      <th>Total expenditure</th>\n      <th>Diphtheria</th>\n      <th>HIV/AIDS</th>\n      <th>GDP</th>\n      <th>Population</th>\n      <th>thinness  1-19 years</th>\n      <th>thinness 5-9 years</th>\n      <th>Income composition of resources</th>\n      <th>Schooling</th>\n      <th>Life expectancy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2015</td>\n      <td>Developing</td>\n      <td>263.0</td>\n      <td>62</td>\n      <td>0.01</td>\n      <td>71.279624</td>\n      <td>65.0</td>\n      <td>1154</td>\n      <td>19.1</td>\n      <td>83</td>\n      <td>...</td>\n      <td>8.16</td>\n      <td>65.0</td>\n      <td>0.1</td>\n      <td>584.259210</td>\n      <td>33736494.0</td>\n      <td>17.2</td>\n      <td>17.3</td>\n      <td>0.479</td>\n      <td>10.1</td>\n      <td>65.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2014</td>\n      <td>Developing</td>\n      <td>271.0</td>\n      <td>64</td>\n      <td>0.01</td>\n      <td>73.523582</td>\n      <td>62.0</td>\n      <td>492</td>\n      <td>18.6</td>\n      <td>86</td>\n      <td>...</td>\n      <td>8.18</td>\n      <td>62.0</td>\n      <td>0.1</td>\n      <td>612.696514</td>\n      <td>327582.0</td>\n      <td>17.5</td>\n      <td>17.5</td>\n      <td>0.476</td>\n      <td>10.0</td>\n      <td>59.9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2013</td>\n      <td>Developing</td>\n      <td>268.0</td>\n      <td>66</td>\n      <td>0.01</td>\n      <td>73.219243</td>\n      <td>64.0</td>\n      <td>430</td>\n      <td>18.1</td>\n      <td>89</td>\n      <td>...</td>\n      <td>8.13</td>\n      <td>64.0</td>\n      <td>0.1</td>\n      <td>631.744976</td>\n      <td>31731688.0</td>\n      <td>17.7</td>\n      <td>17.7</td>\n      <td>0.470</td>\n      <td>9.9</td>\n      <td>59.9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2012</td>\n      <td>Developing</td>\n      <td>272.0</td>\n      <td>69</td>\n      <td>0.01</td>\n      <td>78.184215</td>\n      <td>67.0</td>\n      <td>2787</td>\n      <td>17.6</td>\n      <td>93</td>\n      <td>...</td>\n      <td>8.52</td>\n      <td>67.0</td>\n      <td>0.1</td>\n      <td>669.959000</td>\n      <td>3696958.0</td>\n      <td>17.9</td>\n      <td>18.0</td>\n      <td>0.463</td>\n      <td>9.8</td>\n      <td>59.5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2011</td>\n      <td>Developing</td>\n      <td>275.0</td>\n      <td>71</td>\n      <td>0.01</td>\n      <td>7.097109</td>\n      <td>68.0</td>\n      <td>3013</td>\n      <td>17.2</td>\n      <td>97</td>\n      <td>...</td>\n      <td>7.87</td>\n      <td>68.0</td>\n      <td>0.1</td>\n      <td>63.537231</td>\n      <td>2978599.0</td>\n      <td>18.2</td>\n      <td>18.2</td>\n      <td>0.454</td>\n      <td>9.5</td>\n      <td>59.2</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows  21 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/a3-life-expectancy-cleaned.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acef995-4e5c-43d4-91d7-2d3ce24a73b0",
   "metadata": {},
   "source": [
    "#### Separate Features & Target\n",
    "\n",
    "For your convenience, we split the dataframe into two, one containing the input features, the other containing the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6b13268-9ac1-4990-b96e-89b3c4589487",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T08:05:57.660340900Z",
     "start_time": "2023-10-23T08:05:57.488501700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 1649 samples with 20 features\n"
     ]
    }
   ],
   "source": [
    "df_X = df.iloc[:,0:-1]\n",
    "df_y = df.iloc[:,-1]\n",
    "\n",
    "num_samples, num_features = df_X.shape\n",
    "\n",
    "print('The dataset contains {} samples with {} features'.format(num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "0    65.0\n1    59.9\n2    59.9\n3    59.5\n4    59.2\nName: Life expectancy, dtype: float64"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T08:21:21.950156Z",
     "start_time": "2023-10-23T08:21:21.753437200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50b4070af6874d2ab799f7bae327c41e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d024c0313d642b18ecd15b1a6b89eda"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cbd4f476c70545d09d8fcdb98fd393f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2fa7dff136a64357a2cab6cfc0bf115d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "#using pandas profiling to check this particular dataframe\n",
    "train_profile = ProfileReport(df)\n",
    "train_profile.to_file(f\"who_report.html\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T08:20:06.177621300Z",
     "start_time": "2023-10-23T08:17:18.011815500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Data Preprocessing (2 Points)\n",
    "\n",
    "As usual, the first step is data preprocessing (informed by an EDA). As mentioned above, there's not much to do as this dataset does not contain any \"dirty\" records, particularly, there are no NA values in any of the columns/features. As such, there should be no need to remove any samples.\n",
    "\n",
    "**Perform and data preprocessing/transformation steps you deem appropriate!** As it might affect your decision, the data will be used to train different tree-based models (recall: the tree-based classifiers of sklearn do not support categorical features!). Note that some preprocessing steps might be easier to perform on the pandas dataframe while others on the NumPy arrays. This is why we provide 2 code cells, but it's up to which one to use.\n",
    "\n",
    "**Note:** Perform only preprocessing steps that are indeed needed, and briefly(!) explain your decision by commenting your code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da0cabed-075b-4289-a503-102b6b384a4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T08:57:28.997894600Z",
     "start_time": "2023-10-23T08:57:28.782120900Z"
    }
   },
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "\n",
    "#TODO.1. Status column in X is categorical and has 2 classes 'Developing' & 'Developed'. Categorically encode this\n",
    "# Perform one-hot encoding\n",
    "df_X = pd.get_dummies(df_X, columns=['Status'])\n",
    "\n",
    "#TODO.2 should we just normalize all numerical features? ( perhaps needed only for the adaboostrefressor!?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb2fbae2-44cb-44ee-9868-9978dcd155dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T08:57:31.371397800Z",
     "start_time": "2023-10-23T08:57:31.164219900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert dataframes to numpy arrays\n",
    "X, y = df_X.to_numpy(), df_y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dec6ae36-908c-46af-839e-39c0946f1d14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T08:57:32.433209900Z",
     "start_time": "2023-10-23T08:57:32.243479200Z"
    }
   },
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbf90b6-e701-4c0a-8acd-c0067c1a4ecc",
   "metadata": {},
   "source": [
    "### 3.2 Basic K-Fold Cross Validation\n",
    "\n",
    "The code cell below performs K-Fold Cross Validation. Note that we use `X` and `y` here, and assume our true test data for the final evaluation of the model(s) is a separate dataset. Since we only perform validation here, we can ignore the test data.\n",
    "\n",
    "The code cell below allows you to train a `DecisionTreeRegressor`, a `RandomForestRegressor`, or a `GradientBoostingRegressor` (all `sklearn` implementations). You only need to remove the comment before the regressor of choice, and comment the 2 other regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91afc3b3-73ea-4004-bbed-e40b73d1b01b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T08:57:39.153105800Z",
     "start_time": "2023-10-23T08:57:33.835214900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param = 1, RSME training = 6.0 (0.1), RSME validation = 6.4 (0.9)\n",
      "param = 2, RSME training = 4.5 (0.1), RSME validation = 5.1 (0.8)\n",
      "param = 3, RSME training = 3.4 (0.1), RSME validation = 4.0 (0.8)\n",
      "param = 5, RSME training = 2.4 (0.1), RSME validation = 3.3 (0.4)\n",
      "param = 8, RSME training = 1.5 (0.1), RSME validation = 3.6 (0.6)\n",
      "param = 10, RSME training = 1.0 (0.1), RSME validation = 3.7 (0.5)\n",
      "param = 12, RSME training = 0.6 (0.1), RSME validation = 3.7 (0.4)\n",
      "param = 15, RSME training = 0.3 (0.1), RSME validation = 3.8 (0.4)\n",
      "param = 20, RSME training = 0.1 (0.0), RSME validation = 3.8 (0.6)\n",
      "param = 25, RSME training = 0.0 (0.0), RSME validation = 4.0 (0.6)\n",
      "param = 50, RSME training = 0.0 (0.0), RSME validation = 3.8 (0.6)\n",
      "CPU times: total: 1.84 s\n",
      "Wall time: 5.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Only considered hyperparameter: max depth of trees\n",
    "param_choices = [1, 2, 3, 5, 8, 10, 12, 15, 20, 25, 50]\n",
    "\n",
    "# Keep track of results for visualization\n",
    "param_to_scores = {}\n",
    "\n",
    "for param in param_choices:\n",
    "\n",
    "    # Train regressor with the current parameter setting\n",
    "    regressor = DecisionTreeRegressor(max_depth=param)\n",
    "    #regressor = RandomForestRegressor(max_depth=param)\n",
    "    #regressor = GradientBoostingRegressor(max_depth=param)\n",
    "    \n",
    "    # Perform 10-fold cross_validations\n",
    "    scores = cross_validate(regressor, X, y, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "    \n",
    "    # Extract the 10 RSME scores (training scores and validation scores) for each run/fold\n",
    "    # The (-1) is only needed since we get the negative root mean squared errors (it's a sklearn thing)\n",
    "    rsme_train = scores['train_score'] * (-1)\n",
    "    rsme_valid = scores['test_score'] * (-1)\n",
    "    \n",
    "    ## Keep track of all num_folds f1 scores for current param (for plotting)\n",
    "    param_to_scores[param] = (rsme_train, rsme_valid)\n",
    "    \n",
    "    ## Print statement for some immediate feedback (values in parenthesis represent the Standard Deviation)\n",
    "    print('param = {}, RSME training = {:.1f} ({:.1f}), RSME validation = {:.1f} ({:.1f})'\n",
    "          .format(param, np.mean(rsme_train), np.std(rsme_train), np.mean(rsme_valid), np.std(rsme_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95be1c2-ea9f-4d24-8611-90e220798a37",
   "metadata": {},
   "source": [
    "**Visualization of Results.** We provide you with 2 methods to visualize the results:\n",
    "* `plot_validation_results()` shows all `num_folds` scores for each parameter setting together with the means and standard deviations of the validation scores.\n",
    "* `plot_scores()` shows the training and validation scores for each parameter setting.\n",
    "\n",
    "Just run the code cell below to plot both figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86510a8f-530d-4027-8507-8f2e11416909",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_validation_results(param_to_scores)\n",
    "\n",
    "plot_scores(param_to_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0fbca4-ff7a-475a-b104-0fc212e4a5b7",
   "metadata": {},
   "source": [
    "#### 3.2 a) Comparing Tree-Based Regression Models (5 Points)\n",
    "\n",
    "Run the k-fold cross validation for all 3 regressors and compare and discuss the results! You should see quite a number of differences regarding runtimes, issues of overfitting and underfitting, overall performance, effects of parameter values, etc. You can use the code cells above for cross validation and visualization.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b75e07-30d4-47cc-ae35-30e583a85658",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fe03c5b-dde9-4a18-8b6f-7de32d977d69",
   "metadata": {},
   "source": [
    "#### 3.2 b) Assessing the Evaluation (3 Points)\n",
    "\n",
    "Discuss if we found the regressor with the cross-validation result from above! There is no need to implement anything here.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dec8767-9a8d-4450-a2e1-cb006eb1c57b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e0f5c9-d95d-4cbd-a642-794ddbf64d30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
