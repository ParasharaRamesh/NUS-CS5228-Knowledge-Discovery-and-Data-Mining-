{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763f0fa0-6584-44d4-b163-a891252769f9",
   "metadata": {},
   "source": [
    "# CS5228 Assignment 3 - Tree-Based Models\n",
    "\n",
    "Hello everyone, this assignment notebook covers Tree-Based Models. There are some code-completion tasks and question-answering tasks in this answer sheet. For code completion tasks, please write down your answer (i.e., your lines of code) between sentences that \"Your code starts here\" and \"Your code ends here\". The space between these two lines does not reflect the required or expected lines of code. For answers in plain text, you can refer to [this Markdown guide](https://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd) to customize the layout (although it shouldn't be needed).\n",
    "\n",
    "When you work on this notebook, you can insert additional code cells (e.g., for testing) or markdown cells (e.g., to keep track of your thoughts). However, before the submission, please remove all those additional cells again. Thanks!\n",
    "\n",
    "**Important:** \n",
    "* Rename and save this Jupyter notebook as **cs5228_a3_YourName_YourNUSNETID.ipynb** (e.g., **cs5228_a3_BobSmith_e12345678.ipynb**) before submission!\n",
    "* Rename and save the script file *cs5228_a3_script.py* as **cs5228_a3_YourName_YourNUSNETID.py** (e.g., **cs5228_a3_BobSmith_e12345678.py**) before submission!\n",
    "* Submission deadline is Oct 26, 11.59 pm. Late submissions will be penalized by 10% for each additional day. Failure to appropriately rename both files will yield a penalty of 1 Point. There is no need to use you full name if its a rather long; it's just  important to easily identify you in Canvas etc.\n",
    "\n",
    "Please also add your NUSNET and student id in the code cell below. This is just to make any identification of your notebook doubly sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "119a3b10-7afe-4814-9a79-32310842f1c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:18:29.411850400Z",
     "start_time": "2023-10-15T13:18:29.380419300Z"
    }
   },
   "outputs": [],
   "source": [
    "student_id = 'A0285647M'\n",
    "nusnet_id = 'e1216292'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7455d1c-31ad-4620-8d8a-ad2adc363f56",
   "metadata": {},
   "source": [
    "Here is an overview over the tasks to be solved and the points associated with each task. The notebook can appear very long and verbose, but note that a lot of parts provide additional explanations, documentation, or some discussion. The code and markdown cells you are supposed to complete are well, but you can use the overview below to double-check that you covered everything.\n",
    "\n",
    "* **1 Bagging and Feature Sampling (10 Points)**\n",
    "    * 1.1 Implement Bootstrapping (2 Points)\n",
    "    * 1.2 Implement Feature Sampling (2 Points)\n",
    "    * 1.3 Comparing Bagging and Bagging+FeatureSampling (6 Points)\n",
    "* **2 Implementing AdaBoost with Decision Stumps (30 Points)**\n",
    "    * 2.1 Implementing a Decision Stump Classifier (12 Points)\n",
    "        * 2.1 a) Calculating the Gini Score a Single Node (2 Points)\n",
    "        * 2.1 b) Calculating the Gini Score for a Split (2 Points)\n",
    "        * 2.1 c) Training the Decision Stump: Finding the Best Split (5 Points)\n",
    "        * 2.1 d) Predicting the Classes (3 Points)\n",
    "    * 2.2 Implementing AdaBoost (12 Points)\n",
    "        * 2.2 a) Training the Gradient-Boosted Regressor (8 Points)\n",
    "        * 2.2 b) Predicting Output Values (4 Points)\n",
    "    * 2.3 Questions about AdaBoost (6 Points)\n",
    "        * 2.3 a) Question 1 (2 Points)\n",
    "        * 2.3 b) Question 2 (4 Points)\n",
    "* **3 Evaluation of Tree-Based Models (10 Points)**\n",
    "    * 3.1 Data Preprocessing (2 Points)\n",
    "    * 3.2 Basic K-Fold Cross Validation (8 Points)\n",
    "        * 3.2a) Comparing Tree-Based Regression Models (5 Points)\n",
    "        * 3.2b) Assessing the Evaluation (3 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a4e6d6-30f8-4722-9750-4986c33fcccd",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Enable Auto-Reload\n",
    "\n",
    "This ensures that any saved changes to your `.py` file gets automatically reloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65641534-c0db-491c-8f9a-475ba94a5c6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:18:29.459511100Z",
     "start_time": "2023-10-15T13:18:29.380419300Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad660cb-6a44-4906-8756-d18ec238bf09",
   "metadata": {},
   "source": [
    "### Enable \"Inline Plotting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8067a9c2-d8d6-4cb5-b994-07fdc2c51b17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:18:30.281048800Z",
     "start_time": "2023-10-15T13:18:29.431975900Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bdd634-a20d-491b-9339-49eab3ab7205",
   "metadata": {},
   "source": [
    "### Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ca2bb72-8aa3-4d53-9d2a-e4953aa02592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:18:32.204000500Z",
     "start_time": "2023-10-15T13:18:30.248305100Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score, mean_squared_error\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befecf0d-9ea3-4355-abb3-6cc7b1daa7ec",
   "metadata": {},
   "source": [
    "**Important:** This notebook also requires you to complete in a separate `.py` script file. This keeps this notebook cleaner and simplifies testing your implementations for us. As you need to rename the file `cs5228_a3.py`, you also need to edit the import statement below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c0827e5-fcc6-42d4-93df-96258a07cec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:18:32.268099300Z",
     "start_time": "2023-10-15T13:18:32.172736800Z"
    }
   },
   "outputs": [],
   "source": [
    "from cs5228_a3_ParasharaRamesh_e1216292 import *\n",
    "#from cs5228_a3_BobSmith_e12345678 import get_noise_dbscan # <-- you will need to rename this accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ccaa8-0df8-4b2c-9676-0f872ec645d8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e65ca2e-36ac-4d5e-aa5d-b5c5e39ba1a4",
   "metadata": {},
   "source": [
    "## 1 Bagging and Feature Sampling (10 Points)\n",
    "\n",
    "In the lecture, we discussed the limitations of individual Decision Trees, which motivated the notion of Tree Ensembles. In a nutshell, a Tree Ensemble trains multiple Decision Trees within the same classifier and regressor to reduce variance and improve accuracy. The first approach towards creating Tree Ensembles was to train multiple Decision Trees over different samples of the data:\n",
    "\n",
    "* **Bagging (Bootstrap Aggregation):** Sample a new dataset $D_i$ sampled from $D$ uniformly and with replacement ($|D_i| = |D|$)\n",
    "* **Feature Sampling:** For a given dataset $D$ with $d$ features, consider only a random subset of features of size $m$ with $m<d$.\n",
    "\n",
    "Combining Bagging and Feature Sampling is the underlying idea of *Random Forests*. In this task, you will explore the effects of Bagging and Bagging+FeatureSampling\n",
    "\n",
    "We use the very basic [IRIS](https://archive.ics.uci.edu/ml/datasets/iris) dataset: it's small and clean, and has only numerical features. The dataset contains 3 classes of 50 instances each, where each class refers to a type of iris plant.\n",
    "\n",
    "### Prepare Example Data\n",
    "\n",
    "#### Load Data from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a94584c-9786-461d-8a35-290468e44fdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:18:32.348076800Z",
     "start_time": "2023-10-15T13:18:32.250989200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   sepal_length  sepal_width  petal_length  petal_width  species\n0           5.1          3.5           1.4          0.2        0\n1           4.9          3.0           1.4          0.2        0\n2           4.7          3.2           1.3          0.2        0\n3           4.6          3.1           1.5          0.2        0\n4           5.0          3.6           1.4          0.2        0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal_length</th>\n      <th>sepal_width</th>\n      <th>petal_length</th>\n      <th>petal_width</th>\n      <th>species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/a3-iris.csv')\n",
    "\n",
    "# Convert the species name to numerical categories 0, 1, 2\n",
    "df['species'] = pd.factorize(df['species'])[0]\n",
    "\n",
    "# Show the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5be1d5-ec05-4775-b65b-49072c231f5e",
   "metadata": {},
   "source": [
    "#### Convert to NumPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69cfc1fb-102b-4daf-be5e-39c5d8196d35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:18:32.520531400Z",
     "start_time": "2023-10-15T13:18:32.348076800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (150, 4)\n",
      "Shape of y: (150,)\n"
     ]
    }
   ],
   "source": [
    "data = df.to_numpy()\n",
    "\n",
    "X = data[:,0:4]\n",
    "y = data[:,4].astype(int)\n",
    "\n",
    "print('Shape of X: {}'.format(X.shape))\n",
    "print('Shape of y: {}'.format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3644e33f-34d9-4925-9955-66fb240ce08e",
   "metadata": {},
   "source": [
    "### 1.1 Implement Bootstrapping (2 Points)\n",
    "\n",
    "Implement method `create_boostrap_sample()` to generate a bootstrap sample for a given dataset! The input dataset is represented by feature array `X` and array `y` containing the class labels (classification) or output values (regression). Hint: numpy provides some convenient methods to make this a very simple task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61eaec19-c816-44a7-af9c-3f50d8661b4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:18:32.694008200Z",
     "start_time": "2023-10-15T13:18:32.426153700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_bootstrap: (150, 4)\n",
      "Shape of y_bootstrap: (150,)\n"
     ]
    }
   ],
   "source": [
    "def create_bootstrap_sample(X, y):\n",
    "    N, d = X.shape\n",
    "    \n",
    "    X_bootstrap, y_bootstrap = None, None\n",
    "    \n",
    "    #########################################################################################\n",
    "    ### Your code starts here ###############################################################\n",
    "\n",
    "    sampled_indices = np.random.choice(range(N), size=N, replace=True)\n",
    "    num_unique_elements = len(np.unique(sampled_indices))\n",
    "    # print(f\"No of unique samples is {num_unique_elements}\")\n",
    "    X_bootstrap = X[sampled_indices]\n",
    "    y_bootstrap = y[sampled_indices]\n",
    "\n",
    "    ### Your code ends here #################################################################\n",
    "    #########################################################################################\n",
    "    \n",
    "    return X_bootstrap, y_bootstrap\n",
    "\n",
    "\n",
    "X_bootstrap, y_bootstrap = create_bootstrap_sample(X, y)\n",
    "\n",
    "print('Shape of X_bootstrap: {}'.format(X_bootstrap.shape))\n",
    "print('Shape of y_bootstrap: {}'.format(y_bootstrap.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e2b42-0489-4e61-93da-ba054196ab83",
   "metadata": {},
   "source": [
    "The shapes of `X_bootstrap` and `y_bootstrap` should of course be the same as the shapes of `X` and `y`, but containing randomly selected samples. If you need to convince yourself, you can also print some elements of `X` to see if they are different between runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a2a40-da63-47ff-b950-739a2314dd00",
   "metadata": {},
   "source": [
    "### 1.2 Implement Feature Sampling (2 Points)\n",
    "\n",
    "Implement the method `perform_feature_sampling()`! The input is feature array `X`; use the common approach introduced in the lecture for calculating the number of sampled features -- that is, the number of sample features $m = \\lceil\\sqrt{d}\\rceil$. Apart from the new dataset `X_sample` the method also returns the *indices* of the selected features; we need those for the next task. Hint: Again, numpy should be your best friend here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5742cc84-b8c4-4d83-a75f-20e798a52b19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:18:32.694008200Z",
     "start_time": "2023-10-15T13:18:32.505005800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_sampled: (150, 2)\n",
      "Selected indices: [3 1]\n"
     ]
    }
   ],
   "source": [
    "def perform_feature_sampling(X):\n",
    "    N, d = X.shape\n",
    "    \n",
    "    X_feature_sampled, selected_indices = None, None\n",
    "    \n",
    "    #########################################################################################\n",
    "    ### Your code starts here ###############################################################    \n",
    "    m = np.ceil(np.sqrt(d)).astype(int)\n",
    "    # print(f\"m is {m}\")\n",
    "\n",
    "    selected_indices = np.random.choice(d, size=m, replace=False)\n",
    "\n",
    "    X_feature_sampled = X[:, selected_indices]\n",
    "\n",
    "    ### Your code ends here #################################################################\n",
    "    #########################################################################################    \n",
    "    \n",
    "    return X_feature_sampled, selected_indices\n",
    "    \n",
    "X_sampled, selected_indices = perform_feature_sampling(X)\n",
    "\n",
    "print('Shape of X_sampled: {}'.format(X_sampled.shape))\n",
    "print('Selected indices: {}'.format(selected_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2311868-b59e-4de0-8114-e3563b0146b9",
   "metadata": {},
   "source": [
    "`X_sampled` has to contain the same number of data samples as `X`, but with less features than `X`. The number of selected indices should of course be reflected in the shape of `X`. For example, if the shape of `X` is $(n, m)$, then there should be $m$ selected indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f61fc-378a-4792-86ac-32765abe5857",
   "metadata": {},
   "source": [
    "### 1.3 Comparing Bagging and Bagging+FeatureSampling (6 Points)\n",
    "\n",
    "Intuitively, different sampled dataset will yield different Decision Trees, not only regarding the accuracy, but also how the Decision Trees will \"look like\". In the following, we train a set of Decision Trees (using the Decision Tree implementation from `sklearn`) based on different dataset samples.\n",
    "\n",
    "In the code cell below, we use our implementations of the auxiliary methods `create_boostrap_sample()` and `perform_feature_sampling()` to train a series of Decision Trees (i.e., a Tree Ensemble) using only Bagging as well as using Bagging + Feature Sampling. In the output, *root index* is the index of the feature used for the very first split, and *#nodes* reflects the total number of nodes in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df219876-bcc2-4ccb-ba3f-6c06603b4f9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:18:32.772689200Z",
     "start_time": "2023-10-15T13:18:32.585069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging only\t\t\t\t\tBagging + Feature Sampling\n",
      "root index: 3,  #nodes: 15\t\troot index: 2,  #nodes: 15\n",
      "root index: 2,  #nodes: 15\t\troot index: 0,  #nodes: 57\n",
      "root index: 2,  #nodes: 15\t\troot index: 3,  #nodes: 29\n",
      "root index: 2,  #nodes: 15\t\troot index: 3,  #nodes: 25\n",
      "root index: 2,  #nodes: 15\t\troot index: 2,  #nodes: 21\n",
      "root index: 3,  #nodes: 15\t\troot index: 3,  #nodes: 27\n",
      "root index: 2,  #nodes: 9\t\troot index: 3,  #nodes: 17\n",
      "root index: 3,  #nodes: 15\t\troot index: 3,  #nodes: 21\n",
      "root index: 3,  #nodes: 13\t\troot index: 2,  #nodes: 21\n",
      "root index: 3,  #nodes: 7\t\troot index: 3,  #nodes: 17\n",
      "root index: 2,  #nodes: 15\t\troot index: 3,  #nodes: 19\n",
      "root index: 3,  #nodes: 15\t\troot index: 0,  #nodes: 61\n",
      "root index: 2,  #nodes: 9\t\troot index: 2,  #nodes: 15\n",
      "root index: 2,  #nodes: 13\t\troot index: 3,  #nodes: 19\n",
      "root index: 2,  #nodes: 13\t\troot index: 3,  #nodes: 15\n",
      "root index: 3,  #nodes: 13\t\troot index: 2,  #nodes: 17\n",
      "root index: 2,  #nodes: 19\t\troot index: 3,  #nodes: 17\n",
      "root index: 2,  #nodes: 11\t\troot index: 3,  #nodes: 17\n",
      "root index: 3,  #nodes: 13\t\troot index: 2,  #nodes: 29\n",
      "root index: 3,  #nodes: 15\t\troot index: 3,  #nodes: 13\n"
     ]
    }
   ],
   "source": [
    "# We need to set the seed as the sampling is random, and we want to ensure consistent results\n",
    "np.random.seed(0)\n",
    "\n",
    "print(\"Bagging only\\t\\t\\t\\t\\tBagging + Feature Sampling\")\n",
    "\n",
    "for _ in range(20):\n",
    "    # Create a new bootstrap sample (we can use the same for both ensembles)\n",
    "    X_t, y_t = create_bootstrap_sample(X, y)\n",
    "    classifier_bagging = DecisionTreeClassifier().fit(X_t, y_t)\n",
    "        \n",
    "    # Perform feature sampling on bootstrap sample\n",
    "    X_t_fs, selected_indices = perform_feature_sampling(X_t)\n",
    "    classifier_sampling = DecisionTreeClassifier().fit(X_t_fs, y_t)\n",
    "    \n",
    "    # Print core features of trained Decision Tree\n",
    "    # (feature index of root node, total of number in Decision Trr)\n",
    "    print(f\"root index: {classifier_bagging.tree_.feature[0]},  #nodes: {classifier_bagging.tree_.node_count}\\t\\t\"\n",
    "          f\"root index: {selected_indices[classifier_sampling.tree_.feature[0]]},  #nodes: {classifier_sampling.tree_.node_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645db041-fd10-4816-b656-edc571968081",
   "metadata": {},
   "source": [
    "**Interpret the result!** When comparing the resulting Decision Trees when using only **Bagging** and **Bagging+FeatureSampling** you must have observed several differences. List all your observations together with a brief explanation for the observed difference. What insights into the dataset can you gain from your observations?\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "* In Bagging the root indices are typically only either 2 or 3, whereas in the Bagging + feature sampling case the indices vary between 0,2 & 3.\n",
    "* The number of nodes in Bagging is most often around 15 , but takes values 7,9,11,13,19 occasionally. Whereas the number of nodes in the Bagging + feature sampling case shows a lot of variety & complexity (sometimes being as high as 61)\n",
    "* What this means is that Bagging generally tends to construct decision trees with uniform tree structures and similar node counts. Whereas, Bagging along with feature sampling gives rise to more complex, diverse decision trees resulting in varying root indices and structures\n",
    "* From the number of nodes present it is clear that bagging with feature sampling can handle more complex, high dimensional data with diverse feature importance due to the different root indices being chosen, whereas there is not much variation with just bagging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5413b8a-3fef-4cb6-99ff-f88e6dcee636",
   "metadata": {},
   "source": [
    "## 2 Implementing a AdaBoost with Decision Stumps (30 Points)\n",
    "\n",
    "AdaBoost (Adaptive Boosting) is an ensemble learning method that combines the predictions of multiple base learners to improve overall classification performance. In AdaBoost, the base learners are typically weak learners, which are models that perform slightly better than random guessing (e.g., Decision Stumps). Here's a brief overview of how AdaBoost works using Decision Trees as base learners for classification tasks:\n",
    "\n",
    "* **Initialization:** Assign equal weights to all training samples. These weights determine the importance of each sample in the training process.\n",
    "\n",
    "* **Iterative Phase:**\n",
    "\n",
    "    * *Base Learner Training:* Train a base learner (usually a Decision Stump) on the training data. It tries to minimize the weighted classification error, giving more weight to misclassified samples.\n",
    "\n",
    "    * *Weighted Error Calculation:* Calculate the weighted classification error of the base learner. This error is the sum of the weights of misclassified samples.\n",
    "\n",
    "    * *Classifier Weight Calculation:* Assign a weight to the base learner based on its performance. The better the performance, the higher the weight. This weight is used to determine the contribution of the base learner's prediction in the final ensemble.\n",
    "\n",
    "    * *Update Sample Weights:* Increase the weights of misclassified samples so that they become more important in the next iteration. This focuses the subsequent base learners on the samples that are harder to classify correctly.\n",
    "\n",
    "    * *Ensemble Building:* Combine the predictions of all base learners, weighted by their individual classifier weights, to obtain the final ensemble prediction.\n",
    "\n",
    "* **Final Prediction:** The final prediction is made by aggregating the weighted predictions of all base learners.\n",
    "\n",
    "AdaBoost's strength lies in its ability to focus on the difficult-to-classify examples, allowing it to improve performance even with weak base learners. This makes it particularly effective in situations where a single base learner might struggle. Keep in mind that while AdaBoost is powerful, it's important to be cautious about overfitting. AdaBoost can overfit if the base learners are too complex or if the number of iterations is too high. Therefore, it's advisable to monitor the performance on a validation set and potentially use techniques like early stopping or limiting the complexity of base learners.\n",
    "\n",
    "Your last task will be to implement an AdaBoost Classifier using Decision Stumps as covered in the lecture. But not to worry, this may only sound more difficult than it actually is, and we will guide you through this process step by step. We also keep things simple by assuming that all input features are numerical values.\n",
    "\n",
    "Fundamentally, we can split the implementation into 2 subtasks.\n",
    "\n",
    "* **Weak Learner:** You first implement the simplest \"Decision Stump Classifier\", i.e., a Decision Tree with only one split and therefore a height of 1. This means we do not have to care about the recursive splitting of nodes; it's not complicated but would only add tedious coding.\n",
    "* **AdaBoost:** With the Decision Stump Classifier in place, you can implement AdaBoost as shown in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12989e24-9943-4799-9d0e-0cd36c21448e",
   "metadata": {},
   "source": [
    "### Prepare Dataset\n",
    "\n",
    "Again, we use the [IRIS](https://archive.ics.uci.edu/ml/datasets/iris) dataset here.\n",
    "\n",
    "#### Load Dataset from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca04abbf-91c6-4eb3-b06d-75e882e35ad2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:18:32.993431300Z",
     "start_time": "2023-10-15T13:18:32.709734900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   sepal_length  sepal_width  petal_length  petal_width  species\n0           5.8          2.8           5.1          2.4        2\n1           6.0          2.2           4.0          1.0        1\n2           5.5          4.2           1.4          0.2        0\n3           7.3          2.9           6.3          1.8        2\n4           5.0          3.4           1.5          0.2        0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal_length</th>\n      <th>sepal_width</th>\n      <th>petal_length</th>\n      <th>petal_width</th>\n      <th>species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.8</td>\n      <td>2.8</td>\n      <td>5.1</td>\n      <td>2.4</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6.0</td>\n      <td>2.2</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5.5</td>\n      <td>4.2</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7.3</td>\n      <td>2.9</td>\n      <td>6.3</td>\n      <td>1.8</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.4</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "df_iris = pd.read_csv('data/a3-iris.csv')\n",
    "# Convert the 3 string class labels to 0, 1, and 2\n",
    "df_iris.species = df_iris.species.factorize()[0]\n",
    "df_iris = df_iris.sample(frac=1).reset_index(drop=True)\n",
    "# Show sample of dataset\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea35f1-368b-445f-9615-d4a55bdd633f",
   "metadata": {},
   "source": [
    "#### Convert Dataframe to NumPy arrays + Split into Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bda2a2fe-25e2-45bc-997d-270bafb821bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:18:32.993431300Z",
     "start_time": "2023-10-15T13:18:32.796359500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (30, 4)\n"
     ]
    }
   ],
   "source": [
    "X = df_iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].to_numpy()\n",
    "y = df_iris['species'].to_numpy().squeeze()\n",
    "\n",
    "training_size = int(0.8 * X.shape[0])\n",
    "\n",
    "X_train, y_train = X[:training_size], y[:training_size]\n",
    "X_test, y_test = X[training_size:], y[training_size:]\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be8cea0-007b-471d-b04d-9ab4eab864cd",
   "metadata": {},
   "source": [
    "### 2.1 Implementing a Decision Stump Classifier (12 Points)\n",
    "\n",
    "A Decision Stump is nothing else but a Decision Tree with typically only very few splits -- well, more generally, a Decision Tree with a (very) small height, but we keep it simple here. This means that we consider the smallest Decision Stump consisting of only a single split. This means that there is no need to continue recursively splitting child nodes like for a (full) Decision Tree trained to be a Strong Learner.\n",
    "\n",
    "For finding the best split, we need two main things\n",
    "* A scoring method to quantify how good a split is.\n",
    "* A method to actually find the best split (using the scoring method).\n",
    "\n",
    "You can find the skeleton code for the class `DecisionStumpClassifier` implementing the Decision Stump Classifier in the imported `py` file. You will need to complete this code step by step along with the subtask 2.1 a-d)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a64eae8-efa2-4921-813e-6ac1f9ebce35",
   "metadata": {},
   "source": [
    "#### 2.1 a) Calculating the Gini Score of a Single Node (2 Points)\n",
    "\n",
    "Recall from the lecture, that the Gini score of a node $t$ is defined as:\n",
    "\n",
    "$$Gini(t) = 1 - \\sum_{c\\in C} P(c|t)^2$$\n",
    "\n",
    "where $C$ is the set of classes, and $P(c|t)$ is the relative frequency of class $c$ in node $t$.\n",
    "\n",
    "**Implement this formula in the method `calc_gini_score_node()`!** Hint: Have a look at [`np.unique`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) and basic `numpy` methods such as [`np.sum`](https://numpy.org/doc/stable/reference/generated/numpy.sum.html) and [`np.square`](https://numpy.org/doc/stable/reference/generated/numpy.square.html) to make your life easier. You can use the example calls below to test your implementation of the method. The comments indicate the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52d1dc5d-f32f-4bda-9b8a-d284b7aaa52e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:18:33.069815700Z",
     "start_time": "2023-10-15T13:18:32.872793400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.5\n",
      "0.625\n"
     ]
    }
   ],
   "source": [
    "stump = DecisionStumpClassifier()\n",
    "\n",
    "y1 = np.array([1, 1, 1, 1, 1, 1, 1, 1])\n",
    "y2 = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "y3 = np.array([2, 0, 1, 1, 2, 2, 0, 2])\n",
    "\n",
    "print(stump.calc_gini_score_node(y1)) # 0.0\n",
    "print(stump.calc_gini_score_node(y2)) # 0.5\n",
    "print(stump.calc_gini_score_node(y3)) # 0.625"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d5dbb-393a-4d50-8283-65e070e01732",
   "metadata": {},
   "source": [
    "#### 2.1 b) Calculating the Gini Score for a Split (2 Points)\n",
    "\n",
    "In the lecture, we defined the impurity of a split as the average of the impurities of the child nodes, weighted by their size (in terms of the number of samples in each child node). Since we only consider binary splits (2 child nodes) and consider only the Gini score to measure impurity, the Gini score of a split simplifies to:\n",
    "\n",
    "$$Gini(t_{left}, t_{right}) = \\frac{n_{left}}{n}Gini(t_{left}) + \\frac{n_{right}}{n}Gini(t_{right})$$\n",
    "\n",
    "where $n_{left}$ ($n_{right}$) is the number of samples in the left (right) child node; and $n = n_{left} + n_{right}$\n",
    "\n",
    "**Implement this formula in the method `calc_gini_score_split`!** You obviously can and should use the existing method `calc_gini_score_node()`. You can use the example calls below to test your implementation of the method. The comments indicate the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec2c5d09-e78e-458e-ab5f-2ad413eb38fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:18:33.130983600Z",
     "start_time": "2023-10-15T13:18:32.946163200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.25\n",
      "0.3125\n"
     ]
    }
   ],
   "source": [
    "stump = DecisionStumpClassifier()\n",
    "\n",
    "print(stump.calc_gini_score_split(y1, y1))  # 0.0\n",
    "print(stump.calc_gini_score_split(y1, y2))  # 0.25\n",
    "print(stump.calc_gini_score_split(y1, y3))  # 0.3125"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c975fd-2552-4071-b96f-73b0a69c44d0",
   "metadata": {},
   "source": [
    "#### 2.1 c) Training the Decision Stump: Finding the Best Split (5 Points)\n",
    "\n",
    "With the means to calculate the Gini score for an arbitrary split, we can now train our Decision Stump Classifier. Recall, that our Decision Tree will only have a height of 1 as we only need to make 1 split.\n",
    "\n",
    "**Implement method `fit()`** to find the best split with respect to all features and corresponding thresholds. You obviously can and should use of the existing method `calc_gini_score_split()`. The skeleton code of method `fit()` already provides with the nested loop that goes through all features and the respective thresholds. Note that we keep it simple here as we use all unique values of a features as candidate thresholds.\n",
    "\n",
    "You can use the example calls below to test your implementation of the method. The comments indicate the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "385322ba-a8ac-4a48-84c9-4194b38a15e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:18:33.215672600Z",
     "start_time": "2023-10-15T13:18:33.020934500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sasgf\n",
      "gs: 0.6609243697478991\n",
      "gs: 0.6435344827586207\n",
      "gs: 0.637536231884058\n",
      "gs: 0.6188988095238095\n",
      "gs: 0.6124624624624624\n",
      "gs: 0.5784591194968554\n",
      "gs: 0.5775664408546117\n",
      "gs: 0.5482477100756671\n",
      "gs: 0.5133402995471961\n",
      "gs: 0.49286334527298387\n",
      "gs: 0.4831514762516046\n",
      "gs: 0.43787878787878787\n",
      "gs: 0.45027306697326813\n",
      "gs: 0.4748316498316499\n",
      "gs: 0.46861111111111114\n",
      "gs: 0.4882483987747146\n",
      "gs: 0.4977622377622377\n",
      "gs: 0.5138641372041775\n",
      "gs: 0.5229739252995068\n",
      "gs: 0.5329851218740108\n",
      "gs: 0.559906520747834\n",
      "gs: 0.5802536231884059\n",
      "gs: 0.6017480950246528\n",
      "gs: 0.5977272727272727\n",
      "gs: 0.6062250142775556\n",
      "gs: 0.6123764600179695\n",
      "gs: 0.6234848484848485\n",
      "gs: 0.6138138138138138\n",
      "gs: 0.6200892857142857\n",
      "gs: 0.6382608695652174\n",
      "gs: 0.6441091954022988\n",
      "gs: 0.6498575498575497\n",
      "gs: 0.6610644257703081\n",
      "gs: 0.6607843137254903\n",
      "gs: 0.6561781609195401\n",
      "gs: 0.6524441635061105\n",
      "gs: 0.6415915915915916\n",
      "gs: 0.6412660256410257\n",
      "gs: 0.6353333333333333\n",
      "gs: 0.6244135297326787\n",
      "gs: 0.60032019971779\n",
      "gs: 0.5773333333333334\n",
      "gs: 0.5635267060921807\n",
      "gs: 0.5648595590456056\n",
      "gs: 0.5541609667122663\n",
      "gs: 0.5324554755589238\n",
      "gs: 0.5768542643738058\n",
      "gs: 0.6111185983827493\n",
      "gs: 0.6160689463441756\n",
      "gs: 0.6286286286286287\n",
      "gs: 0.637536231884058\n",
      "gs: 0.6494301994301994\n",
      "gs: 0.6552259887005649\n",
      "gs: 0.6609243697478991\n",
      "gs: 0.6609243697478991\n",
      "gs: 0.6552259887005649\n",
      "gs: 0.6494301994301994\n",
      "gs: 0.6124624624624624\n",
      "gs: 0.541089108910891\n",
      "gs: 0.45402930402930397\n",
      "gs: 0.4029069767441861\n",
      "gs: 0.35752032520325205\n",
      "gs: 0.333125\n",
      "gs: 0.3449521457239889\n",
      "gs: 0.3659921473874963\n",
      "gs: 0.37529904306220097\n",
      "gs: 0.3838518518518519\n",
      "gs: 0.39168625146886016\n",
      "gs: 0.405324074074074\n",
      "gs: 0.4286961419318502\n",
      "gs: 0.43745474798106376\n",
      "gs: 0.4387329813837177\n",
      "gs: 0.4368464961067854\n",
      "gs: 0.42595238095238097\n",
      "gs: 0.4145833333333333\n",
      "gs: 0.374650156045505\n",
      "gs: 0.388125\n",
      "gs: 0.41064814814814815\n",
      "gs: 0.44183211424590724\n",
      "gs: 0.4867021276595745\n",
      "gs: 0.495438596491228\n",
      "gs: 0.5039930555555555\n",
      "gs: 0.520578231292517\n",
      "gs: 0.5442244224422442\n",
      "gs: 0.5665064102564102\n",
      "gs: 0.5875389408099689\n",
      "gs: 0.6074242424242424\n",
      "gs: 0.6200892857142857\n",
      "gs: 0.6323099415204678\n",
      "gs: 0.6441091954022988\n",
      "gs: 0.6498575498575497\n",
      "gs: 0.6555084745762711\n",
      "gs: 0.6610644257703081\n",
      "gs: 0.637536231884058\n",
      "gs: 0.48209219858156027\n",
      "gs: 0.42405303030303027\n",
      "gs: 0.35752032520325205\n",
      "gs: 0.3454732510288066\n",
      "gs: 0.333125\n",
      "gs: 0.37529904306220097\n",
      "gs: 0.39883415913727777\n",
      "gs: 0.4210855356635408\n",
      "gs: 0.4386111111111111\n",
      "gs: 0.434973604826546\n",
      "gs: 0.3982401975918493\n",
      "gs: 0.3767068273092369\n",
      "gs: 0.3878968253968253\n",
      "gs: 0.4867021276595745\n",
      "gs: 0.5123711340206185\n",
      "gs: 0.551797385620915\n",
      "gs: 0.5942901234567901\n",
      "gs: 0.6074242424242424\n",
      "gs: 0.6441091954022988\n",
      "gs: 0.6498575498575497\n",
      "Index of best feature: 2\n",
      "Best threshold: 2.45\n"
     ]
    }
   ],
   "source": [
    "stump = DecisionStumpClassifier().fit(X_train, y_train)\n",
    "\n",
    "print(\"Index of best feature:\", stump.feature_idx)\n",
    "print(\"Best threshold:\", stump.threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5cd9b-d17e-4fe5-be7c-56626a9da316",
   "metadata": {},
   "source": [
    "The output of previous cell should be:\n",
    "\n",
    "```\n",
    "Index of best feature: 2\n",
    "Best threshold: 2.45\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d231267-03e5-48da-bbf7-ce8f77e4796b",
   "metadata": {},
   "source": [
    "#### 2.1 d) Predicting the Class Labels (3 Points)\n",
    "\n",
    "After training our Decision Stump Classifier, we now only need to implement the last step -- that is, the prediction of the class labels for new data samples. Again, since we only have 1 split, this step is also rather easy to implement.\n",
    "\n",
    "**Implement method `predict()`** to predict the class labels for a given set of data samples. Hint: Have again a look at [`np.unique`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) and and [`np.argmax`](https://numpy.org/doc/stable/reference/generated/numpy.square.html) to make your life easier. You can use the example calls below to test your implementation of the method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e44f9595-a004-447c-9324-4cc93a07069a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:18:33.367253600Z",
     "start_time": "2023-10-15T13:18:33.099070500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sasgf\n",
      "gs: 0.6609243697478991\n",
      "gs: 0.6435344827586207\n",
      "gs: 0.637536231884058\n",
      "gs: 0.6188988095238095\n",
      "gs: 0.6124624624624624\n",
      "gs: 0.5784591194968554\n",
      "gs: 0.5775664408546117\n",
      "gs: 0.5482477100756671\n",
      "gs: 0.5133402995471961\n",
      "gs: 0.49286334527298387\n",
      "gs: 0.4831514762516046\n",
      "gs: 0.43787878787878787\n",
      "gs: 0.45027306697326813\n",
      "gs: 0.4748316498316499\n",
      "gs: 0.46861111111111114\n",
      "gs: 0.4882483987747146\n",
      "gs: 0.4977622377622377\n",
      "gs: 0.5138641372041775\n",
      "gs: 0.5229739252995068\n",
      "gs: 0.5329851218740108\n",
      "gs: 0.559906520747834\n",
      "gs: 0.5802536231884059\n",
      "gs: 0.6017480950246528\n",
      "gs: 0.5977272727272727\n",
      "gs: 0.6062250142775556\n",
      "gs: 0.6123764600179695\n",
      "gs: 0.6234848484848485\n",
      "gs: 0.6138138138138138\n",
      "gs: 0.6200892857142857\n",
      "gs: 0.6382608695652174\n",
      "gs: 0.6441091954022988\n",
      "gs: 0.6498575498575497\n",
      "gs: 0.6610644257703081\n",
      "gs: 0.6607843137254903\n",
      "gs: 0.6561781609195401\n",
      "gs: 0.6524441635061105\n",
      "gs: 0.6415915915915916\n",
      "gs: 0.6412660256410257\n",
      "gs: 0.6353333333333333\n",
      "gs: 0.6244135297326787\n",
      "gs: 0.60032019971779\n",
      "gs: 0.5773333333333334\n",
      "gs: 0.5635267060921807\n",
      "gs: 0.5648595590456056\n",
      "gs: 0.5541609667122663\n",
      "gs: 0.5324554755589238\n",
      "gs: 0.5768542643738058\n",
      "gs: 0.6111185983827493\n",
      "gs: 0.6160689463441756\n",
      "gs: 0.6286286286286287\n",
      "gs: 0.637536231884058\n",
      "gs: 0.6494301994301994\n",
      "gs: 0.6552259887005649\n",
      "gs: 0.6609243697478991\n",
      "gs: 0.6609243697478991\n",
      "gs: 0.6552259887005649\n",
      "gs: 0.6494301994301994\n",
      "gs: 0.6124624624624624\n",
      "gs: 0.541089108910891\n",
      "gs: 0.45402930402930397\n",
      "gs: 0.4029069767441861\n",
      "gs: 0.35752032520325205\n",
      "gs: 0.333125\n",
      "gs: 0.3449521457239889\n",
      "gs: 0.3659921473874963\n",
      "gs: 0.37529904306220097\n",
      "gs: 0.3838518518518519\n",
      "gs: 0.39168625146886016\n",
      "gs: 0.405324074074074\n",
      "gs: 0.4286961419318502\n",
      "gs: 0.43745474798106376\n",
      "gs: 0.4387329813837177\n",
      "gs: 0.4368464961067854\n",
      "gs: 0.42595238095238097\n",
      "gs: 0.4145833333333333\n",
      "gs: 0.374650156045505\n",
      "gs: 0.388125\n",
      "gs: 0.41064814814814815\n",
      "gs: 0.44183211424590724\n",
      "gs: 0.4867021276595745\n",
      "gs: 0.495438596491228\n",
      "gs: 0.5039930555555555\n",
      "gs: 0.520578231292517\n",
      "gs: 0.5442244224422442\n",
      "gs: 0.5665064102564102\n",
      "gs: 0.5875389408099689\n",
      "gs: 0.6074242424242424\n",
      "gs: 0.6200892857142857\n",
      "gs: 0.6323099415204678\n",
      "gs: 0.6441091954022988\n",
      "gs: 0.6498575498575497\n",
      "gs: 0.6555084745762711\n",
      "gs: 0.6610644257703081\n",
      "gs: 0.637536231884058\n",
      "gs: 0.48209219858156027\n",
      "gs: 0.42405303030303027\n",
      "gs: 0.35752032520325205\n",
      "gs: 0.3454732510288066\n",
      "gs: 0.333125\n",
      "gs: 0.37529904306220097\n",
      "gs: 0.39883415913727777\n",
      "gs: 0.4210855356635408\n",
      "gs: 0.4386111111111111\n",
      "gs: 0.434973604826546\n",
      "gs: 0.3982401975918493\n",
      "gs: 0.3767068273092369\n",
      "gs: 0.3878968253968253\n",
      "gs: 0.4867021276595745\n",
      "gs: 0.5123711340206185\n",
      "gs: 0.551797385620915\n",
      "gs: 0.5942901234567901\n",
      "gs: 0.6074242424242424\n",
      "gs: 0.6441091954022988\n",
      "gs: 0.6498575498575497\n",
      "[0. 2. 0. 0. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 0. 2. 2. 2. 2. 0.\n",
      " 0. 0. 2. 2. 2. 0.]\n"
     ]
    }
   ],
   "source": [
    "stump = DecisionStumpClassifier().fit(X_train, y_train)\n",
    "\n",
    "print(stump.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c39a12-e5bf-4305-8979-b6ce9e3ca3ed",
   "metadata": {},
   "source": [
    "The output of previous cell should be:\n",
    "\n",
    "```\n",
    "[0. 2. 0. 0. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 0. 2. 2. 2. 2. 0.\n",
    " 0. 0. 2. 2. 2. 0.]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8748b62f-e16f-4698-a558-f82f0a2bad01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:18:33.405024300Z",
     "start_time": "2023-10-15T13:18:33.178277800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score of your Decision Tree implementation on the toy dataset is 0.540\n"
     ]
    }
   ],
   "source": [
    "y_pred = stump.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('The f1 score of your Decision Tree implementation on the toy dataset is {:.3f}'.format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d52a989-c417-4e66-95b0-eb46f2737911",
   "metadata": {},
   "source": [
    "The resulting f1 score should be **0.540**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e6c48-4509-4fee-bf3e-c6248323a0de",
   "metadata": {},
   "source": [
    "**Testing your Implementation on the IRIS Dataset.** This part is only for you to test your implementation on a real-world dataset (IRIS) since the toy dataset might not reveal all bugs in your code. You can also directly compare the result your Decision Stump implementation with the results from scikit-learn's [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html); Of course, we need to set `max_depth=1` to make it a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eb8a27b-29d9-4a8e-94c7-e8c7fbcb45b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:18:33.405024300Z",
     "start_time": "2023-10-15T13:18:33.256955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sasgf\n",
      "gs: 0.6609243697478991\n",
      "gs: 0.6435344827586207\n",
      "gs: 0.637536231884058\n",
      "gs: 0.6188988095238095\n",
      "gs: 0.6124624624624624\n",
      "gs: 0.5784591194968554\n",
      "gs: 0.5775664408546117\n",
      "gs: 0.5482477100756671\n",
      "gs: 0.5133402995471961\n",
      "gs: 0.49286334527298387\n",
      "gs: 0.4831514762516046\n",
      "gs: 0.43787878787878787\n",
      "gs: 0.45027306697326813\n",
      "gs: 0.4748316498316499\n",
      "gs: 0.46861111111111114\n",
      "gs: 0.4882483987747146\n",
      "gs: 0.4977622377622377\n",
      "gs: 0.5138641372041775\n",
      "gs: 0.5229739252995068\n",
      "gs: 0.5329851218740108\n",
      "gs: 0.559906520747834\n",
      "gs: 0.5802536231884059\n",
      "gs: 0.6017480950246528\n",
      "gs: 0.5977272727272727\n",
      "gs: 0.6062250142775556\n",
      "gs: 0.6123764600179695\n",
      "gs: 0.6234848484848485\n",
      "gs: 0.6138138138138138\n",
      "gs: 0.6200892857142857\n",
      "gs: 0.6382608695652174\n",
      "gs: 0.6441091954022988\n",
      "gs: 0.6498575498575497\n",
      "gs: 0.6610644257703081\n",
      "gs: 0.6607843137254903\n",
      "gs: 0.6561781609195401\n",
      "gs: 0.6524441635061105\n",
      "gs: 0.6415915915915916\n",
      "gs: 0.6412660256410257\n",
      "gs: 0.6353333333333333\n",
      "gs: 0.6244135297326787\n",
      "gs: 0.60032019971779\n",
      "gs: 0.5773333333333334\n",
      "gs: 0.5635267060921807\n",
      "gs: 0.5648595590456056\n",
      "gs: 0.5541609667122663\n",
      "gs: 0.5324554755589238\n",
      "gs: 0.5768542643738058\n",
      "gs: 0.6111185983827493\n",
      "gs: 0.6160689463441756\n",
      "gs: 0.6286286286286287\n",
      "gs: 0.637536231884058\n",
      "gs: 0.6494301994301994\n",
      "gs: 0.6552259887005649\n",
      "gs: 0.6609243697478991\n",
      "gs: 0.6609243697478991\n",
      "gs: 0.6552259887005649\n",
      "gs: 0.6494301994301994\n",
      "gs: 0.6124624624624624\n",
      "gs: 0.541089108910891\n",
      "gs: 0.45402930402930397\n",
      "gs: 0.4029069767441861\n",
      "gs: 0.35752032520325205\n",
      "gs: 0.333125\n",
      "gs: 0.3449521457239889\n",
      "gs: 0.3659921473874963\n",
      "gs: 0.37529904306220097\n",
      "gs: 0.3838518518518519\n",
      "gs: 0.39168625146886016\n",
      "gs: 0.405324074074074\n",
      "gs: 0.4286961419318502\n",
      "gs: 0.43745474798106376\n",
      "gs: 0.4387329813837177\n",
      "gs: 0.4368464961067854\n",
      "gs: 0.42595238095238097\n",
      "gs: 0.4145833333333333\n",
      "gs: 0.374650156045505\n",
      "gs: 0.388125\n",
      "gs: 0.41064814814814815\n",
      "gs: 0.44183211424590724\n",
      "gs: 0.4867021276595745\n",
      "gs: 0.495438596491228\n",
      "gs: 0.5039930555555555\n",
      "gs: 0.520578231292517\n",
      "gs: 0.5442244224422442\n",
      "gs: 0.5665064102564102\n",
      "gs: 0.5875389408099689\n",
      "gs: 0.6074242424242424\n",
      "gs: 0.6200892857142857\n",
      "gs: 0.6323099415204678\n",
      "gs: 0.6441091954022988\n",
      "gs: 0.6498575498575497\n",
      "gs: 0.6555084745762711\n",
      "gs: 0.6610644257703081\n",
      "gs: 0.637536231884058\n",
      "gs: 0.48209219858156027\n",
      "gs: 0.42405303030303027\n",
      "gs: 0.35752032520325205\n",
      "gs: 0.3454732510288066\n",
      "gs: 0.333125\n",
      "gs: 0.37529904306220097\n",
      "gs: 0.39883415913727777\n",
      "gs: 0.4210855356635408\n",
      "gs: 0.4386111111111111\n",
      "gs: 0.434973604826546\n",
      "gs: 0.3982401975918493\n",
      "gs: 0.3767068273092369\n",
      "gs: 0.3878968253968253\n",
      "gs: 0.4867021276595745\n",
      "gs: 0.5123711340206185\n",
      "gs: 0.551797385620915\n",
      "gs: 0.5942901234567901\n",
      "gs: 0.6074242424242424\n",
      "gs: 0.6441091954022988\n",
      "gs: 0.6498575498575497\n",
      "The f1 score of your Decision Tree implementation on the IRIS dataset is 0.540\n",
      "The f1 score of sklearn Decision Tree implementation on the IRIS dataset is 0.540\n"
     ]
    }
   ],
   "source": [
    "my_stump = DecisionStumpClassifier().fit(X_train, y_train)\n",
    "sk_stump = DecisionTreeClassifier(max_depth=1).fit(X_train, y_train)\n",
    "\n",
    "my_y_pred = my_stump.predict(X_test)\n",
    "sk_y_pred = sk_stump.predict(X_test)\n",
    "\n",
    "my_f1 = f1_score(y_test, my_y_pred, average='macro')\n",
    "sk_f1 = f1_score(y_test, sk_y_pred, average='macro')\n",
    "\n",
    "print('The f1 score of your Decision Tree implementation on the IRIS dataset is {:.3f}'.format(my_f1))\n",
    "print('The f1 score of sklearn Decision Tree implementation on the IRIS dataset is {:.3f}'.format(sk_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c61b97-eac9-47b2-81d9-3f8082572c0d",
   "metadata": {},
   "source": [
    "You should see an f1 score of **0.540** using both your implementation as well as the one from scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6b0839-d1df-40a5-8e15-fa5dfe538d5f",
   "metadata": {},
   "source": [
    "## 2.2 Implementing AdaBoost (12 Points)\n",
    "\n",
    "AdaBoost is a very popular ensemble technique that trains a series of *Weak Learners* to make predictions. Although AdaBoost is a generic technique, it is very commonly used with Decision Stumps as Weak Learners, since Decision Trees with a limited maximum height make naturally good Weak Learners.\n",
    "\n",
    "Again, we provide you with a skeleton code for the class implementing the AdaBoost Classifier. We call it `AdaBoostTreeClassifier` to avoid naming conflicts with `AdaBoostClassifier` of scikit-learn, and because we limit ourselves to Decision Trees (well, Stumps) as the estimators (i.e., the Weak Learners)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9409b98-9379-4b5a-bd39-c1fc38a4dae0",
   "metadata": {},
   "source": [
    "#### 2.1 a) Training the AdaBoost Classifier (8 Points)\n",
    "\n",
    "In the lecture, we went step by step through the training process of an AdaBoost classifier. We saw that this process comprises multiple but rather straightforward steps.\n",
    "\n",
    "\n",
    "**Implement method `fit()`** to train your `AdaBoostTreeClassifier`. The skeleton code of method `fit()` allows you to focus on the core steps within each iteration for training the next Weak Learner (here, our Decision Stump). To help you a little bit, we list the main 4 steps and give you the first step -- training the next estimator using the current dataset sample -- for free.\n",
    "\n",
    "**Important:** By default, `AdaBoostTreeClassifier` uses your implementation of `DecisionStumpClassifier` as its Weak Learner. In case you had problems implementing `DecisionStumpClassifier` or you simply want to test the results, you can also use scikit-learn's `DecisionTreeClassifier`. To make this change, just use the commented line under Step 1 to train the Weak Learner.\n",
    "\n",
    "You can use the example calls below to test your implementation of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32a73cd4-2e7a-40d2-be68-3b76bef889ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:18:33.521343800Z",
     "start_time": "2023-10-15T13:18:33.367253600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sasgf\n",
      "gs: 0.6609243697478991\n",
      "gs: 0.6435344827586207\n",
      "gs: 0.637536231884058\n",
      "gs: 0.6188988095238095\n",
      "gs: 0.6124624624624624\n",
      "gs: 0.5784591194968554\n",
      "gs: 0.5775664408546117\n",
      "gs: 0.5482477100756671\n",
      "gs: 0.5133402995471961\n",
      "gs: 0.49286334527298387\n",
      "gs: 0.4831514762516046\n",
      "gs: 0.43787878787878787\n",
      "gs: 0.45027306697326813\n",
      "gs: 0.4748316498316499\n",
      "gs: 0.46861111111111114\n",
      "gs: 0.4882483987747146\n",
      "gs: 0.4977622377622377\n",
      "gs: 0.5138641372041775\n",
      "gs: 0.5229739252995068\n",
      "gs: 0.5329851218740108\n",
      "gs: 0.559906520747834\n",
      "gs: 0.5802536231884059\n",
      "gs: 0.6017480950246528\n",
      "gs: 0.5977272727272727\n",
      "gs: 0.6062250142775556\n",
      "gs: 0.6123764600179695\n",
      "gs: 0.6234848484848485\n",
      "gs: 0.6138138138138138\n",
      "gs: 0.6200892857142857\n",
      "gs: 0.6382608695652174\n",
      "gs: 0.6441091954022988\n",
      "gs: 0.6498575498575497\n",
      "gs: 0.6610644257703081\n",
      "gs: 0.6607843137254903\n",
      "gs: 0.6561781609195401\n",
      "gs: 0.6524441635061105\n",
      "gs: 0.6415915915915916\n",
      "gs: 0.6412660256410257\n",
      "gs: 0.6353333333333333\n",
      "gs: 0.6244135297326787\n",
      "gs: 0.60032019971779\n",
      "gs: 0.5773333333333334\n",
      "gs: 0.5635267060921807\n",
      "gs: 0.5648595590456056\n",
      "gs: 0.5541609667122663\n",
      "gs: 0.5324554755589238\n",
      "gs: 0.5768542643738058\n",
      "gs: 0.6111185983827493\n",
      "gs: 0.6160689463441756\n",
      "gs: 0.6286286286286287\n",
      "gs: 0.637536231884058\n",
      "gs: 0.6494301994301994\n",
      "gs: 0.6552259887005649\n",
      "gs: 0.6609243697478991\n",
      "gs: 0.6609243697478991\n",
      "gs: 0.6552259887005649\n",
      "gs: 0.6494301994301994\n",
      "gs: 0.6124624624624624\n",
      "gs: 0.541089108910891\n",
      "gs: 0.45402930402930397\n",
      "gs: 0.4029069767441861\n",
      "gs: 0.35752032520325205\n",
      "gs: 0.333125\n",
      "gs: 0.3449521457239889\n",
      "gs: 0.3659921473874963\n",
      "gs: 0.37529904306220097\n",
      "gs: 0.3838518518518519\n",
      "gs: 0.39168625146886016\n",
      "gs: 0.405324074074074\n",
      "gs: 0.4286961419318502\n",
      "gs: 0.43745474798106376\n",
      "gs: 0.4387329813837177\n",
      "gs: 0.4368464961067854\n",
      "gs: 0.42595238095238097\n",
      "gs: 0.4145833333333333\n",
      "gs: 0.374650156045505\n",
      "gs: 0.388125\n",
      "gs: 0.41064814814814815\n",
      "gs: 0.44183211424590724\n",
      "gs: 0.4867021276595745\n",
      "gs: 0.495438596491228\n",
      "gs: 0.5039930555555555\n",
      "gs: 0.520578231292517\n",
      "gs: 0.5442244224422442\n",
      "gs: 0.5665064102564102\n",
      "gs: 0.5875389408099689\n",
      "gs: 0.6074242424242424\n",
      "gs: 0.6200892857142857\n",
      "gs: 0.6323099415204678\n",
      "gs: 0.6441091954022988\n",
      "gs: 0.6498575498575497\n",
      "gs: 0.6555084745762711\n",
      "gs: 0.6610644257703081\n",
      "gs: 0.637536231884058\n",
      "gs: 0.48209219858156027\n",
      "gs: 0.42405303030303027\n",
      "gs: 0.35752032520325205\n",
      "gs: 0.3454732510288066\n",
      "gs: 0.333125\n",
      "gs: 0.37529904306220097\n",
      "gs: 0.39883415913727777\n",
      "gs: 0.4210855356635408\n",
      "gs: 0.4386111111111111\n",
      "gs: 0.434973604826546\n",
      "gs: 0.3982401975918493\n",
      "gs: 0.3767068273092369\n",
      "gs: 0.3878968253968253\n",
      "gs: 0.4867021276595745\n",
      "gs: 0.5123711340206185\n",
      "gs: 0.551797385620915\n",
      "gs: 0.5942901234567901\n",
      "gs: 0.6074242424242424\n",
      "gs: 0.6441091954022988\n",
      "gs: 0.6498575498575497\n",
      "sasgf\n",
      "gs: 0.5776353276353277\n",
      "gs: 0.5615942028985508\n",
      "gs: 0.5277777777777778\n",
      "gs: 0.5099388379204892\n",
      "gs: 0.5072115384615384\n",
      "gs: 0.5163540163540163\n",
      "gs: 0.5042280414620841\n",
      "gs: 0.49621212121212116\n",
      "gs: 0.4592434314656537\n",
      "gs: 0.46533490011750883\n",
      "gs: 0.5\n",
      "gs: 0.4942113550060202\n",
      "gs: 0.5091841491841491\n",
      "gs: 0.5150112233445567\n",
      "gs: 0.5234741784037559\n",
      "gs: 0.53375\n",
      "gs: 0.5416666666666667\n",
      "gs: 0.5427777777777778\n",
      "gs: 0.5344484269215451\n",
      "gs: 0.5603612992878235\n",
      "gs: 0.5579918523843759\n",
      "gs: 0.5525757575757575\n",
      "gs: 0.5489766081871345\n",
      "gs: 0.5579710144927537\n",
      "gs: 0.5840395480225989\n",
      "gs: 0.592436974789916\n",
      "gs: 0.5969827586206896\n",
      "gs: 0.5903919089759796\n",
      "gs: 0.5822727272727274\n",
      "gs: 0.5498333333333334\n",
      "gs: 0.5555555555555555\n",
      "gs: 0.5464822533788052\n",
      "gs: 0.5202083333333334\n",
      "gs: 0.4498732751337651\n",
      "gs: 0.5129749314531923\n",
      "gs: 0.5099159663865547\n",
      "gs: 0.49492234169653526\n",
      "gs: 0.4729459788084071\n",
      "gs: 0.5277777777777778\n",
      "gs: 0.5449852507374632\n",
      "gs: 0.5533625730994153\n",
      "gs: 0.5615942028985508\n",
      "gs: 0.5696839080459771\n",
      "gs: 0.5776353276353277\n",
      "gs: 0.5931372549019608\n",
      "gs: 0.5776353276353277\n",
      "gs: 0.5615942028985508\n",
      "gs: 0.48191823899371067\n",
      "gs: 0.3758680555555556\n",
      "gs: 0.36403508771929827\n",
      "gs: 0.3141025641025641\n",
      "gs: 0.30092592592592593\n",
      "gs: 0.3295454545454544\n",
      "gs: 0.3857592532291327\n",
      "gs: 0.4259768009768009\n",
      "gs: 0.4325480720829558\n",
      "gs: 0.46785714285714286\n",
      "gs: 0.4900352243233222\n",
      "gs: 0.4942113550060202\n",
      "gs: 0.4958228905597327\n",
      "gs: 0.4964927048260382\n",
      "gs: 0.4763431013431013\n",
      "gs: 0.4596170303808301\n",
      "gs: 0.3926360995326513\n",
      "gs: 0.3823649754500817\n",
      "gs: 0.3676975945017183\n",
      "gs: 0.3801020408163265\n",
      "gs: 0.43851132686084127\n",
      "gs: 0.46031746031746035\n",
      "gs: 0.4709119496855345\n",
      "gs: 0.5015290519877676\n",
      "gs: 0.5113636363636364\n",
      "gs: 0.5398230088495575\n",
      "gs: 0.5489766081871345\n",
      "gs: 0.5579710144927537\n",
      "gs: 0.5840395480225989\n",
      "gs: 0.592436974789916\n",
      "gs: 0.5533625730994153\n",
      "gs: 0.4315181518151815\n",
      "gs: 0.3874570446735396\n",
      "gs: 0.32699275362318836\n",
      "gs: 0.30092592592592593\n",
      "gs: 0.3759920634920635\n",
      "gs: 0.4325480720829558\n",
      "gs: 0.45949074074074076\n",
      "gs: 0.4958228905597327\n",
      "gs: 0.49644444444444435\n",
      "gs: 0.4173636001592991\n",
      "gs: 0.3801020408163265\n",
      "gs: 0.46031746031746035\n",
      "gs: 0.4709119496855345\n",
      "gs: 0.5015290519877676\n",
      "gs: 0.5489766081871345\n",
      "gs: 0.5840395480225989\n",
      "gs: 0.592436974789916\n",
      "sasgf\n",
      "gs: 0.6015536723163841\n",
      "gs: 0.5925925925925926\n",
      "gs: 0.5742028985507246\n",
      "gs: 0.5544544544544545\n",
      "gs: 0.536769991015274\n",
      "gs: 0.5008163974292166\n",
      "gs: 0.4792207792207792\n",
      "gs: 0.45644703421485133\n",
      "gs: 0.45846273291925466\n",
      "gs: 0.4806818181818182\n",
      "gs: 0.4854700854700855\n",
      "gs: 0.5049840510366825\n",
      "gs: 0.5225694444444444\n",
      "gs: 0.522765246449457\n",
      "gs: 0.5154024265999815\n",
      "gs: 0.5049845114052379\n",
      "gs: 0.5167103856990187\n",
      "gs: 0.5314583333333334\n",
      "gs: 0.570123106060606\n",
      "gs: 0.564353026458862\n",
      "gs: 0.5643889798145117\n",
      "gs: 0.561976911976912\n",
      "gs: 0.5661717114030078\n",
      "gs: 0.5552884615384616\n",
      "gs: 0.5643081761006289\n",
      "gs: 0.5854354354354355\n",
      "gs: 0.5933628318584071\n",
      "gs: 0.6131652661064425\n",
      "gs: 0.6050724637681159\n",
      "gs: 0.593489254108723\n",
      "gs: 0.5873511904761906\n",
      "gs: 0.5963492063492064\n",
      "gs: 0.593987493987494\n",
      "gs: 0.6010553564317005\n",
      "gs: 0.5985401063714316\n",
      "gs: 0.5913241352879179\n",
      "gs: 0.5787116176005065\n",
      "gs: 0.5793087121212122\n",
      "gs: 0.5411736411736412\n",
      "gs: 0.5247596153846154\n",
      "gs: 0.5373227689741451\n",
      "gs: 0.5847953216374268\n",
      "gs: 0.5834770114942529\n",
      "gs: 0.6015536723163841\n",
      "gs: 0.6103641456582634\n",
      "gs: 0.6103641456582634\n",
      "gs: 0.5149847094801222\n",
      "gs: 0.47174603174603164\n",
      "gs: 0.4604166666666667\n",
      "gs: 0.4128333333333334\n",
      "gs: 0.4003367003367003\n",
      "gs: 0.41046691403834257\n",
      "gs: 0.426388888888889\n",
      "gs: 0.4324912280701755\n",
      "gs: 0.4494949494949495\n",
      "gs: 0.4395057766367138\n",
      "gs: 0.417597905969999\n",
      "gs: 0.3992264003133569\n",
      "gs: 0.35998863313441315\n",
      "gs: 0.341171500985638\n",
      "gs: 0.2856414534668151\n",
      "gs: 0.2964494569757728\n",
      "gs: 0.33711281614094907\n",
      "gs: 0.37521616632662974\n",
      "gs: 0.43433734939759033\n",
      "gs: 0.44146825396825395\n",
      "gs: 0.44843137254901966\n",
      "gs: 0.45523255813953484\n",
      "gs: 0.4809259259259259\n",
      "gs: 0.4987455197132617\n",
      "gs: 0.5207903780068729\n",
      "gs: 0.5459150326797385\n",
      "gs: 0.5643081761006289\n",
      "gs: 0.5813636363636364\n",
      "gs: 0.6010144927536232\n",
      "gs: 0.6084045584045583\n",
      "gs: 0.5834770114942529\n",
      "gs: 0.49376947040498453\n",
      "gs: 0.44886731391585755\n",
      "gs: 0.4128333333333334\n",
      "gs: 0.4003367003367003\n",
      "gs: 0.44157706093189963\n",
      "gs: 0.44474637681159424\n",
      "gs: 0.4485522115823074\n",
      "gs: 0.417597905969999\n",
      "gs: 0.35998863313441315\n",
      "gs: 0.31027777777777776\n",
      "gs: 0.2565104166666667\n",
      "gs: 0.43433734939759033\n",
      "gs: 0.4618773946360153\n",
      "gs: 0.4987455197132617\n",
      "gs: 0.5207903780068729\n",
      "gs: 0.5506472491909385\n",
      "gs: 0.6047413793103449\n",
      "gs: 0.6084045584045583\n",
      "sasgf\n",
      "gs: 0.6350282485875706\n",
      "gs: 0.6105263157894737\n",
      "gs: 0.6041297935103245\n",
      "gs: 0.541025641025641\n",
      "gs: 0.5438219034812491\n",
      "gs: 0.4993844696969697\n",
      "gs: 0.4773809523809523\n",
      "gs: 0.4575074611505609\n",
      "gs: 0.4136296296296297\n",
      "gs: 0.4127814506084124\n",
      "gs: 0.4138927738927739\n",
      "gs: 0.42500696184906717\n",
      "gs: 0.43513939057145495\n",
      "gs: 0.44942129629629624\n",
      "gs: 0.4952833175055398\n",
      "gs: 0.5098230760881363\n",
      "gs: 0.5366074538488331\n",
      "gs: 0.5670485542825968\n",
      "gs: 0.5777777777777778\n",
      "gs: 0.5807348181991242\n",
      "gs: 0.5823899371069183\n",
      "gs: 0.6072072072072072\n",
      "gs: 0.6165191740412979\n",
      "gs: 0.642577030812325\n",
      "gs: 0.6392156862745098\n",
      "gs: 0.6362410450906025\n",
      "gs: 0.6292292292292292\n",
      "gs: 0.6242424242424243\n",
      "gs: 0.6151467778356783\n",
      "gs: 0.5802343844388064\n",
      "gs: 0.5663243474540008\n",
      "gs: 0.5424242424242424\n",
      "gs: 0.5375390736004546\n",
      "gs: 0.5852799090650753\n",
      "gs: 0.5714583333333334\n",
      "gs: 0.5585598646852724\n",
      "gs: 0.5400701754385965\n",
      "gs: 0.5891975308641976\n",
      "gs: 0.6150297619047619\n",
      "gs: 0.6210703750526758\n",
      "gs: 0.6229885057471264\n",
      "gs: 0.6350282485875706\n",
      "gs: 0.6408963585434174\n",
      "gs: 0.6350282485875706\n",
      "gs: 0.629059829059829\n",
      "gs: 0.5976190476190477\n",
      "gs: 0.5174917491749175\n",
      "gs: 0.44731182795698926\n",
      "gs: 0.3862068965517241\n",
      "gs: 0.3284552845528455\n",
      "gs: 0.3033333333333333\n",
      "gs: 0.3124626942471957\n",
      "gs: 0.3206349206349206\n",
      "gs: 0.3278969092922582\n",
      "gs: 0.3342902711323764\n",
      "gs: 0.34461417939678807\n",
      "gs: 0.35731741972151176\n",
      "gs: 0.3566778900112234\n",
      "gs: 0.3529761904761905\n",
      "gs: 0.3624242424242424\n",
      "gs: 0.3556677890011224\n",
      "gs: 0.34000754147812967\n",
      "gs: 0.34247619047619043\n",
      "gs: 0.35356903324710165\n",
      "gs: 0.4231990231990233\n",
      "gs: 0.4462745098039215\n",
      "gs: 0.4620689655172414\n",
      "gs: 0.4771535580524344\n",
      "gs: 0.4985507246376812\n",
      "gs: 0.5120567375886524\n",
      "gs: 0.5607843137254902\n",
      "gs: 0.5875389408099689\n",
      "gs: 0.5975535168195718\n",
      "gs: 0.6072072072072072\n",
      "gs: 0.6165191740412979\n",
      "gs: 0.6210526315789474\n",
      "gs: 0.6255072463768117\n",
      "gs: 0.6105263157894737\n",
      "gs: 0.4659649122807017\n",
      "gs: 0.4074906367041199\n",
      "gs: 0.3405622489959839\n",
      "gs: 0.31604938271604943\n",
      "gs: 0.3033333333333333\n",
      "gs: 0.3342902711323764\n",
      "gs: 0.34461417939678807\n",
      "gs: 0.34860584863499466\n",
      "gs: 0.35731741972151176\n",
      "gs: 0.415029761904762\n",
      "gs: 0.4395534290271133\n",
      "gs: 0.4123456790123457\n",
      "gs: 0.4380952380952381\n",
      "gs: 0.4915750915750916\n",
      "gs: 0.5185964912280702\n",
      "gs: 0.5493333333333333\n",
      "gs: 0.5663430420711973\n",
      "gs: 0.5771428571428572\n",
      "gs: 0.6341880341880342\n",
      "sasgf\n",
      "gs: 0.5971988795518207\n",
      "gs: 0.5669565217391305\n",
      "gs: 0.5428571428571429\n",
      "gs: 0.5345345345345346\n",
      "gs: 0.5174311926605505\n",
      "gs: 0.5577342047930283\n",
      "gs: 0.5452807646356034\n",
      "gs: 0.5107402652973898\n",
      "gs: 0.4623376623376624\n",
      "gs: 0.45339551151267854\n",
      "gs: 0.4752486501847116\n",
      "gs: 0.488327721661055\n",
      "gs: 0.4982495137538204\n",
      "gs: 0.5049107142857143\n",
      "gs: 0.5066666666666666\n",
      "gs: 0.5374909951631162\n",
      "gs: 0.5429610611895592\n",
      "gs: 0.5389063044235458\n",
      "gs: 0.5374741200828158\n",
      "gs: 0.5546666666666665\n",
      "gs: 0.5653912050256996\n",
      "gs: 0.5650494159928122\n",
      "gs: 0.5363636363636364\n",
      "gs: 0.552046783625731\n",
      "gs: 0.5789173789173788\n",
      "gs: 0.596078431372549\n",
      "gs: 0.596011396011396\n",
      "gs: 0.5934945788156798\n",
      "gs: 0.587936507936508\n",
      "gs: 0.5787041862080945\n",
      "gs: 0.5691216584833606\n",
      "gs: 0.5592364383230639\n",
      "gs: 0.5533485292521436\n",
      "gs: 0.5200344926703075\n",
      "gs: 0.4599275967697022\n",
      "gs: 0.48888888888888893\n",
      "gs: 0.46499786050492087\n",
      "gs: 0.4888888888888888\n",
      "gs: 0.45503472222222224\n",
      "gs: 0.47911669522177797\n",
      "gs: 0.5068283917340521\n",
      "gs: 0.5412121212121213\n",
      "gs: 0.5669565217391305\n",
      "gs: 0.5971988795518207\n",
      "gs: 0.5823361823361822\n",
      "gs: 0.5747126436781609\n",
      "gs: 0.5669565217391305\n",
      "gs: 0.5345345345345346\n",
      "gs: 0.47179487179487173\n",
      "gs: 0.3659574468085106\n",
      "gs: 0.35412186379928307\n",
      "gs: 0.3420289855072464\n",
      "gs: 0.2909090909090909\n",
      "gs: 0.34497354497354493\n",
      "gs: 0.39555418338993514\n",
      "gs: 0.4191387559808612\n",
      "gs: 0.42607407407407405\n",
      "gs: 0.4325891108499804\n",
      "gs: 0.4753379953379953\n",
      "gs: 0.48669074742984164\n",
      "gs: 0.49696969696969695\n",
      "gs: 0.497737556561086\n",
      "gs: 0.4960382068815803\n",
      "gs: 0.48873949579831943\n",
      "gs: 0.45517241379310336\n",
      "gs: 0.4190877192982456\n",
      "gs: 0.4076388888888889\n",
      "gs: 0.4546666666666667\n",
      "gs: 0.4512820512820513\n",
      "gs: 0.4834890965732088\n",
      "gs: 0.4938271604938272\n",
      "gs: 0.513939393939394\n",
      "gs: 0.5427728613569321\n",
      "gs: 0.5701149425287356\n",
      "gs: 0.5789173789173788\n",
      "gs: 0.596078431372549\n",
      "gs: 0.5669565217391305\n",
      "gs: 0.4215488215488214\n",
      "gs: 0.4\n",
      "gs: 0.304119850187266\n",
      "gs: 0.2909090909090909\n",
      "gs: 0.41175878385180714\n",
      "gs: 0.4444444444444444\n",
      "gs: 0.46803717262742894\n",
      "gs: 0.49703937913193447\n",
      "gs: 0.4975579975579977\n",
      "gs: 0.5036552100381888\n",
      "gs: 0.4512820512820513\n",
      "gs: 0.47295597484276725\n",
      "gs: 0.4938271604938272\n",
      "gs: 0.5237237237237237\n",
      "gs: 0.5333333333333333\n",
      "gs: 0.5427728613569321\n",
      "gs: 0.5611594202898551\n",
      "The alpha values -- i.e., the amount-of-says -- are:\n",
      "[0.36544375 0.54110924 0.61476166 0.64369347 0.50502969]\n"
     ]
    }
   ],
   "source": [
    "# We need to set the seed as the sampling is random\n",
    "np.random.seed(0)\n",
    "\n",
    "adaboost = AdaBoostTreeClassifier(n_estimators=5).fit(X_train, y_train)\n",
    "\n",
    "# adaboost_from_skl = AdaBoostClassifier(n_estimators=5).fit(X_train,y_train)\n",
    "\n",
    "print(\"The alpha values -- i.e., the amount-of-says -- are:\")\n",
    "print(adaboost.alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56104f15-a1bb-4775-b178-559df6fad71c",
   "metadata": {},
   "source": [
    "The output of previous cell should be:\n",
    "\n",
    "```\n",
    "The alpha values -- i.e., the amount-of-says -- are:\n",
    "[0.36544375 0.54110924 0.65028309 0.65364937 0.53673646]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31473312-a14c-4ad4-b7a5-40914965af8d",
   "metadata": {},
   "source": [
    "#### 2.2 b) Predicting the Class Labels (5 Points)\n",
    "\n",
    "As the last step, we now only need our AdaBoost classifier to predict the class labels for unseen data samples. Again, we saw in the lecture how this works: For each data sample, we check which of the `n_estimators` estimators predicts a certain class label, and the sum of all the alphas (i.e., the amounts of say) of the estimators of the same class.\n",
    "\n",
    "The skeleton code of the `AdaBoostTreeClassifier` already provides a method `predict()` which takes a list of data samples as input and calls the method `predict_sample()` to predict the class label for each data sample individually. It's not that difficult to do this completely vectorized without the loop over the data samples, but here we want to focus on the basic algorithm and not worry about performance.\n",
    "\n",
    "**Implement method predict_sample()** to predict the class label for a given data sample. Hint: Have again a look at [np.argwhere](https://numpy.org/doc/stable/reference/generated/numpy.argwhere.html) to maybe make your life easier. You can use the example calls below to test your implementation of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f037f2b0-2dbb-444a-9064-e9bf287e443c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:18:39.039262Z",
     "start_time": "2023-10-15T13:18:37.746371200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sasgf\n",
      "gs: 0.6609243697478991\n",
      "gs: 0.6435344827586207\n",
      "gs: 0.637536231884058\n",
      "gs: 0.6188988095238095\n",
      "gs: 0.6124624624624624\n",
      "gs: 0.5784591194968554\n",
      "gs: 0.5775664408546117\n",
      "gs: 0.5482477100756671\n",
      "gs: 0.5133402995471961\n",
      "gs: 0.49286334527298387\n",
      "gs: 0.4831514762516046\n",
      "gs: 0.43787878787878787\n",
      "gs: 0.45027306697326813\n",
      "gs: 0.4748316498316499\n",
      "gs: 0.46861111111111114\n",
      "gs: 0.4882483987747146\n",
      "gs: 0.4977622377622377\n",
      "gs: 0.5138641372041775\n",
      "gs: 0.5229739252995068\n",
      "gs: 0.5329851218740108\n",
      "gs: 0.559906520747834\n",
      "gs: 0.5802536231884059\n",
      "gs: 0.6017480950246528\n",
      "gs: 0.5977272727272727\n",
      "gs: 0.6062250142775556\n",
      "gs: 0.6123764600179695\n",
      "gs: 0.6234848484848485\n",
      "gs: 0.6138138138138138\n",
      "gs: 0.6200892857142857\n",
      "gs: 0.6382608695652174\n",
      "gs: 0.6441091954022988\n",
      "gs: 0.6498575498575497\n",
      "gs: 0.6610644257703081\n",
      "gs: 0.6607843137254903\n",
      "gs: 0.6561781609195401\n",
      "gs: 0.6524441635061105\n",
      "gs: 0.6415915915915916\n",
      "gs: 0.6412660256410257\n",
      "gs: 0.6353333333333333\n",
      "gs: 0.6244135297326787\n",
      "gs: 0.60032019971779\n",
      "gs: 0.5773333333333334\n",
      "gs: 0.5635267060921807\n",
      "gs: 0.5648595590456056\n",
      "gs: 0.5541609667122663\n",
      "gs: 0.5324554755589238\n",
      "gs: 0.5768542643738058\n",
      "gs: 0.6111185983827493\n",
      "gs: 0.6160689463441756\n",
      "gs: 0.6286286286286287\n",
      "gs: 0.637536231884058\n",
      "gs: 0.6494301994301994\n",
      "gs: 0.6552259887005649\n",
      "gs: 0.6609243697478991\n",
      "gs: 0.6609243697478991\n",
      "gs: 0.6552259887005649\n",
      "gs: 0.6494301994301994\n",
      "gs: 0.6124624624624624\n",
      "gs: 0.541089108910891\n",
      "gs: 0.45402930402930397\n",
      "gs: 0.4029069767441861\n",
      "gs: 0.35752032520325205\n",
      "gs: 0.333125\n",
      "gs: 0.3449521457239889\n",
      "gs: 0.3659921473874963\n",
      "gs: 0.37529904306220097\n",
      "gs: 0.3838518518518519\n",
      "gs: 0.39168625146886016\n",
      "gs: 0.405324074074074\n",
      "gs: 0.4286961419318502\n",
      "gs: 0.43745474798106376\n",
      "gs: 0.4387329813837177\n",
      "gs: 0.4368464961067854\n",
      "gs: 0.42595238095238097\n",
      "gs: 0.4145833333333333\n",
      "gs: 0.374650156045505\n",
      "gs: 0.388125\n",
      "gs: 0.41064814814814815\n",
      "gs: 0.44183211424590724\n",
      "gs: 0.4867021276595745\n",
      "gs: 0.495438596491228\n",
      "gs: 0.5039930555555555\n",
      "gs: 0.520578231292517\n",
      "gs: 0.5442244224422442\n",
      "gs: 0.5665064102564102\n",
      "gs: 0.5875389408099689\n",
      "gs: 0.6074242424242424\n",
      "gs: 0.6200892857142857\n",
      "gs: 0.6323099415204678\n",
      "gs: 0.6441091954022988\n",
      "gs: 0.6498575498575497\n",
      "gs: 0.6555084745762711\n",
      "gs: 0.6610644257703081\n",
      "gs: 0.637536231884058\n",
      "gs: 0.48209219858156027\n",
      "gs: 0.42405303030303027\n",
      "gs: 0.35752032520325205\n",
      "gs: 0.3454732510288066\n",
      "gs: 0.333125\n",
      "gs: 0.37529904306220097\n",
      "gs: 0.39883415913727777\n",
      "gs: 0.4210855356635408\n",
      "gs: 0.4386111111111111\n",
      "gs: 0.434973604826546\n",
      "gs: 0.3982401975918493\n",
      "gs: 0.3767068273092369\n",
      "gs: 0.3878968253968253\n",
      "gs: 0.4867021276595745\n",
      "gs: 0.5123711340206185\n",
      "gs: 0.551797385620915\n",
      "gs: 0.5942901234567901\n",
      "gs: 0.6074242424242424\n",
      "gs: 0.6441091954022988\n",
      "gs: 0.6498575498575497\n",
      "sasgf\n",
      "gs: 0.5776353276353277\n",
      "gs: 0.5615942028985508\n",
      "gs: 0.5277777777777778\n",
      "gs: 0.5099388379204892\n",
      "gs: 0.5072115384615384\n",
      "gs: 0.5163540163540163\n",
      "gs: 0.5042280414620841\n",
      "gs: 0.49621212121212116\n",
      "gs: 0.4592434314656537\n",
      "gs: 0.46533490011750883\n",
      "gs: 0.5\n",
      "gs: 0.4942113550060202\n",
      "gs: 0.5091841491841491\n",
      "gs: 0.5150112233445567\n",
      "gs: 0.5234741784037559\n",
      "gs: 0.53375\n",
      "gs: 0.5416666666666667\n",
      "gs: 0.5427777777777778\n",
      "gs: 0.5344484269215451\n",
      "gs: 0.5603612992878235\n",
      "gs: 0.5579918523843759\n",
      "gs: 0.5525757575757575\n",
      "gs: 0.5489766081871345\n",
      "gs: 0.5579710144927537\n",
      "gs: 0.5840395480225989\n",
      "gs: 0.592436974789916\n",
      "gs: 0.5969827586206896\n",
      "gs: 0.5903919089759796\n",
      "gs: 0.5822727272727274\n",
      "gs: 0.5498333333333334\n",
      "gs: 0.5555555555555555\n",
      "gs: 0.5464822533788052\n",
      "gs: 0.5202083333333334\n",
      "gs: 0.4498732751337651\n",
      "gs: 0.5129749314531923\n",
      "gs: 0.5099159663865547\n",
      "gs: 0.49492234169653526\n",
      "gs: 0.4729459788084071\n",
      "gs: 0.5277777777777778\n",
      "gs: 0.5449852507374632\n",
      "gs: 0.5533625730994153\n",
      "gs: 0.5615942028985508\n",
      "gs: 0.5696839080459771\n",
      "gs: 0.5776353276353277\n",
      "gs: 0.5931372549019608\n",
      "gs: 0.5776353276353277\n",
      "gs: 0.5615942028985508\n",
      "gs: 0.48191823899371067\n",
      "gs: 0.3758680555555556\n",
      "gs: 0.36403508771929827\n",
      "gs: 0.3141025641025641\n",
      "gs: 0.30092592592592593\n",
      "gs: 0.3295454545454544\n",
      "gs: 0.3857592532291327\n",
      "gs: 0.4259768009768009\n",
      "gs: 0.4325480720829558\n",
      "gs: 0.46785714285714286\n",
      "gs: 0.4900352243233222\n",
      "gs: 0.4942113550060202\n",
      "gs: 0.4958228905597327\n",
      "gs: 0.4964927048260382\n",
      "gs: 0.4763431013431013\n",
      "gs: 0.4596170303808301\n",
      "gs: 0.3926360995326513\n",
      "gs: 0.3823649754500817\n",
      "gs: 0.3676975945017183\n",
      "gs: 0.3801020408163265\n",
      "gs: 0.43851132686084127\n",
      "gs: 0.46031746031746035\n",
      "gs: 0.4709119496855345\n",
      "gs: 0.5015290519877676\n",
      "gs: 0.5113636363636364\n",
      "gs: 0.5398230088495575\n",
      "gs: 0.5489766081871345\n",
      "gs: 0.5579710144927537\n",
      "gs: 0.5840395480225989\n",
      "gs: 0.592436974789916\n",
      "gs: 0.5533625730994153\n",
      "gs: 0.4315181518151815\n",
      "gs: 0.3874570446735396\n",
      "gs: 0.32699275362318836\n",
      "gs: 0.30092592592592593\n",
      "gs: 0.3759920634920635\n",
      "gs: 0.4325480720829558\n",
      "gs: 0.45949074074074076\n",
      "gs: 0.4958228905597327\n",
      "gs: 0.49644444444444435\n",
      "gs: 0.4173636001592991\n",
      "gs: 0.3801020408163265\n",
      "gs: 0.46031746031746035\n",
      "gs: 0.4709119496855345\n",
      "gs: 0.5015290519877676\n",
      "gs: 0.5489766081871345\n",
      "gs: 0.5840395480225989\n",
      "gs: 0.592436974789916\n",
      "sasgf\n",
      "gs: 0.6015536723163841\n",
      "gs: 0.5925925925925926\n",
      "gs: 0.5742028985507246\n",
      "gs: 0.5544544544544545\n",
      "gs: 0.536769991015274\n",
      "gs: 0.5008163974292166\n",
      "gs: 0.4792207792207792\n",
      "gs: 0.45644703421485133\n",
      "gs: 0.45846273291925466\n",
      "gs: 0.4806818181818182\n",
      "gs: 0.4854700854700855\n",
      "gs: 0.5049840510366825\n",
      "gs: 0.5225694444444444\n",
      "gs: 0.522765246449457\n",
      "gs: 0.5154024265999815\n",
      "gs: 0.5049845114052379\n",
      "gs: 0.5167103856990187\n",
      "gs: 0.5314583333333334\n",
      "gs: 0.570123106060606\n",
      "gs: 0.564353026458862\n",
      "gs: 0.5643889798145117\n",
      "gs: 0.561976911976912\n",
      "gs: 0.5661717114030078\n",
      "gs: 0.5552884615384616\n",
      "gs: 0.5643081761006289\n",
      "gs: 0.5854354354354355\n",
      "gs: 0.5933628318584071\n",
      "gs: 0.6131652661064425\n",
      "gs: 0.6050724637681159\n",
      "gs: 0.593489254108723\n",
      "gs: 0.5873511904761906\n",
      "gs: 0.5963492063492064\n",
      "gs: 0.593987493987494\n",
      "gs: 0.6010553564317005\n",
      "gs: 0.5985401063714316\n",
      "gs: 0.5913241352879179\n",
      "gs: 0.5787116176005065\n",
      "gs: 0.5793087121212122\n",
      "gs: 0.5411736411736412\n",
      "gs: 0.5247596153846154\n",
      "gs: 0.5373227689741451\n",
      "gs: 0.5847953216374268\n",
      "gs: 0.5834770114942529\n",
      "gs: 0.6015536723163841\n",
      "gs: 0.6103641456582634\n",
      "gs: 0.6103641456582634\n",
      "gs: 0.5149847094801222\n",
      "gs: 0.47174603174603164\n",
      "gs: 0.4604166666666667\n",
      "gs: 0.4128333333333334\n",
      "gs: 0.4003367003367003\n",
      "gs: 0.41046691403834257\n",
      "gs: 0.426388888888889\n",
      "gs: 0.4324912280701755\n",
      "gs: 0.4494949494949495\n",
      "gs: 0.4395057766367138\n",
      "gs: 0.417597905969999\n",
      "gs: 0.3992264003133569\n",
      "gs: 0.35998863313441315\n",
      "gs: 0.341171500985638\n",
      "gs: 0.2856414534668151\n",
      "gs: 0.2964494569757728\n",
      "gs: 0.33711281614094907\n",
      "gs: 0.37521616632662974\n",
      "gs: 0.43433734939759033\n",
      "gs: 0.44146825396825395\n",
      "gs: 0.44843137254901966\n",
      "gs: 0.45523255813953484\n",
      "gs: 0.4809259259259259\n",
      "gs: 0.4987455197132617\n",
      "gs: 0.5207903780068729\n",
      "gs: 0.5459150326797385\n",
      "gs: 0.5643081761006289\n",
      "gs: 0.5813636363636364\n",
      "gs: 0.6010144927536232\n",
      "gs: 0.6084045584045583\n",
      "gs: 0.5834770114942529\n",
      "gs: 0.49376947040498453\n",
      "gs: 0.44886731391585755\n",
      "gs: 0.4128333333333334\n",
      "gs: 0.4003367003367003\n",
      "gs: 0.44157706093189963\n",
      "gs: 0.44474637681159424\n",
      "gs: 0.4485522115823074\n",
      "gs: 0.417597905969999\n",
      "gs: 0.35998863313441315\n",
      "gs: 0.31027777777777776\n",
      "gs: 0.2565104166666667\n",
      "gs: 0.43433734939759033\n",
      "gs: 0.4618773946360153\n",
      "gs: 0.4987455197132617\n",
      "gs: 0.5207903780068729\n",
      "gs: 0.5506472491909385\n",
      "gs: 0.6047413793103449\n",
      "gs: 0.6084045584045583\n",
      "sasgf\n",
      "gs: 0.6350282485875706\n",
      "gs: 0.6105263157894737\n",
      "gs: 0.6041297935103245\n",
      "gs: 0.541025641025641\n",
      "gs: 0.5438219034812491\n",
      "gs: 0.4993844696969697\n",
      "gs: 0.4773809523809523\n",
      "gs: 0.4575074611505609\n",
      "gs: 0.4136296296296297\n",
      "gs: 0.4127814506084124\n",
      "gs: 0.4138927738927739\n",
      "gs: 0.42500696184906717\n",
      "gs: 0.43513939057145495\n",
      "gs: 0.44942129629629624\n",
      "gs: 0.4952833175055398\n",
      "gs: 0.5098230760881363\n",
      "gs: 0.5366074538488331\n",
      "gs: 0.5670485542825968\n",
      "gs: 0.5777777777777778\n",
      "gs: 0.5807348181991242\n",
      "gs: 0.5823899371069183\n",
      "gs: 0.6072072072072072\n",
      "gs: 0.6165191740412979\n",
      "gs: 0.642577030812325\n",
      "gs: 0.6392156862745098\n",
      "gs: 0.6362410450906025\n",
      "gs: 0.6292292292292292\n",
      "gs: 0.6242424242424243\n",
      "gs: 0.6151467778356783\n",
      "gs: 0.5802343844388064\n",
      "gs: 0.5663243474540008\n",
      "gs: 0.5424242424242424\n",
      "gs: 0.5375390736004546\n",
      "gs: 0.5852799090650753\n",
      "gs: 0.5714583333333334\n",
      "gs: 0.5585598646852724\n",
      "gs: 0.5400701754385965\n",
      "gs: 0.5891975308641976\n",
      "gs: 0.6150297619047619\n",
      "gs: 0.6210703750526758\n",
      "gs: 0.6229885057471264\n",
      "gs: 0.6350282485875706\n",
      "gs: 0.6408963585434174\n",
      "gs: 0.6350282485875706\n",
      "gs: 0.629059829059829\n",
      "gs: 0.5976190476190477\n",
      "gs: 0.5174917491749175\n",
      "gs: 0.44731182795698926\n",
      "gs: 0.3862068965517241\n",
      "gs: 0.3284552845528455\n",
      "gs: 0.3033333333333333\n",
      "gs: 0.3124626942471957\n",
      "gs: 0.3206349206349206\n",
      "gs: 0.3278969092922582\n",
      "gs: 0.3342902711323764\n",
      "gs: 0.34461417939678807\n",
      "gs: 0.35731741972151176\n",
      "gs: 0.3566778900112234\n",
      "gs: 0.3529761904761905\n",
      "gs: 0.3624242424242424\n",
      "gs: 0.3556677890011224\n",
      "gs: 0.34000754147812967\n",
      "gs: 0.34247619047619043\n",
      "gs: 0.35356903324710165\n",
      "gs: 0.4231990231990233\n",
      "gs: 0.4462745098039215\n",
      "gs: 0.4620689655172414\n",
      "gs: 0.4771535580524344\n",
      "gs: 0.4985507246376812\n",
      "gs: 0.5120567375886524\n",
      "gs: 0.5607843137254902\n",
      "gs: 0.5875389408099689\n",
      "gs: 0.5975535168195718\n",
      "gs: 0.6072072072072072\n",
      "gs: 0.6165191740412979\n",
      "gs: 0.6210526315789474\n",
      "gs: 0.6255072463768117\n",
      "gs: 0.6105263157894737\n",
      "gs: 0.4659649122807017\n",
      "gs: 0.4074906367041199\n",
      "gs: 0.3405622489959839\n",
      "gs: 0.31604938271604943\n",
      "gs: 0.3033333333333333\n",
      "gs: 0.3342902711323764\n",
      "gs: 0.34461417939678807\n",
      "gs: 0.34860584863499466\n",
      "gs: 0.35731741972151176\n",
      "gs: 0.415029761904762\n",
      "gs: 0.4395534290271133\n",
      "gs: 0.4123456790123457\n",
      "gs: 0.4380952380952381\n",
      "gs: 0.4915750915750916\n",
      "gs: 0.5185964912280702\n",
      "gs: 0.5493333333333333\n",
      "gs: 0.5663430420711973\n",
      "gs: 0.5771428571428572\n",
      "gs: 0.6341880341880342\n",
      "sasgf\n",
      "gs: 0.5971988795518207\n",
      "gs: 0.5669565217391305\n",
      "gs: 0.5428571428571429\n",
      "gs: 0.5345345345345346\n",
      "gs: 0.5174311926605505\n",
      "gs: 0.5577342047930283\n",
      "gs: 0.5452807646356034\n",
      "gs: 0.5107402652973898\n",
      "gs: 0.4623376623376624\n",
      "gs: 0.45339551151267854\n",
      "gs: 0.4752486501847116\n",
      "gs: 0.488327721661055\n",
      "gs: 0.4982495137538204\n",
      "gs: 0.5049107142857143\n",
      "gs: 0.5066666666666666\n",
      "gs: 0.5374909951631162\n",
      "gs: 0.5429610611895592\n",
      "gs: 0.5389063044235458\n",
      "gs: 0.5374741200828158\n",
      "gs: 0.5546666666666665\n",
      "gs: 0.5653912050256996\n",
      "gs: 0.5650494159928122\n",
      "gs: 0.5363636363636364\n",
      "gs: 0.552046783625731\n",
      "gs: 0.5789173789173788\n",
      "gs: 0.596078431372549\n",
      "gs: 0.596011396011396\n",
      "gs: 0.5934945788156798\n",
      "gs: 0.587936507936508\n",
      "gs: 0.5787041862080945\n",
      "gs: 0.5691216584833606\n",
      "gs: 0.5592364383230639\n",
      "gs: 0.5533485292521436\n",
      "gs: 0.5200344926703075\n",
      "gs: 0.4599275967697022\n",
      "gs: 0.48888888888888893\n",
      "gs: 0.46499786050492087\n",
      "gs: 0.4888888888888888\n",
      "gs: 0.45503472222222224\n",
      "gs: 0.47911669522177797\n",
      "gs: 0.5068283917340521\n",
      "gs: 0.5412121212121213\n",
      "gs: 0.5669565217391305\n",
      "gs: 0.5971988795518207\n",
      "gs: 0.5823361823361822\n",
      "gs: 0.5747126436781609\n",
      "gs: 0.5669565217391305\n",
      "gs: 0.5345345345345346\n",
      "gs: 0.47179487179487173\n",
      "gs: 0.3659574468085106\n",
      "gs: 0.35412186379928307\n",
      "gs: 0.3420289855072464\n",
      "gs: 0.2909090909090909\n",
      "gs: 0.34497354497354493\n",
      "gs: 0.39555418338993514\n",
      "gs: 0.4191387559808612\n",
      "gs: 0.42607407407407405\n",
      "gs: 0.4325891108499804\n",
      "gs: 0.4753379953379953\n",
      "gs: 0.48669074742984164\n",
      "gs: 0.49696969696969695\n",
      "gs: 0.497737556561086\n",
      "gs: 0.4960382068815803\n",
      "gs: 0.48873949579831943\n",
      "gs: 0.45517241379310336\n",
      "gs: 0.4190877192982456\n",
      "gs: 0.4076388888888889\n",
      "gs: 0.4546666666666667\n",
      "gs: 0.4512820512820513\n",
      "gs: 0.4834890965732088\n",
      "gs: 0.4938271604938272\n",
      "gs: 0.513939393939394\n",
      "gs: 0.5427728613569321\n",
      "gs: 0.5701149425287356\n",
      "gs: 0.5789173789173788\n",
      "gs: 0.596078431372549\n",
      "gs: 0.5669565217391305\n",
      "gs: 0.4215488215488214\n",
      "gs: 0.4\n",
      "gs: 0.304119850187266\n",
      "gs: 0.2909090909090909\n",
      "gs: 0.41175878385180714\n",
      "gs: 0.4444444444444444\n",
      "gs: 0.46803717262742894\n",
      "gs: 0.49703937913193447\n",
      "gs: 0.4975579975579977\n",
      "gs: 0.5036552100381888\n",
      "gs: 0.4512820512820513\n",
      "gs: 0.47295597484276725\n",
      "gs: 0.4938271604938272\n",
      "gs: 0.5237237237237237\n",
      "gs: 0.5333333333333333\n",
      "gs: 0.5427728613569321\n",
      "gs: 0.5611594202898551\n",
      "[0 2 0 0 2 0 2 1 1 1 2 2 1 1 0 1 2 2 0 1 1 2 1 0 0 0 2 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "# We need to set the seed as the sampling is random\n",
    "np.random.seed(0)\n",
    "\n",
    "adaboost = AdaBoostTreeClassifier(n_estimators=5).fit(X_train, y_train)\n",
    "\n",
    "print(adaboost.predict(X_test))\n",
    "\n",
    "#TODO.x the results are wrong here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5f3b08-9358-475b-819a-fd54330b45bb",
   "metadata": {},
   "source": [
    "The output of previous cell should be:\n",
    "\n",
    "```\n",
    "[0 2 0 0 2 0 2 1 1 1 2 2 1 2 0 1 2 2 0 1 1 2 1 0 0 0 2 1 2 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bdb6f2-f184-419c-a271-f307da7f6fbd",
   "metadata": {},
   "source": [
    "**Testing your Implementation on the IRIS Dataset.** We the code cell below, you can again directly compare the result your AdaBoost implementation with the scikit-learn [`AdaBoostClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html). Two things to note:\n",
    "\n",
    "* By default, `AdaBoostClassifier` uses the `DecisionTreeClassifier` class as the estimators (i.e., the Weak Learners) with `max_depth=1`\n",
    "\n",
    "* By default, `AdaBoostClassifier` sets `n_estimators=50`; we therefore choose the same default value for `AdaBoostTreeClassifier`\n",
    "\n",
    "As a result, we do not have to set any parameters for `AdaBoostClassifier` and can use the default ones to allow for a fair comparison with your implementation of `AdaBoostTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b52fc85-d2cf-4a3f-9b8b-11a25034bc87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:19:04.320751600Z",
     "start_time": "2023-10-15T13:19:03.662765100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sasgf\n",
      "gs: 0.6609243697478991\n",
      "gs: 0.6435344827586207\n",
      "gs: 0.637536231884058\n",
      "gs: 0.6188988095238095\n",
      "gs: 0.6124624624624624\n",
      "gs: 0.5784591194968554\n",
      "gs: 0.5775664408546117\n",
      "gs: 0.5482477100756671\n",
      "gs: 0.5133402995471961\n",
      "gs: 0.49286334527298387\n",
      "gs: 0.4831514762516046\n",
      "gs: 0.43787878787878787\n",
      "gs: 0.45027306697326813\n",
      "gs: 0.4748316498316499\n",
      "gs: 0.46861111111111114\n",
      "gs: 0.4882483987747146\n",
      "gs: 0.4977622377622377\n",
      "gs: 0.5138641372041775\n",
      "gs: 0.5229739252995068\n",
      "gs: 0.5329851218740108\n",
      "gs: 0.559906520747834\n",
      "gs: 0.5802536231884059\n",
      "gs: 0.6017480950246528\n",
      "gs: 0.5977272727272727\n",
      "gs: 0.6062250142775556\n",
      "gs: 0.6123764600179695\n",
      "gs: 0.6234848484848485\n",
      "gs: 0.6138138138138138\n",
      "gs: 0.6200892857142857\n",
      "gs: 0.6382608695652174\n",
      "gs: 0.6441091954022988\n",
      "gs: 0.6498575498575497\n",
      "gs: 0.6610644257703081\n",
      "gs: 0.6607843137254903\n",
      "gs: 0.6561781609195401\n",
      "gs: 0.6524441635061105\n",
      "gs: 0.6415915915915916\n",
      "gs: 0.6412660256410257\n",
      "gs: 0.6353333333333333\n",
      "gs: 0.6244135297326787\n",
      "gs: 0.60032019971779\n",
      "gs: 0.5773333333333334\n",
      "gs: 0.5635267060921807\n",
      "gs: 0.5648595590456056\n",
      "gs: 0.5541609667122663\n",
      "gs: 0.5324554755589238\n",
      "gs: 0.5768542643738058\n",
      "gs: 0.6111185983827493\n",
      "gs: 0.6160689463441756\n",
      "gs: 0.6286286286286287\n",
      "gs: 0.637536231884058\n",
      "gs: 0.6494301994301994\n",
      "gs: 0.6552259887005649\n",
      "gs: 0.6609243697478991\n",
      "gs: 0.6609243697478991\n",
      "gs: 0.6552259887005649\n",
      "gs: 0.6494301994301994\n",
      "gs: 0.6124624624624624\n",
      "gs: 0.541089108910891\n",
      "gs: 0.45402930402930397\n",
      "gs: 0.4029069767441861\n",
      "gs: 0.35752032520325205\n",
      "gs: 0.333125\n",
      "gs: 0.3449521457239889\n",
      "gs: 0.3659921473874963\n",
      "gs: 0.37529904306220097\n",
      "gs: 0.3838518518518519\n",
      "gs: 0.39168625146886016\n",
      "gs: 0.405324074074074\n",
      "gs: 0.4286961419318502\n",
      "gs: 0.43745474798106376\n",
      "gs: 0.4387329813837177\n",
      "gs: 0.4368464961067854\n",
      "gs: 0.42595238095238097\n",
      "gs: 0.4145833333333333\n",
      "gs: 0.374650156045505\n",
      "gs: 0.388125\n",
      "gs: 0.41064814814814815\n",
      "gs: 0.44183211424590724\n",
      "gs: 0.4867021276595745\n",
      "gs: 0.495438596491228\n",
      "gs: 0.5039930555555555\n",
      "gs: 0.520578231292517\n",
      "gs: 0.5442244224422442\n",
      "gs: 0.5665064102564102\n",
      "gs: 0.5875389408099689\n",
      "gs: 0.6074242424242424\n",
      "gs: 0.6200892857142857\n",
      "gs: 0.6323099415204678\n",
      "gs: 0.6441091954022988\n",
      "gs: 0.6498575498575497\n",
      "gs: 0.6555084745762711\n",
      "gs: 0.6610644257703081\n",
      "gs: 0.637536231884058\n",
      "gs: 0.48209219858156027\n",
      "gs: 0.42405303030303027\n",
      "gs: 0.35752032520325205\n",
      "gs: 0.3454732510288066\n",
      "gs: 0.333125\n",
      "gs: 0.37529904306220097\n",
      "gs: 0.39883415913727777\n",
      "gs: 0.4210855356635408\n",
      "gs: 0.4386111111111111\n",
      "gs: 0.434973604826546\n",
      "gs: 0.3982401975918493\n",
      "gs: 0.3767068273092369\n",
      "gs: 0.3878968253968253\n",
      "gs: 0.4867021276595745\n",
      "gs: 0.5123711340206185\n",
      "gs: 0.551797385620915\n",
      "gs: 0.5942901234567901\n",
      "gs: 0.6074242424242424\n",
      "gs: 0.6441091954022988\n",
      "gs: 0.6498575498575497\n",
      "sasgf\n",
      "gs: 0.5298319327731092\n",
      "gs: 0.5109686609686608\n",
      "gs: 0.5012931034482759\n",
      "gs: 0.49144927536231886\n",
      "gs: 0.4608630952380953\n",
      "gs: 0.46875149772346036\n",
      "gs: 0.4980875541610639\n",
      "gs: 0.4974086197490452\n",
      "gs: 0.4932453416149068\n",
      "gs: 0.4775925925925926\n",
      "gs: 0.44521008403361345\n",
      "gs: 0.46343700159489637\n",
      "gs: 0.47159720198192956\n",
      "gs: 0.4696220517192384\n",
      "gs: 0.47139894419306183\n",
      "gs: 0.48072960178223334\n",
      "gs: 0.4936868686868686\n",
      "gs: 0.49152711323763953\n",
      "gs: 0.4980851519041506\n",
      "gs: 0.4933893557422969\n",
      "gs: 0.47704938739421493\n",
      "gs: 0.5111111111111111\n",
      "gs: 0.516826923076923\n",
      "gs: 0.5229229229229229\n",
      "gs: 0.5136231884057971\n",
      "gs: 0.501867816091954\n",
      "gs: 0.5113960113960113\n",
      "gs: 0.5299719887955183\n",
      "gs: 0.5259587020648967\n",
      "gs: 0.5283333333333333\n",
      "gs: 0.5265079365079365\n",
      "gs: 0.5184905332638527\n",
      "gs: 0.52746863066012\n",
      "gs: 0.5228432487053176\n",
      "gs: 0.5153779171020549\n",
      "gs: 0.500140872437579\n",
      "gs: 0.48394597605123924\n",
      "gs: 0.4909290271132376\n",
      "gs: 0.4862215909090909\n",
      "gs: 0.4703333333333334\n",
      "gs: 0.4471254521225965\n",
      "gs: 0.4664664664664665\n",
      "gs: 0.4766369047619048\n",
      "gs: 0.5055072463768117\n",
      "gs: 0.5204802259887006\n",
      "gs: 0.5298319327731092\n",
      "gs: 0.5298319327731092\n",
      "gs: 0.49144927536231886\n",
      "gs: 0.4503003003003004\n",
      "gs: 0.394496855345912\n",
      "gs: 0.35841423948220064\n",
      "gs: 0.3069023569023569\n",
      "gs: 0.29336734693877564\n",
      "gs: 0.3220486111111111\n",
      "gs: 0.3564317005177221\n",
      "gs: 0.41022571819425446\n",
      "gs: 0.4209656084656084\n",
      "gs: 0.42575708238358834\n",
      "gs: 0.45065789473684204\n",
      "gs: 0.46521493212669685\n",
      "gs: 0.46769230769230763\n",
      "gs: 0.4686111111111111\n",
      "gs: 0.458074074074074\n",
      "gs: 0.45266284103493404\n",
      "gs: 0.39484112601184\n",
      "gs: 0.39722222222222225\n",
      "gs: 0.3768333333333334\n",
      "gs: 0.38890046899426783\n",
      "gs: 0.40809968847352024\n",
      "gs: 0.43027522935779816\n",
      "gs: 0.44106060606060604\n",
      "gs: 0.48230994152046774\n",
      "gs: 0.5113960113960113\n",
      "gs: 0.5299719887955183\n",
      "gs: 0.5012931034482759\n",
      "gs: 0.35841423948220064\n",
      "gs: 0.33316831683168324\n",
      "gs: 0.29336734693877564\n",
      "gs: 0.3977272727272727\n",
      "gs: 0.4209656084656084\n",
      "gs: 0.4531851851851852\n",
      "gs: 0.4685324422166528\n",
      "gs: 0.4602159704461495\n",
      "gs: 0.3872222222222222\n",
      "gs: 0.297108843537415\n",
      "gs: 0.31043771043771046\n",
      "gs: 0.4192901234567902\n",
      "gs: 0.48230994152046774\n",
      "gs: 0.501867816091954\n",
      "gs: 0.5113960113960113\n",
      "sasgf\n",
      "gs: 0.5869747899159664\n",
      "gs: 0.5679487179487179\n",
      "gs: 0.5581896551724137\n",
      "gs: 0.5718750000000001\n",
      "gs: 0.5584236864053377\n",
      "gs: 0.5204761904761904\n",
      "gs: 0.5100961538461538\n",
      "gs: 0.46831578947368424\n",
      "gs: 0.47003105590062116\n",
      "gs: 0.4850852272727273\n",
      "gs: 0.4937419768934531\n",
      "gs: 0.5118555109601729\n",
      "gs: 0.5101037851037851\n",
      "gs: 0.5042420814479637\n",
      "gs: 0.49530033370411575\n",
      "gs: 0.48189655172413803\n",
      "gs: 0.5210855356635408\n",
      "gs: 0.5145575354877681\n",
      "gs: 0.5198700410396717\n",
      "gs: 0.5109717868338558\n",
      "gs: 0.5236111111111111\n",
      "gs: 0.5405\n",
      "gs: 0.5560096153846154\n",
      "gs: 0.5457142857142857\n",
      "gs: 0.5495283018867925\n",
      "gs: 0.564090909090909\n",
      "gs: 0.5675675675675675\n",
      "gs: 0.5743362831858407\n",
      "gs: 0.5871794871794873\n",
      "gs: 0.5899159663865546\n",
      "gs: 0.5951566951566951\n",
      "gs: 0.594396551724138\n",
      "gs: 0.5869883040935673\n",
      "gs: 0.5869788692175899\n",
      "gs: 0.5850217864923747\n",
      "gs: 0.5865263157894737\n",
      "gs: 0.5798987021209243\n",
      "gs: 0.5604166666666667\n",
      "gs: 0.5463572267920094\n",
      "gs: 0.5565147625160461\n",
      "gs: 0.5503789314134141\n",
      "gs: 0.5393939393939393\n",
      "gs: 0.5464959568733154\n",
      "gs: 0.55470884255931\n",
      "gs: 0.5455379482902418\n",
      "gs: 0.5545454545454546\n",
      "gs: 0.5581896551724137\n",
      "gs: 0.5869747899159664\n",
      "gs: 0.5869747899159664\n",
      "gs: 0.5775423728813559\n",
      "gs: 0.5581896551724137\n",
      "gs: 0.5482608695652175\n",
      "gs: 0.48486238532110093\n",
      "gs: 0.47361111111111115\n",
      "gs: 0.4264423076923078\n",
      "gs: 0.40147058823529413\n",
      "gs: 0.41083897863470564\n",
      "gs: 0.4184999999999999\n",
      "gs: 0.4246753246753247\n",
      "gs: 0.43768421052631584\n",
      "gs: 0.43687002652519896\n",
      "gs: 0.4174789915966386\n",
      "gs: 0.40706610224682516\n",
      "gs: 0.3527162977867204\n",
      "gs: 0.3213235294117647\n",
      "gs: 0.27265624999999993\n",
      "gs: 0.2844611528822055\n",
      "gs: 0.34736842105263166\n",
      "gs: 0.41454440931302095\n",
      "gs: 0.41265822784810124\n",
      "gs: 0.41937500000000005\n",
      "gs: 0.42592592592592604\n",
      "gs: 0.43855421686746987\n",
      "gs: 0.46761363636363623\n",
      "gs: 0.47833333333333333\n",
      "gs: 0.5123711340206185\n",
      "gs: 0.5338235294117647\n",
      "gs: 0.5495283018867925\n",
      "gs: 0.5569444444444445\n",
      "gs: 0.5743362831858407\n",
      "gs: 0.5776315789473684\n",
      "gs: 0.5871794871794873\n",
      "gs: 0.5932773109243697\n",
      "gs: 0.5679487179487179\n",
      "gs: 0.47361111111111115\n",
      "gs: 0.4621495327102805\n",
      "gs: 0.41407766990291267\n",
      "gs: 0.40147058823529413\n",
      "gs: 0.4184999999999999\n",
      "gs: 0.4246753246753247\n",
      "gs: 0.43593750000000003\n",
      "gs: 0.4125\n",
      "gs: 0.3948717948717949\n",
      "gs: 0.30991270064770493\n",
      "gs: 0.25409836065573765\n",
      "gs: 0.27619047619047626\n",
      "gs: 0.384\n",
      "gs: 0.41265822784810124\n",
      "gs: 0.4730337078651686\n",
      "gs: 0.5123711340206185\n",
      "gs: 0.5212121212121211\n",
      "gs: 0.564090909090909\n",
      "gs: 0.5743362831858407\n",
      "sasgf\n",
      "gs: 0.6408963585434174\n",
      "gs: 0.629059829059829\n",
      "gs: 0.6168115942028985\n",
      "gs: 0.5632398753894081\n",
      "gs: 0.5642701525054467\n",
      "gs: 0.4996376811594203\n",
      "gs: 0.43215686274509807\n",
      "gs: 0.42913274720503625\n",
      "gs: 0.41831407787762087\n",
      "gs: 0.3909645909645909\n",
      "gs: 0.41313514038667054\n",
      "gs: 0.4168591007228011\n",
      "gs: 0.40213022135778465\n",
      "gs: 0.42098628105302194\n",
      "gs: 0.43091060985797824\n",
      "gs: 0.44725028058361394\n",
      "gs: 0.45847619047619054\n",
      "gs: 0.4629629629629629\n",
      "gs: 0.5195948084836974\n",
      "gs: 0.5285714285714285\n",
      "gs: 0.566097308488613\n",
      "gs: 0.5557555919258046\n",
      "gs: 0.5643333333333334\n",
      "gs: 0.5859775641025641\n",
      "gs: 0.6058641975308643\n",
      "gs: 0.5975535168195718\n",
      "gs: 0.6341880341880342\n",
      "gs: 0.6384180790960452\n",
      "gs: 0.642577030812325\n",
      "gs: 0.6392156862745098\n",
      "gs: 0.6421652421652421\n",
      "gs: 0.6359420289855073\n",
      "gs: 0.6241689128481581\n",
      "gs: 0.6109668109668109\n",
      "gs: 0.5954451345755694\n",
      "gs: 0.575\n",
      "gs: 0.5673513050919983\n",
      "gs: 0.5261111111111112\n",
      "gs: 0.5487268518518519\n",
      "gs: 0.5194444444444444\n",
      "gs: 0.48444444444444446\n",
      "gs: 0.5256237860451218\n",
      "gs: 0.5920934411500449\n",
      "gs: 0.5958854601056436\n",
      "gs: 0.6088088088088088\n",
      "gs: 0.629059829059829\n",
      "gs: 0.6408963585434174\n",
      "gs: 0.6408963585434174\n",
      "gs: 0.6350282485875706\n",
      "gs: 0.629059829059829\n",
      "gs: 0.6041297935103245\n",
      "gs: 0.5559748427672956\n",
      "gs: 0.49251700680272104\n",
      "gs: 0.3969696969696969\n",
      "gs: 0.35238095238095235\n",
      "gs: 0.3033333333333333\n",
      "gs: 0.3124626942471957\n",
      "gs: 0.3342902711323764\n",
      "gs: 0.3518518518518518\n",
      "gs: 0.35437386222094464\n",
      "gs: 0.3561904761904761\n",
      "gs: 0.3529761904761905\n",
      "gs: 0.35074404761904765\n",
      "gs: 0.34396270396270395\n",
      "gs: 0.3095765842568911\n",
      "gs: 0.33275845549487404\n",
      "gs: 0.39506694855532076\n",
      "gs: 0.4915750915750916\n",
      "gs: 0.4985507246376812\n",
      "gs: 0.5053763440860215\n",
      "gs: 0.5185964912280702\n",
      "gs: 0.5374149659863946\n",
      "gs: 0.555115511551155\n",
      "gs: 0.5717948717948718\n",
      "gs: 0.611904761904762\n",
      "gs: 0.6210526315789474\n",
      "gs: 0.6298850574712643\n",
      "gs: 0.6341880341880342\n",
      "gs: 0.6384180790960452\n",
      "gs: 0.642577030812325\n",
      "gs: 0.6229885057471264\n",
      "gs: 0.47500000000000003\n",
      "gs: 0.4278388278388278\n",
      "gs: 0.3751937984496124\n",
      "gs: 0.36392156862745095\n",
      "gs: 0.3033333333333333\n",
      "gs: 0.3124626942471957\n",
      "gs: 0.3278969092922582\n",
      "gs: 0.34860584863499466\n",
      "gs: 0.357551863324885\n",
      "gs: 0.3670093544503103\n",
      "gs: 0.37483726804624495\n",
      "gs: 0.4123456790123457\n",
      "gs: 0.4297188755020081\n",
      "gs: 0.4771535580524344\n",
      "gs: 0.5053763440860215\n",
      "gs: 0.555115511551155\n",
      "gs: 0.5925925925925927\n",
      "gs: 0.5975535168195718\n",
      "gs: 0.6298850574712643\n",
      "gs: 0.6341880341880342\n",
      "sasgf\n",
      "gs: 0.5942090395480225\n",
      "gs: 0.5765804597701151\n",
      "gs: 0.5583333333333333\n",
      "gs: 0.5198484848484849\n",
      "gs: 0.5477777777777778\n",
      "gs: 0.5394420394420394\n",
      "gs: 0.5170877192982455\n",
      "gs: 0.5375925925925926\n",
      "gs: 0.5292618098344811\n",
      "gs: 0.4991316617822642\n",
      "gs: 0.5070601851851851\n",
      "gs: 0.507095238095238\n",
      "gs: 0.5010416666666667\n",
      "gs: 0.5241548578308789\n",
      "gs: 0.5227777777777777\n",
      "gs: 0.5342571644042232\n",
      "gs: 0.5377301214257737\n",
      "gs: 0.5453308008863564\n",
      "gs: 0.5761837121212121\n",
      "gs: 0.5649633945913641\n",
      "gs: 0.5784696890741706\n",
      "gs: 0.5740196078431372\n",
      "gs: 0.5811111111111111\n",
      "gs: 0.5691468967169903\n",
      "gs: 0.5759259259259258\n",
      "gs: 0.545945945945946\n",
      "gs: 0.568859649122807\n",
      "gs: 0.5762318840579711\n",
      "gs: 0.583477011494253\n",
      "gs: 0.6044817927170869\n",
      "gs: 0.5974702380952381\n",
      "gs: 0.5845679012345679\n",
      "gs: 0.5586937641132534\n",
      "gs: 0.5590877192982456\n",
      "gs: 0.5642036124794745\n",
      "gs: 0.5605286008900466\n",
      "gs: 0.566183574879227\n",
      "gs: 0.5260381593714927\n",
      "gs: 0.5575925925925925\n",
      "gs: 0.5583196944899073\n",
      "gs: 0.523202614379085\n",
      "gs: 0.5044444444444444\n",
      "gs: 0.5434806783430636\n",
      "gs: 0.5697916666666667\n",
      "gs: 0.5747076023391813\n",
      "gs: 0.5831884057971015\n",
      "gs: 0.5854700854700855\n",
      "gs: 0.6028011204481792\n",
      "gs: 0.6028011204481792\n",
      "gs: 0.5854700854700855\n",
      "gs: 0.5765804597701151\n",
      "gs: 0.548967551622419\n",
      "gs: 0.46761904761904766\n",
      "gs: 0.42211221122112214\n",
      "gs: 0.39797979797979793\n",
      "gs: 0.37285223367697595\n",
      "gs: 0.3866319444444445\n",
      "gs: 0.44592592592592584\n",
      "gs: 0.45285731545245855\n",
      "gs: 0.47456582633053224\n",
      "gs: 0.49104166666666665\n",
      "gs: 0.49866666666666665\n",
      "gs: 0.4879142300194932\n",
      "gs: 0.4685324422166528\n",
      "gs: 0.4549943883277217\n",
      "gs: 0.41562962962962957\n",
      "gs: 0.40751594896331733\n",
      "gs: 0.32175925925925924\n",
      "gs: 0.3545787545787545\n",
      "gs: 0.3774193548387097\n",
      "gs: 0.399298245614035\n",
      "gs: 0.4783171521035598\n",
      "gs: 0.4873397435897436\n",
      "gs: 0.4961904761904762\n",
      "gs: 0.5133956386292836\n",
      "gs: 0.5299694189602446\n",
      "gs: 0.545945945945946\n",
      "gs: 0.5613569321533923\n",
      "gs: 0.583477011494253\n",
      "gs: 0.5905982905982906\n",
      "gs: 0.5975988700564971\n",
      "gs: 0.6044817927170869\n",
      "gs: 0.5765804597701151\n",
      "gs: 0.4453074433656958\n",
      "gs: 0.42211221122112214\n",
      "gs: 0.37285223367697595\n",
      "gs: 0.46994984040127674\n",
      "gs: 0.4885406774295663\n",
      "gs: 0.4976475279106858\n",
      "gs: 0.4794444444444444\n",
      "gs: 0.42327653740697224\n",
      "gs: 0.36925925925925923\n",
      "gs: 0.3774193548387097\n",
      "gs: 0.3884751773049646\n",
      "gs: 0.4873397435897436\n",
      "gs: 0.5133956386292836\n",
      "gs: 0.538030303030303\n",
      "gs: 0.568859649122807\n",
      "gs: 0.5905982905982906\n",
      "gs: 0.5975988700564971\n",
      "sasgf\n",
      "gs: 0.6289173789173789\n",
      "gs: 0.614766081871345\n",
      "gs: 0.6098820058997051\n",
      "gs: 0.5998498498498499\n",
      "gs: 0.5946969696969696\n",
      "gs: 0.561698717948718\n",
      "gs: 0.5734384662956091\n",
      "gs: 0.5156718014548568\n",
      "gs: 0.5175518925518925\n",
      "gs: 0.4856541323932629\n",
      "gs: 0.43755244755244743\n",
      "gs: 0.43701334816462745\n",
      "gs: 0.46198092031425375\n",
      "gs: 0.46789079957252505\n",
      "gs: 0.47576380728554646\n",
      "gs: 0.48686197523406827\n",
      "gs: 0.5248015873015873\n",
      "gs: 0.5565870910698496\n",
      "gs: 0.558876811594203\n",
      "gs: 0.5643170051772203\n",
      "gs: 0.5764533085961658\n",
      "gs: 0.5888483585200626\n",
      "gs: 0.5896630496858937\n",
      "gs: 0.6011262880421759\n",
      "gs: 0.6125757575757577\n",
      "gs: 0.6014492753623188\n",
      "gs: 0.6182336182336182\n",
      "gs: 0.626412429378531\n",
      "gs: 0.634453781512605\n",
      "gs: 0.6257183908045977\n",
      "gs: 0.630880880880881\n",
      "gs: 0.6243397275507367\n",
      "gs: 0.5986666666666667\n",
      "gs: 0.5826513911620295\n",
      "gs: 0.5760869565217391\n",
      "gs: 0.5493277310924369\n",
      "gs: 0.525369978858351\n",
      "gs: 0.472730686191683\n",
      "gs: 0.4671709375321601\n",
      "gs: 0.4844444444444444\n",
      "gs: 0.47527472527472525\n",
      "gs: 0.5375000000000001\n",
      "gs: 0.5841049382716049\n",
      "gs: 0.5946969696969696\n",
      "gs: 0.6098820058997051\n",
      "gs: 0.614766081871345\n",
      "gs: 0.6195652173913043\n",
      "gs: 0.6242816091954022\n",
      "gs: 0.6379551820728292\n",
      "gs: 0.6379551820728292\n",
      "gs: 0.6242816091954022\n",
      "gs: 0.6049107142857143\n",
      "gs: 0.5786604361370716\n",
      "gs: 0.5112847222222222\n",
      "gs: 0.4086345381526104\n",
      "gs: 0.31644144144144143\n",
      "gs: 0.2805164319248827\n",
      "gs: 0.26785714285714285\n",
      "gs: 0.2955316742081448\n",
      "gs: 0.3536619326093011\n",
      "gs: 0.37302028341205895\n",
      "gs: 0.3983592880978866\n",
      "gs: 0.4369343891402714\n",
      "gs: 0.44642857142857145\n",
      "gs: 0.46296296296296297\n",
      "gs: 0.47551820728291305\n",
      "gs: 0.4703935910832463\n",
      "gs: 0.46260722483991784\n",
      "gs: 0.4690539345711759\n",
      "gs: 0.44049122807017543\n",
      "gs: 0.4860509860509859\n",
      "gs: 0.4975961538461539\n",
      "gs: 0.507936507936508\n",
      "gs: 0.5280373831775702\n",
      "gs: 0.5751488095238095\n",
      "gs: 0.5928362573099415\n",
      "gs: 0.6182336182336182\n",
      "gs: 0.626412429378531\n",
      "gs: 0.634453781512605\n",
      "gs: 0.6049107142857143\n",
      "gs: 0.42647058823529416\n",
      "gs: 0.3802083333333333\n",
      "gs: 0.30479452054794515\n",
      "gs: 0.26785714285714285\n",
      "gs: 0.3321678321678322\n",
      "gs: 0.37302028341205895\n",
      "gs: 0.3983592880978866\n",
      "gs: 0.44642857142857145\n",
      "gs: 0.5026907249129472\n",
      "gs: 0.5073823171790863\n",
      "gs: 0.507936507936508\n",
      "gs: 0.5180817610062892\n",
      "gs: 0.547400611620795\n",
      "gs: 0.5751488095238095\n",
      "gs: 0.6182336182336182\n",
      "gs: 0.634453781512605\n",
      "sasgf\n",
      "gs: 0.603921568627451\n",
      "gs: 0.5803160919540229\n",
      "gs: 0.5638888888888889\n",
      "gs: 0.546875\n",
      "gs: 0.5381381381381383\n",
      "gs: 0.5109567901234567\n",
      "gs: 0.5188466215042558\n",
      "gs: 0.4696180555555557\n",
      "gs: 0.4304724011620564\n",
      "gs: 0.42277395191494516\n",
      "gs: 0.4020063839489284\n",
      "gs: 0.3942545109211776\n",
      "gs: 0.4232057416267942\n",
      "gs: 0.4297629456912465\n",
      "gs: 0.4600113122171946\n",
      "gs: 0.46509539842873177\n",
      "gs: 0.5034948482316904\n",
      "gs: 0.5293791897406004\n",
      "gs: 0.5421650717703349\n",
      "gs: 0.5581728712023962\n",
      "gs: 0.552116402116402\n",
      "gs: 0.5629919888540579\n",
      "gs: 0.5489122807017544\n",
      "gs: 0.557\n",
      "gs: 0.570404984423676\n",
      "gs: 0.5739197530864197\n",
      "gs: 0.5937134502923976\n",
      "gs: 0.5968115942028985\n",
      "gs: 0.6086834733893558\n",
      "gs: 0.5998563218390806\n",
      "gs: 0.6071637426900585\n",
      "gs: 0.6114583333333333\n",
      "gs: 0.6066247858366648\n",
      "gs: 0.5982919254658385\n",
      "gs: 0.5910738714090287\n",
      "gs: 0.5779904306220096\n",
      "gs: 0.5721704070727679\n",
      "gs: 0.5846190476190476\n",
      "gs: 0.5626765083440308\n",
      "gs: 0.5504140786749483\n",
      "gs: 0.5485\n",
      "gs: 0.5683641975308641\n",
      "gs: 0.5894894894894895\n",
      "gs: 0.598132183908046\n",
      "gs: 0.6045584045584045\n",
      "gs: 0.5961864406779661\n",
      "gs: 0.603921568627451\n",
      "gs: 0.603921568627451\n",
      "gs: 0.5961864406779661\n",
      "gs: 0.5292424242424242\n",
      "gs: 0.4305\n",
      "gs: 0.38524305555555555\n",
      "gs: 0.33605072463768115\n",
      "gs: 0.3230769230769231\n",
      "gs: 0.3311111111111112\n",
      "gs: 0.3436079545454544\n",
      "gs: 0.34824103099965165\n",
      "gs: 0.35462184873949576\n",
      "gs: 0.35776636713735555\n",
      "gs: 0.3572649572649572\n",
      "gs: 0.35604166666666665\n",
      "gs: 0.33470426948687826\n",
      "gs: 0.34638095238095246\n",
      "gs: 0.3394430235862461\n",
      "gs: 0.30559440559440565\n",
      "gs: 0.328614429934241\n",
      "gs: 0.35740327380952375\n",
      "gs: 0.41554086423301717\n",
      "gs: 0.466468253968254\n",
      "gs: 0.49870370370370376\n",
      "gs: 0.5085144927536233\n",
      "gs: 0.5355442176870747\n",
      "gs: 0.551797385620915\n",
      "gs: 0.5631746031746032\n",
      "gs: 0.5807575757575758\n",
      "gs: 0.5873511904761906\n",
      "gs: 0.5998563218390806\n",
      "gs: 0.6028490028490029\n",
      "gs: 0.6057909604519774\n",
      "gs: 0.5883190883190883\n",
      "gs: 0.451797385620915\n",
      "gs: 0.37333333333333335\n",
      "gs: 0.3611702127659574\n",
      "gs: 0.3230769230769231\n",
      "gs: 0.3379243687326327\n",
      "gs: 0.34824103099965165\n",
      "gs: 0.35776636713735555\n",
      "gs: 0.35152625152625144\n",
      "gs: 0.3817715617715618\n",
      "gs: 0.40214493569886417\n",
      "gs: 0.393607305936073\n",
      "gs: 0.4084444444444445\n",
      "gs: 0.466468253968254\n",
      "gs: 0.4884469696969697\n",
      "gs: 0.5224561403508772\n",
      "gs: 0.5478547854785479\n",
      "gs: 0.5968115942028985\n",
      "gs: 0.5998563218390806\n",
      "sasgf\n",
      "gs: 0.6070028011204481\n",
      "gs: 0.5977401129943503\n",
      "gs: 0.5590643274853802\n",
      "gs: 0.5876460017969451\n",
      "gs: 0.5617647058823529\n",
      "gs: 0.5527176527176526\n",
      "gs: 0.552291325695581\n",
      "gs: 0.5385499557913351\n",
      "gs: 0.5489109848484848\n",
      "gs: 0.5568627450980392\n",
      "gs: 0.5452991452991454\n",
      "gs: 0.5449760765550239\n",
      "gs: 0.5580588749635674\n",
      "gs: 0.5631071560993697\n",
      "gs: 0.5695297099408617\n",
      "gs: 0.5752380952380953\n",
      "gs: 0.5932216905901116\n",
      "gs: 0.5992020063839489\n",
      "gs: 0.6048148148148149\n",
      "gs: 0.6076796653219781\n",
      "gs: 0.6093750000000001\n",
      "gs: 0.6041666666666667\n",
      "gs: 0.5871345029239766\n",
      "gs: 0.5921739130434783\n",
      "gs: 0.5971264367816093\n",
      "gs: 0.6136752136752136\n",
      "gs: 0.6139130434782609\n",
      "gs: 0.602569960022844\n",
      "gs: 0.5885614035087718\n",
      "gs: 0.6016415868673051\n",
      "gs: 0.5928140270192584\n",
      "gs: 0.5776603633667038\n",
      "gs: 0.581013431013431\n",
      "gs: 0.5599761051373955\n",
      "gs: 0.534640522875817\n",
      "gs: 0.5066618739515936\n",
      "gs: 0.5282282282282282\n",
      "gs: 0.5590643274853802\n",
      "gs: 0.5689855072463769\n",
      "gs: 0.5883190883190883\n",
      "gs: 0.5977401129943503\n",
      "gs: 0.6070028011204481\n",
      "gs: 0.6070028011204481\n",
      "gs: 0.5977401129943503\n",
      "gs: 0.5883190883190883\n",
      "gs: 0.5689855072463769\n",
      "gs: 0.5175757575757576\n",
      "gs: 0.4612698412698413\n",
      "gs: 0.43721682847896437\n",
      "gs: 0.4248366013071896\n",
      "gs: 0.4365815528921314\n",
      "gs: 0.44666666666666666\n",
      "gs: 0.4553150553150553\n",
      "gs: 0.4743055555555556\n",
      "gs: 0.49037037037037035\n",
      "gs: 0.49076976663183564\n",
      "gs: 0.4853793552588733\n",
      "gs: 0.48046850269072494\n",
      "gs: 0.508018648018648\n",
      "gs: 0.5033110119047618\n",
      "gs: 0.46067947646895013\n",
      "gs: 0.44703040636544467\n",
      "gs: 0.4161881977671451\n",
      "gs: 0.45229166666666654\n",
      "gs: 0.4485507246376813\n",
      "gs: 0.4562724014336918\n",
      "gs: 0.4712280701754385\n",
      "gs: 0.4925170068027211\n",
      "gs: 0.525242718446602\n",
      "gs: 0.5492211838006231\n",
      "gs: 0.566060606060606\n",
      "gs: 0.5820058997050147\n",
      "gs: 0.5921739130434783\n",
      "gs: 0.5971264367816093\n",
      "gs: 0.601994301994302\n",
      "gs: 0.5883190883190883\n",
      "gs: 0.4844236760124611\n",
      "gs: 0.47295597484276725\n",
      "gs: 0.43721682847896437\n",
      "gs: 0.4248366013071896\n",
      "gs: 0.44666666666666666\n",
      "gs: 0.46270871985157697\n",
      "gs: 0.4875776397515529\n",
      "gs: 0.48309798887462563\n",
      "gs: 0.456060606060606\n",
      "gs: 0.37581018518518516\n",
      "gs: 0.30865800865800874\n",
      "gs: 0.38015873015873014\n",
      "gs: 0.4562724014336918\n",
      "gs: 0.47847222222222235\n",
      "gs: 0.4925170068027211\n",
      "gs: 0.5189542483660131\n",
      "gs: 0.5433962264150943\n",
      "gs: 0.5921739130434783\n",
      "sasgf\n",
      "gs: 0.6605042016806723\n",
      "gs: 0.6426724137931035\n",
      "gs: 0.6238938053097345\n",
      "gs: 0.5902777777777778\n",
      "gs: 0.583177570093458\n",
      "gs: 0.5746753246753247\n",
      "gs: 0.5192592592592592\n",
      "gs: 0.49151863462208284\n",
      "gs: 0.48148148148148145\n",
      "gs: 0.4713611201563008\n",
      "gs: 0.4281414016671812\n",
      "gs: 0.44947121034077564\n",
      "gs: 0.472\n",
      "gs: 0.4660401002506266\n",
      "gs: 0.4772158933036954\n",
      "gs: 0.522\n",
      "gs: 0.5413277511961722\n",
      "gs: 0.5394744790093629\n",
      "gs: 0.5945141065830721\n",
      "gs: 0.6040413193185937\n",
      "gs: 0.6122859418558344\n",
      "gs: 0.6113747954173486\n",
      "gs: 0.6015\n",
      "gs: 0.5882897603485839\n",
      "gs: 0.5958333333333333\n",
      "gs: 0.6555084745762711\n",
      "gs: 0.6609243697478991\n",
      "gs: 0.6605042016806723\n",
      "gs: 0.6554093567251462\n",
      "gs: 0.6658048373644703\n",
      "gs: 0.657843137254902\n",
      "gs: 0.6484848484848484\n",
      "gs: 0.6459776981282358\n",
      "gs: 0.606074074074074\n",
      "gs: 0.5966435185185186\n",
      "gs: 0.5434285714285714\n",
      "gs: 0.5450344327654948\n",
      "gs: 0.56121067303863\n",
      "gs: 0.5534779050736497\n",
      "gs: 0.574048983845753\n",
      "gs: 0.6220679012345679\n",
      "gs: 0.6395069532237674\n",
      "gs: 0.6453216374269005\n",
      "gs: 0.6487179487179487\n",
      "gs: 0.6546610169491525\n",
      "gs: 0.6605042016806723\n",
      "gs: 0.6546610169491525\n",
      "gs: 0.6487179487179487\n",
      "gs: 0.6426724137931035\n",
      "gs: 0.5902777777777778\n",
      "gs: 0.5376237623762377\n",
      "gs: 0.3959302325581396\n",
      "gs: 0.3614457831325301\n",
      "gs: 0.3370370370370371\n",
      "gs: 0.34875000000000006\n",
      "gs: 0.3595554183389935\n",
      "gs: 0.4016157461809636\n",
      "gs: 0.42300000000000004\n",
      "gs: 0.43678321678321674\n",
      "gs: 0.4619966206702337\n",
      "gs: 0.45511508951406654\n",
      "gs: 0.4346109006120664\n",
      "gs: 0.4317583732057416\n",
      "gs: 0.42619635690027785\n",
      "gs: 0.47691798941798935\n",
      "gs: 0.5239795918367347\n",
      "gs: 0.5318181818181819\n",
      "gs: 0.5470297029702971\n",
      "gs: 0.5544117647058824\n",
      "gs: 0.5892523364485982\n",
      "gs: 0.6443965517241379\n",
      "gs: 0.65\n",
      "gs: 0.6609243697478991\n",
      "gs: 0.6238938053097345\n",
      "gs: 0.47712765957446807\n",
      "gs: 0.4176136363636363\n",
      "gs: 0.3614457831325301\n",
      "gs: 0.3370370370370371\n",
      "gs: 0.34875000000000006\n",
      "gs: 0.3695054945054945\n",
      "gs: 0.43014705882352944\n",
      "gs: 0.4695488721804511\n",
      "gs: 0.5329411764705883\n",
      "gs: 0.5395\n",
      "gs: 0.5616504854368932\n",
      "gs: 0.6022935779816513\n",
      "gs: 0.6086363636363635\n",
      "gs: 0.6209821428571429\n",
      "gs: 0.6386956521739131\n",
      "gs: 0.6443965517241379\n",
      "gs: 0.6609243697478991\n",
      "sasgf\n",
      "gs: 0.6226890756302521\n",
      "gs: 0.61454802259887\n",
      "gs: 0.5978448275862069\n",
      "gs: 0.5892753623188406\n",
      "gs: 0.5626488095238096\n",
      "gs: 0.5438271604938272\n",
      "gs: 0.5161526746620978\n",
      "gs: 0.5049474335188622\n",
      "gs: 0.5206597222222222\n",
      "gs: 0.5105614035087719\n",
      "gs: 0.467165593027662\n",
      "gs: 0.48792717086834725\n",
      "gs: 0.5137958532695375\n",
      "gs: 0.5070476190476191\n",
      "gs: 0.5147247360482654\n",
      "gs: 0.514089927719891\n",
      "gs: 0.5377363737486096\n",
      "gs: 0.5639047619047619\n",
      "gs: 0.5764444444444444\n",
      "gs: 0.5918749999999999\n",
      "gs: 0.6028011204481792\n",
      "gs: 0.6088888888888889\n",
      "gs: 0.6097308488612837\n",
      "gs: 0.6169934640522876\n",
      "gs: 0.6135873472322071\n",
      "gs: 0.5965773809523809\n",
      "gs: 0.5851032448377581\n",
      "gs: 0.6182203389830508\n",
      "gs: 0.6245098039215686\n",
      "gs: 0.6271708683473389\n",
      "gs: 0.625072463768116\n",
      "gs: 0.6266540244416351\n",
      "gs: 0.6200320512820513\n",
      "gs: 0.6137152777777779\n",
      "gs: 0.5974692202462379\n",
      "gs: 0.5818095238095238\n",
      "gs: 0.5659906759906759\n",
      "gs: 0.5732709151313803\n",
      "gs: 0.5236136281261325\n",
      "gs: 0.523063284233497\n",
      "gs: 0.5165223665223665\n",
      "gs: 0.5641624730409777\n",
      "gs: 0.5881818181818181\n",
      "gs: 0.5970760233918128\n",
      "gs: 0.6050724637681159\n",
      "gs: 0.6062678062678063\n",
      "gs: 0.61454802259887\n",
      "gs: 0.6226890756302521\n",
      "gs: 0.6226890756302521\n",
      "gs: 0.61454802259887\n",
      "gs: 0.5892753623188406\n",
      "gs: 0.5149532710280375\n",
      "gs: 0.4625816993464052\n",
      "gs: 0.42861952861952857\n",
      "gs: 0.39253472222222224\n",
      "gs: 0.37999999999999995\n",
      "gs: 0.405416168857029\n",
      "gs: 0.4163819875776398\n",
      "gs: 0.45771508185301296\n",
      "gs: 0.47850862911103875\n",
      "gs: 0.49832549941245596\n",
      "gs: 0.49868842902943755\n",
      "gs: 0.4942551393973529\n",
      "gs: 0.4854079836991757\n",
      "gs: 0.4682284382284382\n",
      "gs: 0.42395833333333344\n",
      "gs: 0.41239316239316237\n",
      "gs: 0.36861360718870345\n",
      "gs: 0.4143866849065208\n",
      "gs: 0.41702508960573487\n",
      "gs: 0.427127659574468\n",
      "gs: 0.44670138888888894\n",
      "gs: 0.47457912457912454\n",
      "gs: 0.5412772585669782\n",
      "gs: 0.5710210210210211\n",
      "gs: 0.605316091954023\n",
      "gs: 0.6182203389830508\n",
      "gs: 0.6245098039215686\n",
      "gs: 0.61454802259887\n",
      "gs: 0.5149532710280375\n",
      "gs: 0.4625816993464052\n",
      "gs: 0.40481099656357383\n",
      "gs: 0.39253472222222224\n",
      "gs: 0.37999999999999995\n",
      "gs: 0.39333060556464816\n",
      "gs: 0.4435665096049294\n",
      "gs: 0.4741402116402116\n",
      "gs: 0.49872685185185184\n",
      "gs: 0.5147611001595793\n",
      "gs: 0.4668660142563731\n",
      "gs: 0.44670138888888894\n",
      "gs: 0.45618556701030927\n",
      "gs: 0.5008169934640523\n",
      "gs: 0.5092233009708739\n",
      "gs: 0.5255555555555556\n",
      "gs: 0.5564220183486239\n",
      "gs: 0.5710210210210211\n",
      "gs: 0.5919590643274854\n",
      "gs: 0.5986956521739131\n",
      "sasgf\n",
      "gs: 0.6271708683473389\n",
      "gs: 0.6047197640117994\n",
      "gs: 0.5966966966966967\n",
      "gs: 0.5883792048929664\n",
      "gs: 0.5797507788161995\n",
      "gs: 0.5258680555555556\n",
      "gs: 0.5267669858641132\n",
      "gs: 0.5095848595848597\n",
      "gs: 0.4355499438832773\n",
      "gs: 0.3974323322209863\n",
      "gs: 0.3870890062054274\n",
      "gs: 0.3865497076023392\n",
      "gs: 0.4057342657342657\n",
      "gs: 0.4308823529411765\n",
      "gs: 0.3971112388617418\n",
      "gs: 0.40486111111111117\n",
      "gs: 0.471827851948334\n",
      "gs: 0.5020370370370371\n",
      "gs: 0.5199322978892871\n",
      "gs: 0.5534013605442176\n",
      "gs: 0.5725299828669331\n",
      "gs: 0.5859164420485176\n",
      "gs: 0.5965248818459827\n",
      "gs: 0.6013013013013013\n",
      "gs: 0.5970760233918128\n",
      "gs: 0.5892753623188406\n",
      "gs: 0.6245098039215685\n",
      "gs: 0.6157971014492754\n",
      "gs: 0.6245535714285714\n",
      "gs: 0.6165151515151515\n",
      "gs: 0.6087601078167115\n",
      "gs: 0.5956427015250545\n",
      "gs: 0.568\n",
      "gs: 0.5168650793650793\n",
      "gs: 0.5165843367294431\n",
      "gs: 0.51796875\n",
      "gs: 0.5250199362041467\n",
      "gs: 0.5099293205654355\n",
      "gs: 0.4909259259259259\n",
      "gs: 0.5468333333333334\n",
      "gs: 0.5883792048929664\n",
      "gs: 0.6007440476190476\n",
      "gs: 0.6047197640117994\n",
      "gs: 0.6271708683473389\n",
      "gs: 0.6271708683473389\n",
      "gs: 0.623587570621469\n",
      "gs: 0.5883792048929664\n",
      "gs: 0.464922480620155\n",
      "gs: 0.3965367965367966\n",
      "gs: 0.33119047619047626\n",
      "gs: 0.2876262626262626\n",
      "gs: 0.25132275132275134\n",
      "gs: 0.2650352243233222\n",
      "gs: 0.29055555555555557\n",
      "gs: 0.3024080763174956\n",
      "gs: 0.31368186874304793\n",
      "gs: 0.36191683093964144\n",
      "gs: 0.3700226244343891\n",
      "gs: 0.39140557631503303\n",
      "gs: 0.46223544973544956\n",
      "gs: 0.4638375350140057\n",
      "gs: 0.46515151515151515\n",
      "gs: 0.4838250517598345\n",
      "gs: 0.4862847222222223\n",
      "gs: 0.5067408781694496\n",
      "gs: 0.504874213836478\n",
      "gs: 0.5149532710280375\n",
      "gs: 0.5248456790123457\n",
      "gs: 0.5534534534534535\n",
      "gs: 0.5626488095238096\n",
      "gs: 0.61454802259887\n",
      "gs: 0.608625730994152\n",
      "gs: 0.4509920634920635\n",
      "gs: 0.3207729468599033\n",
      "gs: 0.2638020833333333\n",
      "gs: 0.25132275132275134\n",
      "gs: 0.31368186874304793\n",
      "gs: 0.3345610119047619\n",
      "gs: 0.3700226244343891\n",
      "gs: 0.39140557631503303\n",
      "gs: 0.4412884635175466\n",
      "gs: 0.4862847222222223\n",
      "gs: 0.504874213836478\n",
      "gs: 0.5345565749235475\n",
      "gs: 0.5716814159292036\n",
      "gs: 0.5805555555555556\n",
      "gs: 0.5892753623188406\n",
      "gs: 0.5978448275862069\n",
      "gs: 0.61454802259887\n",
      "sasgf\n",
      "gs: 0.6294117647058823\n",
      "gs: 0.622457627118644\n",
      "gs: 0.6008695652173913\n",
      "gs: 0.593421052631579\n",
      "gs: 0.5541284403669725\n",
      "gs: 0.6149754500818331\n",
      "gs: 0.5955882352941178\n",
      "gs: 0.5631249999999999\n",
      "gs: 0.555279407224452\n",
      "gs: 0.4899305555555556\n",
      "gs: 0.4700198920147769\n",
      "gs: 0.48016273849607183\n",
      "gs: 0.48796992481203005\n",
      "gs: 0.5011111111111111\n",
      "gs: 0.5165263748597082\n",
      "gs: 0.5101674641148324\n",
      "gs: 0.5510924369747899\n",
      "gs: 0.5448863636363636\n",
      "gs: 0.5995\n",
      "gs: 0.5990196078431371\n",
      "gs: 0.6015873015873016\n",
      "gs: 0.5990294751976994\n",
      "gs: 0.6030864197530865\n",
      "gs: 0.6027027027027027\n",
      "gs: 0.6327731092436976\n",
      "gs: 0.6289915966386554\n",
      "gs: 0.6273913043478261\n",
      "gs: 0.6209821428571428\n",
      "gs: 0.6011676396997498\n",
      "gs: 0.6168758404303004\n",
      "gs: 0.6032670454545455\n",
      "gs: 0.5827393536695863\n",
      "gs: 0.5467633928571428\n",
      "gs: 0.5515004167824396\n",
      "gs: 0.42886989553656224\n",
      "gs: 0.5040545661235316\n",
      "gs: 0.49704861111111115\n",
      "gs: 0.4823713323713323\n",
      "gs: 0.5373831775700936\n",
      "gs: 0.5702702702702703\n",
      "gs: 0.593421052631579\n",
      "gs: 0.6081896551724139\n",
      "gs: 0.6153846153846154\n",
      "gs: 0.622457627118644\n",
      "gs: 0.6294117647058823\n",
      "gs: 0.622457627118644\n",
      "gs: 0.6081896551724139\n",
      "gs: 0.5373831775700936\n",
      "gs: 0.43281250000000004\n",
      "gs: 0.3769230769230769\n",
      "gs: 0.35280898876404493\n",
      "gs: 0.3275862068965517\n",
      "gs: 0.33652530779753775\n",
      "gs: 0.3660968660968661\n",
      "gs: 0.36937499999999995\n",
      "gs: 0.37187403519604817\n",
      "gs: 0.3736780258519389\n",
      "gs: 0.3720489653162342\n",
      "gs: 0.4469866071428571\n",
      "gs: 0.4411188811188811\n",
      "gs: 0.42748521543227264\n",
      "gs: 0.4447570332480818\n",
      "gs: 0.4765185185185185\n",
      "gs: 0.4975579975579976\n",
      "gs: 0.5328125\n",
      "gs: 0.5433673469387755\n",
      "gs: 0.5484848484848485\n",
      "gs: 0.585981308411215\n",
      "gs: 0.5986363636363636\n",
      "gs: 0.6292372881355932\n",
      "gs: 0.6327731092436976\n",
      "gs: 0.622457627118644\n",
      "gs: 0.4535714285714286\n",
      "gs: 0.4221052631578947\n",
      "gs: 0.35280898876404493\n",
      "gs: 0.3403409090909091\n",
      "gs: 0.3275862068965517\n",
      "gs: 0.3660968660968661\n",
      "gs: 0.36937499999999995\n",
      "gs: 0.37466022349743283\n",
      "gs: 0.3746666666666667\n",
      "gs: 0.44760630808223034\n",
      "gs: 0.48357814336075206\n",
      "gs: 0.48579545454545453\n",
      "gs: 0.5433673469387755\n",
      "gs: 0.585981308411215\n",
      "gs: 0.5944954128440366\n",
      "gs: 0.6106194690265486\n",
      "gs: 0.6182608695652174\n",
      "gs: 0.6219827586206896\n",
      "gs: 0.6327731092436976\n",
      "sasgf\n",
      "gs: 0.5791316526610645\n",
      "gs: 0.5611111111111111\n",
      "gs: 0.5231563421828909\n",
      "gs: 0.5173333333333333\n",
      "gs: 0.5379649122807018\n",
      "gs: 0.48621481212999884\n",
      "gs: 0.49951236502960644\n",
      "gs: 0.5068110278953653\n",
      "gs: 0.5191080237591866\n",
      "gs: 0.531089089672593\n",
      "gs: 0.518954248366013\n",
      "gs: 0.5256172839506174\n",
      "gs: 0.528018648018648\n",
      "gs: 0.5527067164894126\n",
      "gs: 0.5602083333333333\n",
      "gs: 0.5601456157011713\n",
      "gs: 0.5698320647577625\n",
      "gs: 0.5718327649362132\n",
      "gs: 0.5720701754385964\n",
      "gs: 0.5736111111111111\n",
      "gs: 0.579727564102564\n",
      "gs: 0.5815001198178769\n",
      "gs: 0.5681159420289855\n",
      "gs: 0.5570402298850575\n",
      "gs: 0.5727401129943502\n",
      "gs: 0.580392156862745\n",
      "gs: 0.5803418803418803\n",
      "gs: 0.5816235752015569\n",
      "gs: 0.5783369278696382\n",
      "gs: 0.5720352564102564\n",
      "gs: 0.5670894708128751\n",
      "gs: 0.5646116335771508\n",
      "gs: 0.5620833333333333\n",
      "gs: 0.5468506523983854\n",
      "gs: 0.5347781791238306\n",
      "gs: 0.5502083333333332\n",
      "gs: 0.5421923314780458\n",
      "gs: 0.5229302832244008\n",
      "gs: 0.5356060606060604\n",
      "gs: 0.5328947368421052\n",
      "gs: 0.551867816091954\n",
      "gs: 0.5611111111111111\n",
      "gs: 0.5791316526610645\n",
      "gs: 0.5791316526610645\n",
      "gs: 0.5701977401129943\n",
      "gs: 0.4928787878787878\n",
      "gs: 0.4385714285714286\n",
      "gs: 0.35323129251700686\n",
      "gs: 0.39207017543859646\n",
      "gs: 0.4211438923395445\n",
      "gs: 0.443155732753413\n",
      "gs: 0.4492424242424242\n",
      "gs: 0.4644257703081234\n",
      "gs: 0.48772777609986917\n",
      "gs: 0.4905185185185185\n",
      "gs: 0.4926659125188538\n",
      "gs: 0.529156010230179\n",
      "gs: 0.5261473603525917\n",
      "gs: 0.49106760804450145\n",
      "gs: 0.4837095303237573\n",
      "gs: 0.43779323513366064\n",
      "gs: 0.4781849103277674\n",
      "gs: 0.4975535168195719\n",
      "gs: 0.5065151515151515\n",
      "gs: 0.5239583333333334\n",
      "gs: 0.532448377581121\n",
      "gs: 0.564957264957265\n",
      "gs: 0.5727401129943502\n",
      "gs: 0.580392156862745\n",
      "gs: 0.5424637681159421\n",
      "gs: 0.4609034267912774\n",
      "gs: 0.4034313725490196\n",
      "gs: 0.35323129251700686\n",
      "gs: 0.4492424242424242\n",
      "gs: 0.45983812129502954\n",
      "gs: 0.485927960927961\n",
      "gs: 0.49282407407407414\n",
      "gs: 0.49668615984405456\n",
      "gs: 0.47314814814814815\n",
      "gs: 0.40883333333333327\n",
      "gs: 0.4791277258566979\n",
      "gs: 0.5065151515151515\n",
      "gs: 0.532448377581121\n",
      "gs: 0.5489855072463768\n",
      "gs: 0.5570402298850575\n",
      "gs: 0.5727401129943502\n",
      "gs: 0.580392156862745\n",
      "sasgf\n",
      "gs: 0.592156862745098\n",
      "gs: 0.5824858757062146\n",
      "gs: 0.5746031746031746\n",
      "gs: 0.5887709991158268\n",
      "gs: 0.5928838951310862\n",
      "gs: 0.5905259491466388\n",
      "gs: 0.5814005602240896\n",
      "gs: 0.5664608418236081\n",
      "gs: 0.563533098315707\n",
      "gs: 0.5699074074074073\n",
      "gs: 0.5562659846547313\n",
      "gs: 0.5613916947250281\n",
      "gs: 0.5600466200466201\n",
      "gs: 0.5770466118292206\n",
      "gs: 0.5831407787762088\n",
      "gs: 0.5812260536398468\n",
      "gs: 0.5938897981451173\n",
      "gs: 0.5944444444444444\n",
      "gs: 0.5956709956709957\n",
      "gs: 0.5958152958152958\n",
      "gs: 0.5967827907862173\n",
      "gs: 0.5935058710759645\n",
      "gs: 0.5708333333333334\n",
      "gs: 0.5943502824858756\n",
      "gs: 0.590960451977401\n",
      "gs: 0.5939682539682539\n",
      "gs: 0.596949891067538\n",
      "gs: 0.5936988936988936\n",
      "gs: 0.5841206133580322\n",
      "gs: 0.5787136889147965\n",
      "gs: 0.5767148683722656\n",
      "gs: 0.5625083836351441\n",
      "gs: 0.5562962962962963\n",
      "gs: 0.5459259259259259\n",
      "gs: 0.5204390847247989\n",
      "gs: 0.5338779956427014\n",
      "gs: 0.5225396825396826\n",
      "gs: 0.556060606060606\n",
      "gs: 0.5648648648648649\n",
      "gs: 0.5692372524230931\n",
      "gs: 0.5855072463768116\n",
      "gs: 0.5726495726495726\n",
      "gs: 0.5824858757062146\n",
      "gs: 0.592156862745098\n",
      "gs: 0.5824858757062146\n",
      "gs: 0.5626436781609194\n",
      "gs: 0.5421052631578948\n",
      "gs: 0.4759259259259259\n",
      "gs: 0.4522012578616352\n",
      "gs: 0.4275641025641026\n",
      "gs: 0.4466230936819172\n",
      "gs: 0.469819214104288\n",
      "gs: 0.4724912280701754\n",
      "gs: 0.47277686852154943\n",
      "gs: 0.47244125846276386\n",
      "gs: 0.45959595959595956\n",
      "gs: 0.4518207282913166\n",
      "gs: 0.44251600998588947\n",
      "gs: 0.42583333333333323\n",
      "gs: 0.5134111327220523\n",
      "gs: 0.4952910641453468\n",
      "gs: 0.48757157608185486\n",
      "gs: 0.48162962962962963\n",
      "gs: 0.5392156862745098\n",
      "gs: 0.51010101010101\n",
      "gs: 0.5153333333333333\n",
      "gs: 0.5204620462046204\n",
      "gs: 0.5254901960784314\n",
      "gs: 0.5446540880503145\n",
      "gs: 0.5492211838006231\n",
      "gs: 0.5749262536873156\n",
      "gs: 0.5789473684210527\n",
      "gs: 0.5905982905982906\n",
      "gs: 0.5943502824858756\n",
      "gs: 0.592156862745098\n",
      "gs: 0.4874617737003058\n",
      "gs: 0.4275641025641026\n",
      "gs: 0.4715277777777779\n",
      "gs: 0.47244125846276386\n",
      "gs: 0.4681481481481481\n",
      "gs: 0.44251600998588947\n",
      "gs: 0.44166666666666665\n",
      "gs: 0.4677839308267755\n",
      "gs: 0.4166666666666666\n",
      "gs: 0.5047619047619047\n",
      "gs: 0.54\n",
      "gs: 0.5492211838006231\n",
      "gs: 0.5666666666666667\n",
      "gs: 0.5708333333333334\n",
      "gs: 0.5749262536873156\n",
      "gs: 0.5867816091954023\n",
      "gs: 0.5905982905982906\n",
      "sasgf\n",
      "gs: 0.588235294117647\n",
      "gs: 0.5800847457627119\n",
      "gs: 0.5633620689655172\n",
      "gs: 0.5547826086956522\n",
      "gs: 0.49999999999999994\n",
      "gs: 0.5615027829313544\n",
      "gs: 0.5516842105263158\n",
      "gs: 0.5146248640811888\n",
      "gs: 0.5002521008403362\n",
      "gs: 0.48057635949202215\n",
      "gs: 0.46111111111111114\n",
      "gs: 0.4569629629629629\n",
      "gs: 0.43699999999999994\n",
      "gs: 0.4409502262443438\n",
      "gs: 0.4368206139115741\n",
      "gs: 0.4627262443438914\n",
      "gs: 0.517505539727762\n",
      "gs: 0.5258873331162487\n",
      "gs: 0.5663157894736841\n",
      "gs: 0.5610389610389611\n",
      "gs: 0.5640692640692639\n",
      "gs: 0.5585\n",
      "gs: 0.5616727462219907\n",
      "gs: 0.5593457943925234\n",
      "gs: 0.5655963302752293\n",
      "gs: 0.5884615384615384\n",
      "gs: 0.588235294117647\n",
      "gs: 0.5859090909090909\n",
      "gs: 0.5924382716049382\n",
      "gs: 0.5882539682539683\n",
      "gs: 0.5791925465838509\n",
      "gs: 0.5533034887310898\n",
      "gs: 0.5529175050301811\n",
      "gs: 0.5194931773879143\n",
      "gs: 0.5125598422979443\n",
      "gs: 0.4575239398084816\n",
      "gs: 0.4850931677018634\n",
      "gs: 0.45013914656771803\n",
      "gs: 0.4702614379084968\n",
      "gs: 0.518918918918919\n",
      "gs: 0.5547826086956522\n",
      "gs: 0.5633620689655172\n",
      "gs: 0.5717948717948718\n",
      "gs: 0.588235294117647\n",
      "gs: 0.5633620689655172\n",
      "gs: 0.5095454545454545\n",
      "gs: 0.42794117647058816\n",
      "gs: 0.3821428571428572\n",
      "gs: 0.33244680851063824\n",
      "gs: 0.31935483870967735\n",
      "gs: 0.3269409937888199\n",
      "gs: 0.3332322849564229\n",
      "gs: 0.3383333333333334\n",
      "gs: 0.34233417905038055\n",
      "gs: 0.34733542319749217\n",
      "gs: 0.3482142857142857\n",
      "gs: 0.34216524216524224\n",
      "gs: 0.32458471760797347\n",
      "gs: 0.31200000000000006\n",
      "gs: 0.3047884841363102\n",
      "gs: 0.39051339285714287\n",
      "gs: 0.3825814536340852\n",
      "gs: 0.38209224784662404\n",
      "gs: 0.3982316903369536\n",
      "gs: 0.4653308073447974\n",
      "gs: 0.49499999999999994\n",
      "gs: 0.4994505494505494\n",
      "gs: 0.503804347826087\n",
      "gs: 0.5122340425531915\n",
      "gs: 0.5495192307692307\n",
      "gs: 0.5561320754716981\n",
      "gs: 0.577433628318584\n",
      "gs: 0.5857758620689656\n",
      "gs: 0.5884615384615384\n",
      "gs: 0.5911016949152541\n",
      "gs: 0.5800847457627119\n",
      "gs: 0.4600000000000001\n",
      "gs: 0.393939393939394\n",
      "gs: 0.35781250000000003\n",
      "gs: 0.31935483870967735\n",
      "gs: 0.3383333333333334\n",
      "gs: 0.3484610123119016\n",
      "gs: 0.3482142857142857\n",
      "gs: 0.32994505494505494\n",
      "gs: 0.39118464961067856\n",
      "gs: 0.503968253968254\n",
      "gs: 0.49499999999999994\n",
      "gs: 0.5318181818181819\n",
      "gs: 0.5528571428571429\n",
      "gs: 0.5593457943925234\n",
      "gs: 0.5655963302752293\n",
      "gs: 0.5686363636363636\n",
      "gs: 0.5936974789915967\n",
      "sasgf\n",
      "gs: 0.5741525423728813\n",
      "gs: 0.5641025641025642\n",
      "gs: 0.5538793103448275\n",
      "gs: 0.5328947368421053\n",
      "gs: 0.5590075062552127\n",
      "gs: 0.546875\n",
      "gs: 0.5245\n",
      "gs: 0.5377633348274318\n",
      "gs: 0.5257865392273995\n",
      "gs: 0.5268642136112016\n",
      "gs: 0.5230009262117937\n",
      "gs: 0.5240740740740741\n",
      "gs: 0.5260416666666667\n",
      "gs: 0.5208571428571429\n",
      "gs: 0.5460972850678733\n",
      "gs: 0.5738358840768479\n",
      "gs: 0.5788352272727273\n",
      "gs: 0.5881891528462573\n",
      "gs: 0.5881261595547309\n",
      "gs: 0.5887445887445887\n",
      "gs: 0.5889423076923076\n",
      "gs: 0.5877777777777777\n",
      "gs: 0.5677272727272727\n",
      "gs: 0.546875\n",
      "gs: 0.5826271186440678\n",
      "gs: 0.546875\n",
      "gs: 0.5819554277498203\n",
      "gs: 0.5920479302832243\n",
      "gs: 0.5881481481481482\n",
      "gs: 0.575091575091575\n",
      "gs: 0.5801182765418191\n",
      "gs: 0.5656108597285068\n",
      "gs: 0.5747607655502391\n",
      "gs: 0.5377339705296695\n",
      "gs: 0.5262707024557396\n",
      "gs: 0.5179726815240835\n",
      "gs: 0.5095454545454545\n",
      "gs: 0.5434782608695652\n",
      "gs: 0.5538793103448275\n",
      "gs: 0.5641025641025642\n",
      "gs: 0.5741525423728813\n",
      "gs: 0.5840336134453782\n",
      "gs: 0.5840336134453782\n",
      "gs: 0.5741525423728813\n",
      "gs: 0.5641025641025642\n",
      "gs: 0.5221238938053097\n",
      "gs: 0.46527777777777773\n",
      "gs: 0.42857142857142866\n",
      "gs: 0.4411057692307693\n",
      "gs: 0.46847316310578424\n",
      "gs: 0.4890183774092336\n",
      "gs: 0.5\n",
      "gs: 0.494047619047619\n",
      "gs: 0.484375\n",
      "gs: 0.47802197802197804\n",
      "gs: 0.44811727507904564\n",
      "gs: 0.46727930938457257\n",
      "gs: 0.4616240266963293\n",
      "gs: 0.41342657342657335\n",
      "gs: 0.4365586814435919\n",
      "gs: 0.41238038277511946\n",
      "gs: 0.49226890756302516\n",
      "gs: 0.4702970297029704\n",
      "gs: 0.4779411764705882\n",
      "gs: 0.48543689320388345\n",
      "gs: 0.5405405405405406\n",
      "gs: 0.576923076923077\n",
      "gs: 0.5826271186440678\n",
      "gs: 0.49999999999999994\n",
      "gs: 0.47706422018348615\n",
      "gs: 0.4410377358490566\n",
      "gs: 0.42857142857142866\n",
      "gs: 0.46078431372549017\n",
      "gs: 0.49820788530465954\n",
      "gs: 0.4984326018808778\n",
      "gs: 0.47802197802197804\n",
      "gs: 0.49150654413812317\n",
      "gs: 0.5397727272727272\n",
      "gs: 0.48543689320388345\n",
      "gs: 0.5140186915887851\n",
      "gs: 0.5592105263157895\n",
      "gs: 0.5711206896551725\n",
      "gs: 0.5826271186440678\n",
      "gs: 0.5882352941176471\n",
      "sasgf\n",
      "gs: 0.6283045977011494\n",
      "gs: 0.621159420289855\n",
      "gs: 0.5989583333333333\n",
      "gs: 0.5912912912912913\n",
      "gs: 0.5755351681957186\n",
      "gs: 0.5926921545201115\n",
      "gs: 0.5662312783002439\n",
      "gs: 0.5267263558711537\n",
      "gs: 0.5276401892680962\n",
      "gs: 0.5189393939393938\n",
      "gs: 0.4895947111238861\n",
      "gs: 0.5223223505115929\n",
      "gs: 0.5538375973303671\n",
      "gs: 0.5498189919242551\n",
      "gs: 0.5644654088050315\n",
      "gs: 0.5662518853695324\n",
      "gs: 0.6051260504201681\n",
      "gs: 0.6241816693944353\n",
      "gs: 0.6272569444444445\n",
      "gs: 0.6279920097272885\n",
      "gs: 0.6358730158730159\n",
      "gs: 0.6386343216531896\n",
      "gs: 0.6442313038643314\n",
      "gs: 0.6430059523809524\n",
      "gs: 0.6339181286549707\n",
      "gs: 0.6240579710144929\n",
      "gs: 0.643361581920904\n",
      "gs: 0.6486956521739131\n",
      "gs: 0.6532738095238095\n",
      "gs: 0.6498484848484848\n",
      "gs: 0.6039143480632843\n",
      "gs: 0.5924190606475147\n",
      "gs: 0.5816140949133276\n",
      "gs: 0.5564045568213393\n",
      "gs: 0.5379419373383156\n",
      "gs: 0.5026610644257703\n",
      "gs: 0.470152835670077\n",
      "gs: 0.4798145117294053\n",
      "gs: 0.48003472222222227\n",
      "gs: 0.5334935897435898\n",
      "gs: 0.5989583333333333\n",
      "gs: 0.6138888888888889\n",
      "gs: 0.6422316384180791\n",
      "gs: 0.6490196078431373\n",
      "gs: 0.5989583333333333\n",
      "gs: 0.5245954692556634\n",
      "gs: 0.44666666666666666\n",
      "gs: 0.40256410256410247\n",
      "gs: 0.37902621722846436\n",
      "gs: 0.3544061302681992\n",
      "gs: 0.3675330597355221\n",
      "gs: 0.3796638655462185\n",
      "gs: 0.40122652773255185\n",
      "gs: 0.41078305519897296\n",
      "gs: 0.4277083333333333\n",
      "gs: 0.4679393762751385\n",
      "gs: 0.48010798522307463\n",
      "gs: 0.48484848484848486\n",
      "gs: 0.48607630186577555\n",
      "gs: 0.5825185185185184\n",
      "gs: 0.583489378838216\n",
      "gs: 0.5822916666666667\n",
      "gs: 0.583664021164021\n",
      "gs: 0.5967379485320768\n",
      "gs: 0.6115683229813665\n",
      "gs: 0.582262996941896\n",
      "gs: 0.5895454545454546\n",
      "gs: 0.6173976608187134\n",
      "gs: 0.6370370370370371\n",
      "gs: 0.643361581920904\n",
      "gs: 0.5507861635220126\n",
      "gs: 0.45711805555555557\n",
      "gs: 0.37902621722846436\n",
      "gs: 0.36685606060606046\n",
      "gs: 0.3544061302681992\n",
      "gs: 0.3796638655462185\n",
      "gs: 0.41078305519897296\n",
      "gs: 0.44200244200244204\n",
      "gs: 0.48484848484848486\n",
      "gs: 0.5165023936919177\n",
      "gs: 0.5464583333333333\n",
      "gs: 0.4737847222222222\n",
      "gs: 0.5895454545454546\n",
      "gs: 0.6370370370370371\n",
      "gs: 0.6495798319327731\n",
      "sasgf\n",
      "gs: 0.4887464387464387\n",
      "gs: 0.47801724137931034\n",
      "gs: 0.4559941520467837\n",
      "gs: 0.40954545454545455\n",
      "gs: 0.48244348244348234\n",
      "gs: 0.45961403508771936\n",
      "gs: 0.451652727996814\n",
      "gs: 0.43128710370089673\n",
      "gs: 0.43106060606060614\n",
      "gs: 0.4366888255777146\n",
      "gs: 0.43786147988062163\n",
      "gs: 0.43850800362428266\n",
      "gs: 0.4441381230854916\n",
      "gs: 0.4764952153110047\n",
      "gs: 0.4864583333333333\n",
      "gs: 0.5039596273291926\n",
      "gs: 0.5055356431700517\n",
      "gs: 0.5053819444444445\n",
      "gs: 0.5012102196324518\n",
      "gs: 0.48893698893698884\n",
      "gs: 0.4933472294597882\n",
      "gs: 0.48513071895424825\n",
      "gs: 0.5130747126436782\n",
      "gs: 0.5165254237288136\n",
      "gs: 0.5039039039039039\n",
      "gs: 0.514720600500417\n",
      "gs: 0.519158878504673\n",
      "gs: 0.5151656314699794\n",
      "gs: 0.5068899866213852\n",
      "gs: 0.4943061840120664\n",
      "gs: 0.48942857142857144\n",
      "gs: 0.48761574074074077\n",
      "gs: 0.46950000000000003\n",
      "gs: 0.46739006282124484\n",
      "gs: 0.475413371675054\n",
      "gs: 0.4720679012345679\n",
      "gs: 0.5056521739130435\n",
      "gs: 0.49929378531073443\n",
      "gs: 0.5096638655462186\n",
      "gs: 0.4887464387464387\n",
      "gs: 0.46710144927536223\n",
      "gs: 0.4559941520467837\n",
      "gs: 0.39740061162079515\n",
      "gs: 0.38503086419753096\n",
      "gs: 0.3724299065420562\n",
      "gs: 0.34650793650793665\n",
      "gs: 0.3331730769230769\n",
      "gs: 0.3441721132897604\n",
      "gs: 0.34705575820739964\n",
      "gs: 0.34576423128641876\n",
      "gs: 0.3391578947368422\n",
      "gs: 0.3294305057745919\n",
      "gs: 0.3020659659296846\n",
      "gs: 0.3931974889369147\n",
      "gs: 0.3860500610500611\n",
      "gs: 0.370115629984051\n",
      "gs: 0.4024144869215292\n",
      "gs: 0.4162457912457913\n",
      "gs: 0.45777529761904767\n",
      "gs: 0.4388235294117647\n",
      "gs: 0.4452107279693486\n",
      "gs: 0.44829545454545444\n",
      "gs: 0.48050000000000004\n",
      "gs: 0.5076696165191741\n",
      "gs: 0.509502923976608\n",
      "gs: 0.5165254237288136\n",
      "gs: 0.46710144927536223\n",
      "gs: 0.39740061162079515\n",
      "gs: 0.34650793650793665\n",
      "gs: 0.3331730769230769\n",
      "gs: 0.3441721132897604\n",
      "gs: 0.3477272727272728\n",
      "gs: 0.3391578947368422\n",
      "gs: 0.3294305057745919\n",
      "gs: 0.3997863247863248\n",
      "gs: 0.4665178571428571\n",
      "gs: 0.4388235294117647\n",
      "gs: 0.46807017543859647\n",
      "gs: 0.48958333333333337\n",
      "gs: 0.4938679245283019\n",
      "gs: 0.501969696969697\n",
      "gs: 0.5058035714285715\n",
      "gs: 0.5076696165191741\n",
      "gs: 0.5165254237288136\n",
      "sasgf\n",
      "gs: 0.5980392156862744\n",
      "gs: 0.5789173789173789\n",
      "gs: 0.5489766081871346\n",
      "gs: 0.5174174174174174\n",
      "gs: 0.5326666666666666\n",
      "gs: 0.547037037037037\n",
      "gs: 0.54109243697479\n",
      "gs: 0.5397210463475524\n",
      "gs: 0.5517094017094016\n",
      "gs: 0.5678240740740741\n",
      "gs: 0.5716550116550116\n",
      "gs: 0.573015873015873\n",
      "gs: 0.5844244758237056\n",
      "gs: 0.5981879729191558\n",
      "gs: 0.6005000000000001\n",
      "gs: 0.5915768032590463\n",
      "gs: 0.6003030303030303\n",
      "gs: 0.6043154761904761\n",
      "gs: 0.6054150863885377\n",
      "gs: 0.6057971014492755\n",
      "gs: 0.603448275862069\n",
      "gs: 0.6067796610169491\n",
      "gs: 0.6021008403361344\n",
      "gs: 0.596011396011396\n",
      "gs: 0.6028301886792452\n",
      "gs: 0.605606320197982\n",
      "gs: 0.6049019607843138\n",
      "gs: 0.5684710351377018\n",
      "gs: 0.5317907444668009\n",
      "gs: 0.571043771043771\n",
      "gs: 0.5335416666666666\n",
      "gs: 0.5341344479275514\n",
      "gs: 0.4876150772972034\n",
      "gs: 0.49606997364006716\n",
      "gs: 0.5281250000000002\n",
      "gs: 0.5386430678466075\n",
      "gs: 0.5489766081871346\n",
      "gs: 0.5591304347826087\n",
      "gs: 0.5789173789173789\n",
      "gs: 0.5885593220338983\n",
      "gs: 0.5980392156862744\n",
      "gs: 0.5980392156862744\n",
      "gs: 0.5885593220338983\n",
      "gs: 0.5386430678466075\n",
      "gs: 0.5065151515151516\n",
      "gs: 0.48410493827160495\n",
      "gs: 0.4242718446601942\n",
      "gs: 0.4473163105784262\n",
      "gs: 0.45649999999999996\n",
      "gs: 0.4770058269834155\n",
      "gs: 0.48610526315789465\n",
      "gs: 0.4946428571428572\n",
      "gs: 0.49747899159663866\n",
      "gs: 0.49110477999366886\n",
      "gs: 0.47997583811537303\n",
      "gs: 0.559203142536476\n",
      "gs: 0.5535067873303167\n",
      "gs: 0.538310185185185\n",
      "gs: 0.5652572233967584\n",
      "gs: 0.5722507488232778\n",
      "gs: 0.595925925925926\n",
      "gs: 0.5504545454545454\n",
      "gs: 0.5566066066066067\n",
      "gs: 0.5967514124293785\n",
      "gs: 0.6021008403361344\n",
      "gs: 0.48410493827160495\n",
      "gs: 0.46084905660377357\n",
      "gs: 0.4242718446601942\n",
      "gs: 0.4711966604823747\n",
      "gs: 0.48956628477905073\n",
      "gs: 0.4987215909090909\n",
      "gs: 0.48318070818070813\n",
      "gs: 0.5248597081930415\n",
      "gs: 0.5528083028083028\n",
      "gs: 0.4256272401433692\n",
      "gs: 0.5378086419753086\n",
      "gs: 0.5857758620689656\n",
      "gs: 0.5913105413105413\n",
      "gs: 0.5967514124293785\n",
      "gs: 0.6021008403361344\n",
      "sasgf\n",
      "gs: 0.5778248587570621\n",
      "gs: 0.5678062678062678\n",
      "gs: 0.5366959064327484\n",
      "gs: 0.5622596153846154\n",
      "gs: 0.5616557734204793\n",
      "gs: 0.5544641306235887\n",
      "gs: 0.5383116883116883\n",
      "gs: 0.5225040916530278\n",
      "gs: 0.5318840579710146\n",
      "gs: 0.5410112359550562\n",
      "gs: 0.5380756953944369\n",
      "gs: 0.5421957671957671\n",
      "gs: 0.5161031769163509\n",
      "gs: 0.5531911429501791\n",
      "gs: 0.5544762595143169\n",
      "gs: 0.5831597222222222\n",
      "gs: 0.583704390847248\n",
      "gs: 0.5812107367218732\n",
      "gs: 0.5770032051282051\n",
      "gs: 0.5717460317460318\n",
      "gs: 0.5628086419753087\n",
      "gs: 0.5410606060606061\n",
      "gs: 0.592156862745098\n",
      "gs: 0.5915178571428572\n",
      "gs: 0.5948484848484847\n",
      "gs: 0.5944444444444446\n",
      "gs: 0.5841666666666666\n",
      "gs: 0.5681481481481481\n",
      "gs: 0.5595661846496107\n",
      "gs: 0.5605011018491903\n",
      "gs: 0.5586666666666666\n",
      "gs: 0.5272569444444445\n",
      "gs: 0.5317538126361656\n",
      "gs: 0.5384735202492212\n",
      "gs: 0.5407978871281625\n",
      "gs: 0.5576149425287357\n",
      "gs: 0.5678062678062678\n",
      "gs: 0.5778248587570621\n",
      "gs: 0.5876750700280112\n",
      "gs: 0.5576149425287357\n",
      "gs: 0.5150297619047619\n",
      "gs: 0.4925757575757576\n",
      "gs: 0.46929012345679005\n",
      "gs: 0.43269841269841264\n",
      "gs: 0.46405228758169936\n",
      "gs: 0.4714434601354872\n",
      "gs: 0.4776666666666667\n",
      "gs: 0.49357638888888894\n",
      "gs: 0.49743589743589745\n",
      "gs: 0.4985663082437276\n",
      "gs: 0.49944423392699244\n",
      "gs: 0.5388955342902711\n",
      "gs: 0.5288349142473892\n",
      "gs: 0.5230036942313157\n",
      "gs: 0.5487329434697856\n",
      "gs: 0.5652157738095238\n",
      "gs: 0.5781187837453823\n",
      "gs: 0.47350000000000003\n",
      "gs: 0.48085808580858086\n",
      "gs: 0.5285493827160493\n",
      "gs: 0.586864406779661\n",
      "gs: 0.592156862745098\n",
      "gs: 0.5876750700280112\n",
      "gs: 0.4925757575757576\n",
      "gs: 0.45732087227414336\n",
      "gs: 0.445125786163522\n",
      "gs: 0.43269841269841264\n",
      "gs: 0.4714434601354872\n",
      "gs: 0.4776666666666667\n",
      "gs: 0.49357638888888894\n",
      "gs: 0.49922360248447206\n",
      "gs: 0.536875\n",
      "gs: 0.562873946466611\n",
      "gs: 0.4505154639175258\n",
      "gs: 0.5020833333333334\n",
      "gs: 0.5589970501474926\n",
      "gs: 0.5704347826086956\n",
      "gs: 0.5760057471264368\n",
      "gs: 0.5814814814814815\n",
      "gs: 0.592156862745098\n",
      "sasgf\n",
      "gs: 0.5841736694677871\n",
      "gs: 0.5639601139601139\n",
      "gs: 0.5535919540229886\n",
      "gs: 0.562\n",
      "gs: 0.5589225589225589\n",
      "gs: 0.5611310324219334\n",
      "gs: 0.5564236111111112\n",
      "gs: 0.528888888888889\n",
      "gs: 0.5392518939393938\n",
      "gs: 0.5507936507936507\n",
      "gs: 0.5542245549037769\n",
      "gs: 0.5622547639162678\n",
      "gs: 0.5768749999999999\n",
      "gs: 0.586032741205155\n",
      "gs: 0.5866003787878787\n",
      "gs: 0.5878136200716846\n",
      "gs: 0.588561403508772\n",
      "gs: 0.5871666666666667\n",
      "gs: 0.5796825396825397\n",
      "gs: 0.5580303030303031\n",
      "gs: 0.5462462462462464\n",
      "gs: 0.5519345238095239\n",
      "gs: 0.5840395480225988\n",
      "gs: 0.5890756302521009\n",
      "gs: 0.5788011695906432\n",
      "gs: 0.5919377258826799\n",
      "gs: 0.5648745519713262\n",
      "gs: 0.5228011204481793\n",
      "gs: 0.5718670796958603\n",
      "gs: 0.5645637965331058\n",
      "gs: 0.5594074074074075\n",
      "gs: 0.5423136645962734\n",
      "gs: 0.5333072780962307\n",
      "gs: 0.5122222222222224\n",
      "gs: 0.5053459119496855\n",
      "gs: 0.5202202202202203\n",
      "gs: 0.5417193426042982\n",
      "gs: 0.5620289855072463\n",
      "gs: 0.5715517241379311\n",
      "gs: 0.5741525423728813\n",
      "gs: 0.5841736694677871\n",
      "gs: 0.5741525423728813\n",
      "gs: 0.5323099415204678\n",
      "gs: 0.4756880733944954\n",
      "gs: 0.4391509433962264\n",
      "gs: 0.4511111111111111\n",
      "gs: 0.47619825708061\n",
      "gs: 0.48186555497655026\n",
      "gs: 0.4902356902356902\n",
      "gs: 0.4954280591662931\n",
      "gs: 0.49704861111111115\n",
      "gs: 0.49864975450081833\n",
      "gs: 0.4983695652173913\n",
      "gs: 0.557608695652174\n",
      "gs: 0.5558493820063237\n",
      "gs: 0.5324534592942484\n",
      "gs: 0.5218377976190476\n",
      "gs: 0.5270833333333333\n",
      "gs: 0.5535890466123026\n",
      "gs: 0.461734693877551\n",
      "gs: 0.46902356902356895\n",
      "gs: 0.5224299065420561\n",
      "gs: 0.5285493827160493\n",
      "gs: 0.5737068965517241\n",
      "gs: 0.5840395480225988\n",
      "gs: 0.5890756302521009\n",
      "gs: 0.5741525423728813\n",
      "gs: 0.5323099415204678\n",
      "gs: 0.4874242424242425\n",
      "gs: 0.4391509433962264\n",
      "gs: 0.47619825708061\n",
      "gs: 0.4954280591662931\n",
      "gs: 0.4981052631578946\n",
      "gs: 0.4964814814814815\n",
      "gs: 0.5256290682988438\n",
      "gs: 0.5352752639517346\n",
      "gs: 0.4229390681003585\n",
      "gs: 0.5098412698412699\n",
      "gs: 0.5404545454545454\n",
      "gs: 0.5630116959064327\n",
      "gs: 0.5684057971014492\n",
      "gs: 0.5789173789173789\n",
      "sasgf\n",
      "gs: 0.5396011396011395\n",
      "gs: 0.5294540229885057\n",
      "gs: 0.5191304347826088\n",
      "gs: 0.4759759759759761\n",
      "gs: 0.5209183673469387\n",
      "gs: 0.5135215897206036\n",
      "gs: 0.4970526315789473\n",
      "gs: 0.5025095471903982\n",
      "gs: 0.4980072463768117\n",
      "gs: 0.5042592592592592\n",
      "gs: 0.5117086834733894\n",
      "gs: 0.49358151476251605\n",
      "gs: 0.5033333333333332\n",
      "gs: 0.5313415825464017\n",
      "gs: 0.5370588235294117\n",
      "gs: 0.5522776868521548\n",
      "gs: 0.5530180785895712\n",
      "gs: 0.5586666666666666\n",
      "gs: 0.5556018759770713\n",
      "gs: 0.5536360175138015\n",
      "gs: 0.5466930265995686\n",
      "gs: 0.5378086419753086\n",
      "gs: 0.5666666666666667\n",
      "gs: 0.5378086419753086\n",
      "gs: 0.5519317160826595\n",
      "gs: 0.5194035087719298\n",
      "gs: 0.4870074894171279\n",
      "gs: 0.5491375291375291\n",
      "gs: 0.5301470588235294\n",
      "gs: 0.5358518518518518\n",
      "gs: 0.49949999999999994\n",
      "gs: 0.508573717948718\n",
      "gs: 0.4989393939393939\n",
      "gs: 0.5052083333333333\n",
      "gs: 0.5294540229885057\n",
      "gs: 0.5396011396011395\n",
      "gs: 0.5495762711864406\n",
      "gs: 0.5593837535014006\n",
      "gs: 0.5495762711864406\n",
      "gs: 0.5191304347826088\n",
      "gs: 0.4759759759759761\n",
      "gs: 0.42959501557632407\n",
      "gs: 0.40507936507936504\n",
      "gs: 0.3924679487179487\n",
      "gs: 0.3796116504854369\n",
      "gs: 0.38779956427015244\n",
      "gs: 0.39418099704707305\n",
      "gs: 0.39900000000000013\n",
      "gs: 0.4060763888888889\n",
      "gs: 0.4054035087719298\n",
      "gs: 0.40171246515332526\n",
      "gs: 0.48094576515385407\n",
      "gs: 0.4776251526251525\n",
      "gs: 0.4535946759933936\n",
      "gs: 0.46638249502699625\n",
      "gs: 0.4866444382698898\n",
      "gs: 0.5317715818721855\n",
      "gs: 0.47537037037037033\n",
      "gs: 0.48745519713261654\n",
      "gs: 0.5319182389937107\n",
      "gs: 0.5378086419753086\n",
      "gs: 0.5666666666666667\n",
      "gs: 0.5495762711864406\n",
      "gs: 0.4646969696969696\n",
      "gs: 0.4415123456790124\n",
      "gs: 0.3924679487179487\n",
      "gs: 0.3796116504854369\n",
      "gs: 0.39418099704707305\n",
      "gs: 0.4039279869067104\n",
      "gs: 0.4694776714513556\n",
      "gs: 0.5296576879910213\n",
      "gs: 0.4794871794871795\n",
      "gs: 0.5194444444444445\n",
      "gs: 0.5489583333333334\n",
      "gs: 0.5542397660818712\n",
      "gs: 0.5593390804597702\n",
      "gs: 0.5618233618233618\n",
      "sasgf\n",
      "gs: 0.5859943977591037\n",
      "gs: 0.5660968660968662\n",
      "gs: 0.5349415204678362\n",
      "gs: 0.5363492063492062\n",
      "gs: 0.5206270627062708\n",
      "gs: 0.5286666666666667\n",
      "gs: 0.5195526695526695\n",
      "gs: 0.5237847222222223\n",
      "gs: 0.5521306818181818\n",
      "gs: 0.5555899272766742\n",
      "gs: 0.5412698412698413\n",
      "gs: 0.5395534290271132\n",
      "gs: 0.48486048265460036\n",
      "gs: 0.5186202686202686\n",
      "gs: 0.5131054131054131\n",
      "gs: 0.5891156462585034\n",
      "gs: 0.5851323053493241\n",
      "gs: 0.5822115384615385\n",
      "gs: 0.5725308641975309\n",
      "gs: 0.5501488095238095\n",
      "gs: 0.5848870056497175\n",
      "gs: 0.5903361344537815\n",
      "gs: 0.5926086956521739\n",
      "gs: 0.5956845238095237\n",
      "gs: 0.5951451451451452\n",
      "gs: 0.5915\n",
      "gs: 0.5813881841246828\n",
      "gs: 0.5779342723004695\n",
      "gs: 0.5643000862316758\n",
      "gs: 0.5537869062901155\n",
      "gs: 0.528\n",
      "gs: 0.5199976036424635\n",
      "gs: 0.5334821428571428\n",
      "gs: 0.5241887905604719\n",
      "gs: 0.5455072463768115\n",
      "gs: 0.5558908045977012\n",
      "gs: 0.5660968660968662\n",
      "gs: 0.5761299435028249\n",
      "gs: 0.5859943977591037\n",
      "gs: 0.5859943977591037\n",
      "gs: 0.5558908045977012\n",
      "gs: 0.5241887905604719\n",
      "gs: 0.4792048929663609\n",
      "gs: 0.4674382716049383\n",
      "gs: 0.4432389937106918\n",
      "gs: 0.43079365079365073\n",
      "gs: 0.4431891025641026\n",
      "gs: 0.45368360936607655\n",
      "gs: 0.46258169934640514\n",
      "gs: 0.47012332812228586\n",
      "gs: 0.4764999999999999\n",
      "gs: 0.490049305244285\n",
      "gs: 0.4994047619047619\n",
      "gs: 0.4998148148148148\n",
      "gs: 0.49629502963976296\n",
      "gs: 0.5118181818181817\n",
      "gs: 0.5034670008354218\n",
      "gs: 0.4940307492822079\n",
      "gs: 0.4862433862433863\n",
      "gs: 0.48486048265460036\n",
      "gs: 0.5079955926032386\n",
      "gs: 0.48316993464052277\n",
      "gs: 0.49045307443365693\n",
      "gs: 0.5046031746031746\n",
      "gs: 0.5439939939939941\n",
      "gs: 0.5793447293447294\n",
      "gs: 0.5848870056497175\n",
      "gs: 0.5903361344537815\n",
      "gs: 0.5660968660968662\n",
      "gs: 0.5021021021021022\n",
      "gs: 0.4674382716049383\n",
      "gs: 0.43079365079365073\n",
      "gs: 0.4431891025641026\n",
      "gs: 0.47012332812228586\n",
      "gs: 0.495438596491228\n",
      "gs: 0.49629502963976296\n",
      "gs: 0.5311111111111111\n",
      "gs: 0.5684103551561831\n",
      "gs: 0.4975961538461539\n",
      "gs: 0.5439939939939941\n",
      "gs: 0.5737068965517241\n",
      "gs: 0.5848870056497175\n",
      "gs: 0.5903361344537815\n",
      "sasgf\n",
      "gs: 0.6140056022408964\n",
      "gs: 0.596011396011396\n",
      "gs: 0.5911111111111111\n",
      "gs: 0.5793028322440087\n",
      "gs: 0.5676666666666665\n",
      "gs: 0.5458947368421052\n",
      "gs: 0.5267146646456992\n",
      "gs: 0.5268353853169174\n",
      "gs: 0.5406290115532734\n",
      "gs: 0.5442002442002442\n",
      "gs: 0.5527407407407408\n",
      "gs: 0.5555816686251469\n",
      "gs: 0.5917843388960204\n",
      "gs: 0.5980848861283644\n",
      "gs: 0.60147351652728\n",
      "gs: 0.6114597340505006\n",
      "gs: 0.6116666666666667\n",
      "gs: 0.6101962827861734\n",
      "gs: 0.6086538461538462\n",
      "gs: 0.595470884255931\n",
      "gs: 0.5848484848484848\n",
      "gs: 0.5732732732732733\n",
      "gs: 0.6176470588235294\n",
      "gs: 0.5903863432165319\n",
      "gs: 0.6013621794871795\n",
      "gs: 0.6080610021786492\n",
      "gs: 0.5725925925925925\n",
      "gs: 0.5483525887890457\n",
      "gs: 0.5837968112717835\n",
      "gs: 0.5558826386864859\n",
      "gs: 0.5541422249665535\n",
      "gs: 0.5123263888888889\n",
      "gs: 0.4956025128498001\n",
      "gs: 0.4849056603773585\n",
      "gs: 0.4959501557632399\n",
      "gs: 0.5381381381381382\n",
      "gs: 0.5581120943952802\n",
      "gs: 0.5678362573099416\n",
      "gs: 0.577391304347826\n",
      "gs: 0.6050847457627118\n",
      "gs: 0.6140056022408964\n",
      "gs: 0.5482142857142858\n",
      "gs: 0.47365079365079377\n",
      "gs: 0.45048543689320386\n",
      "gs: 0.4139999999999999\n",
      "gs: 0.4263107263107263\n",
      "gs: 0.4371057513914656\n",
      "gs: 0.46210526315789463\n",
      "gs: 0.4926158133054685\n",
      "gs: 0.4949735449735449\n",
      "gs: 0.4944801026957637\n",
      "gs: 0.49249999999999994\n",
      "gs: 0.5515911010982821\n",
      "gs: 0.5445558340295182\n",
      "gs: 0.5392331203111976\n",
      "gs: 0.5424479166666667\n",
      "gs: 0.5477752639517346\n",
      "gs: 0.5960790367397344\n",
      "gs: 0.5220064724919093\n",
      "gs: 0.5288461538461537\n",
      "gs: 0.5672727272727273\n",
      "gs: 0.6124293785310735\n",
      "gs: 0.6176470588235294\n",
      "gs: 0.5867816091954023\n",
      "gs: 0.4959501557632399\n",
      "gs: 0.47365079365079377\n",
      "gs: 0.4385620915032679\n",
      "gs: 0.4264026402640263\n",
      "gs: 0.4139999999999999\n",
      "gs: 0.4548611111111111\n",
      "gs: 0.47387495021903625\n",
      "gs: 0.4886915549111997\n",
      "gs: 0.4949202214262456\n",
      "gs: 0.516746456397259\n",
      "gs: 0.5829629629629629\n",
      "gs: 0.5079207920792078\n",
      "gs: 0.5672727272727273\n",
      "gs: 0.5906432748538012\n",
      "gs: 0.6017241379310344\n",
      "gs: 0.6124293785310735\n",
      "gs: 0.6176470588235294\n",
      "sasgf\n",
      "gs: 0.5683473389355742\n",
      "gs: 0.5577683615819208\n",
      "gs: 0.5470085470085471\n",
      "gs: 0.5360632183908045\n",
      "gs: 0.5135964912280702\n",
      "gs: 0.5436256889527918\n",
      "gs: 0.5475681778704187\n",
      "gs: 0.5425685425685425\n",
      "gs: 0.5473958333333334\n",
      "gs: 0.5411987256073278\n",
      "gs: 0.5371212121212121\n",
      "gs: 0.5472916666666666\n",
      "gs: 0.535805422647528\n",
      "gs: 0.5402222222222222\n",
      "gs: 0.5378476302389346\n",
      "gs: 0.48418403302124247\n",
      "gs: 0.5261243386243386\n",
      "gs: 0.519859943977591\n",
      "gs: 0.5661458333333333\n",
      "gs: 0.5687957567607949\n",
      "gs: 0.5730880230880231\n",
      "gs: 0.5711666666666667\n",
      "gs: 0.5645032051282052\n",
      "gs: 0.5596825396825397\n",
      "gs: 0.5298484848484848\n",
      "gs: 0.5742296918767508\n",
      "gs: 0.5746498599439775\n",
      "gs: 0.5702160493827161\n",
      "gs: 0.5783224400871461\n",
      "gs: 0.5766666666666667\n",
      "gs: 0.576751893939394\n",
      "gs: 0.5701005562687206\n",
      "gs: 0.5402534113060429\n",
      "gs: 0.5597916666666667\n",
      "gs: 0.556910569105691\n",
      "gs: 0.5296161195066875\n",
      "gs: 0.5058641975308642\n",
      "gs: 0.5126488095238095\n",
      "gs: 0.5470085470085471\n",
      "gs: 0.5577683615819208\n",
      "gs: 0.5683473389355742\n",
      "gs: 0.5683473389355742\n",
      "gs: 0.5577683615819208\n",
      "gs: 0.5360632183908045\n",
      "gs: 0.5135964912280702\n",
      "gs: 0.490327380952381\n",
      "gs: 0.4783783783783785\n",
      "gs: 0.4662121212121213\n",
      "gs: 0.4538226299694189\n",
      "gs: 0.4651234567901234\n",
      "gs: 0.4904647435897436\n",
      "gs: 0.49350847134970494\n",
      "gs: 0.4956427015250544\n",
      "gs: 0.4969949626541602\n",
      "gs: 0.49496527777777777\n",
      "gs: 0.4792592592592592\n",
      "gs: 0.4629730962152303\n",
      "gs: 0.4582633053221289\n",
      "gs: 0.48834704269486884\n",
      "gs: 0.47227172559164515\n",
      "gs: 0.4748366013071895\n",
      "gs: 0.49638325460776134\n",
      "gs: 0.5404802500710428\n",
      "gs: 0.46464646464646464\n",
      "gs: 0.5077044025157234\n",
      "gs: 0.5133956386292836\n",
      "gs: 0.5696327683615818\n",
      "gs: 0.5742296918767508\n",
      "gs: 0.5683473389355742\n",
      "gs: 0.5135964912280702\n",
      "gs: 0.4783783783783785\n",
      "gs: 0.4538226299694189\n",
      "gs: 0.4956427015250544\n",
      "gs: 0.4972789115646258\n",
      "gs: 0.49633945913641125\n",
      "gs: 0.47558294067899\n",
      "gs: 0.5061660561660561\n",
      "gs: 0.5550253904378653\n",
      "gs: 0.46464646464646464\n",
      "gs: 0.501904761904762\n",
      "gs: 0.5298484848484848\n",
      "gs: 0.5351351351351352\n",
      "gs: 0.5553623188405797\n",
      "gs: 0.5696327683615818\n",
      "sasgf\n",
      "gs: 0.5621848739495798\n",
      "gs: 0.551412429378531\n",
      "gs: 0.5293103448275862\n",
      "gs: 0.5179710144927536\n",
      "gs: 0.5346780218138354\n",
      "gs: 0.5386128364389234\n",
      "gs: 0.5481937900205388\n",
      "gs: 0.5419981060606059\n",
      "gs: 0.5460784313725491\n",
      "gs: 0.5489851297080213\n",
      "gs: 0.5491895701198029\n",
      "gs: 0.5480063795853269\n",
      "gs: 0.5324761904761905\n",
      "gs: 0.5534427724578204\n",
      "gs: 0.5525925925925925\n",
      "gs: 0.5706666666666667\n",
      "gs: 0.568888888888889\n",
      "gs: 0.568727534148095\n",
      "gs: 0.5684181262162913\n",
      "gs: 0.5651515151515152\n",
      "gs: 0.5421828908554572\n",
      "gs: 0.5686274509803921\n",
      "gs: 0.5683473389355742\n",
      "gs: 0.5620517097581318\n",
      "gs: 0.5717460317460318\n",
      "gs: 0.5721132897603486\n",
      "gs: 0.5616806722689076\n",
      "gs: 0.534962962962963\n",
      "gs: 0.5656849124756878\n",
      "gs: 0.5568376068376069\n",
      "gs: 0.5573319400905608\n",
      "gs: 0.5450980392156863\n",
      "gs: 0.5590909090909091\n",
      "gs: 0.5522321428571428\n",
      "gs: 0.545906432748538\n",
      "gs: 0.5404558404558404\n",
      "gs: 0.551412429378531\n",
      "gs: 0.5621848739495798\n",
      "gs: 0.5404558404558404\n",
      "gs: 0.4946902654867257\n",
      "gs: 0.47057057057057067\n",
      "gs: 0.4581818181818182\n",
      "gs: 0.48332135154565065\n",
      "gs: 0.4949891067538126\n",
      "gs: 0.4947889525794685\n",
      "gs: 0.49268879268879273\n",
      "gs: 0.4831578947368421\n",
      "gs: 0.47626443647949024\n",
      "gs: 0.46816976127320953\n",
      "gs: 0.4637037037037037\n",
      "gs: 0.5145780531305737\n",
      "gs: 0.5099326599326599\n",
      "gs: 0.4883018168335187\n",
      "gs: 0.4953598221728258\n",
      "gs: 0.4956631934666291\n",
      "gs: 0.5364444444444444\n",
      "gs: 0.4680134680134681\n",
      "gs: 0.474\n",
      "gs: 0.4798679867986798\n",
      "gs: 0.48562091503267973\n",
      "gs: 0.5327327327327327\n",
      "gs: 0.5375000000000001\n",
      "gs: 0.5644067796610169\n",
      "gs: 0.5686274509803921\n",
      "gs: 0.5621848739495798\n",
      "gs: 0.48273809523809524\n",
      "gs: 0.4581818181818182\n",
      "gs: 0.4932692307692308\n",
      "gs: 0.4947889525794685\n",
      "gs: 0.490909090909091\n",
      "gs: 0.4539772727272727\n",
      "gs: 0.5168227121715494\n",
      "gs: 0.5405797101449276\n",
      "gs: 0.44930555555555557\n",
      "gs: 0.5229357798165137\n",
      "gs: 0.551304347826087\n",
      "gs: 0.5601139601139601\n",
      "gs: 0.5644067796610169\n",
      "sasgf\n",
      "gs: 0.5764705882352942\n",
      "gs: 0.5665254237288135\n",
      "gs: 0.5461206896551724\n",
      "gs: 0.5356521739130435\n",
      "gs: 0.56302113078241\n",
      "gs: 0.5567717996289424\n",
      "gs: 0.5465263157894736\n",
      "gs: 0.5305854241338113\n",
      "gs: 0.5218167701863354\n",
      "gs: 0.5250663129973474\n",
      "gs: 0.518593693367162\n",
      "gs: 0.5268479322696189\n",
      "gs: 0.5262836970474968\n",
      "gs: 0.5252453308008863\n",
      "gs: 0.5512276785714285\n",
      "gs: 0.5735879332477535\n",
      "gs: 0.57\n",
      "gs: 0.5819172113289761\n",
      "gs: 0.5768253968253968\n",
      "gs: 0.5753774263120058\n",
      "gs: 0.5702160493827161\n",
      "gs: 0.5531818181818181\n",
      "gs: 0.5831932773109244\n",
      "gs: 0.5741525423728813\n",
      "gs: 0.5808561808561808\n",
      "gs: 0.5838861497086509\n",
      "gs: 0.5833333333333333\n",
      "gs: 0.5747058823529412\n",
      "gs: 0.5367142857142858\n",
      "gs: 0.5630604288499026\n",
      "gs: 0.5337912936091386\n",
      "gs: 0.5384402474763921\n",
      "gs: 0.46548821548821545\n",
      "gs: 0.47535174570088584\n",
      "gs: 0.5070892410341951\n",
      "gs: 0.4918918918918919\n",
      "gs: 0.5141592920353983\n",
      "gs: 0.5564102564102563\n",
      "gs: 0.5665254237288135\n",
      "gs: 0.5764705882352942\n",
      "gs: 0.5764705882352942\n",
      "gs: 0.5249999999999999\n",
      "gs: 0.4918918918918919\n",
      "gs: 0.4569444444444444\n",
      "gs: 0.4325471698113208\n",
      "gs: 0.42\n",
      "gs: 0.4295673076923077\n",
      "gs: 0.4371787549971445\n",
      "gs: 0.4476810838978635\n",
      "gs: 0.45324675324675323\n",
      "gs: 0.4536842105263158\n",
      "gs: 0.4520458265139116\n",
      "gs: 0.44982078853046603\n",
      "gs: 0.4848379629629629\n",
      "gs: 0.47904570278815745\n",
      "gs: 0.43580419580419577\n",
      "gs: 0.45777777777777773\n",
      "gs: 0.47636363636363627\n",
      "gs: 0.540873015873016\n",
      "gs: 0.49531250000000004\n",
      "gs: 0.5\n",
      "gs: 0.5531818181818181\n",
      "gs: 0.5831932773109244\n",
      "gs: 0.5764705882352942\n",
      "gs: 0.48045454545454536\n",
      "gs: 0.4448598130841122\n",
      "gs: 0.4325471698113208\n",
      "gs: 0.42\n",
      "gs: 0.4476810838978635\n",
      "gs: 0.45099999999999996\n",
      "gs: 0.4545454545454545\n",
      "gs: 0.44376657824933685\n",
      "gs: 0.5203584960652872\n",
      "gs: 0.5591849336214881\n",
      "gs: 0.5135000000000001\n",
      "gs: 0.5531818181818181\n",
      "gs: 0.5769230769230771\n",
      "gs: 0.5800847457627119\n",
      "gs: 0.5831932773109244\n",
      "sasgf\n",
      "gs: 0.5560224089635855\n",
      "gs: 0.5453389830508475\n",
      "gs: 0.5344729344729343\n",
      "gs: 0.5007309941520467\n",
      "gs: 0.5182539682539682\n",
      "gs: 0.5122549019607843\n",
      "gs: 0.5113333333333334\n",
      "gs: 0.5028138528138528\n",
      "gs: 0.5147691618108472\n",
      "gs: 0.5238245614035087\n",
      "gs: 0.5359006886553098\n",
      "gs: 0.5355182072829132\n",
      "gs: 0.5391741548994438\n",
      "gs: 0.5298571428571428\n",
      "gs: 0.5375160462130938\n",
      "gs: 0.526506443747823\n",
      "gs: 0.5507111935683364\n",
      "gs: 0.5471861471861472\n",
      "gs: 0.5474465867639395\n",
      "gs: 0.5376261184085284\n",
      "gs: 0.5312500000000001\n",
      "gs: 0.4905864197530864\n",
      "gs: 0.4975535168195718\n",
      "gs: 0.5607843137254902\n",
      "gs: 0.5634453781512605\n",
      "gs: 0.5629372102823431\n",
      "gs: 0.5635416666666667\n",
      "gs: 0.5654654654654655\n",
      "gs: 0.5530062530062531\n",
      "gs: 0.530072463768116\n",
      "gs: 0.5585819835180448\n",
      "gs: 0.5533630353549871\n",
      "gs: 0.5424603174603174\n",
      "gs: 0.5309537407195888\n",
      "gs: 0.5246031746031746\n",
      "gs: 0.5387345679012345\n",
      "gs: 0.5233733733733734\n",
      "gs: 0.5243994943109987\n",
      "gs: 0.5340643274853801\n",
      "gs: 0.5344729344729343\n",
      "gs: 0.5453389830508475\n",
      "gs: 0.5560224089635855\n",
      "gs: 0.5453389830508475\n",
      "gs: 0.5234195402298851\n",
      "gs: 0.5007309941520467\n",
      "gs: 0.48908554572271384\n",
      "gs: 0.4772321428571428\n",
      "gs: 0.46516516516516515\n",
      "gs: 0.44036697247706413\n",
      "gs: 0.4527777777777777\n",
      "gs: 0.4627845674574647\n",
      "gs: 0.47088948787062\n",
      "gs: 0.48277243589743596\n",
      "gs: 0.4949999999999999\n",
      "gs: 0.49740350877192985\n",
      "gs: 0.4944616977225673\n",
      "gs: 0.48906608674640567\n",
      "gs: 0.5191146881287727\n",
      "gs: 0.513441318556408\n",
      "gs: 0.48687430478309235\n",
      "gs: 0.496969696969697\n",
      "gs: 0.5157857345836885\n",
      "gs: 0.5358544641989701\n",
      "gs: 0.437953795379538\n",
      "gs: 0.4459150326797385\n",
      "gs: 0.45372168284789655\n",
      "gs: 0.4975535168195718\n",
      "gs: 0.5490028490028489\n",
      "gs: 0.5607843137254902\n",
      "gs: 0.5560224089635855\n",
      "gs: 0.5007309941520467\n",
      "gs: 0.46516516516516515\n",
      "gs: 0.4528787878787879\n",
      "gs: 0.44036697247706413\n",
      "gs: 0.47088948787062\n",
      "gs: 0.4870359794403198\n",
      "gs: 0.4949999999999999\n",
      "gs: 0.4929139825691549\n",
      "gs: 0.5153110047846889\n",
      "gs: 0.5382849901436215\n",
      "gs: 0.4298333333333334\n",
      "gs: 0.4905864197530864\n",
      "gs: 0.524188790560472\n",
      "gs: 0.5429597701149426\n",
      "gs: 0.5549435028248587\n",
      "sasgf\n",
      "gs: 0.48714689265536715\n",
      "gs: 0.49811594202898557\n",
      "gs: 0.5052083333333333\n",
      "gs: 0.49639977759243814\n",
      "gs: 0.4938271604938272\n",
      "gs: 0.48688230008984723\n",
      "gs: 0.46797385620915033\n",
      "gs: 0.4698333333333333\n",
      "gs: 0.4762626262626263\n",
      "gs: 0.4452300785634119\n",
      "gs: 0.4963333333333334\n",
      "gs: 0.4940755973364669\n",
      "gs: 0.5100931677018634\n",
      "gs: 0.5110877192982456\n",
      "gs: 0.5112847222222222\n",
      "gs: 0.5108919766920663\n",
      "gs: 0.5041394335511982\n",
      "gs: 0.46838006230529605\n",
      "gs: 0.5001412429378531\n",
      "gs: 0.4916619074814392\n",
      "gs: 0.49744008714596943\n",
      "gs: 0.5059298245614035\n",
      "gs: 0.48058995268297605\n",
      "gs: 0.5052083333333334\n",
      "gs: 0.5087948450017417\n",
      "gs: 0.5077777777777779\n",
      "gs: 0.4758086253369272\n",
      "gs: 0.49553571428571425\n",
      "gs: 0.49371345029239766\n",
      "gs: 0.48419540229885055\n",
      "gs: 0.49943977591036426\n",
      "gs: 0.47464387464387464\n",
      "gs: 0.4831884057971015\n",
      "gs: 0.4823099415204679\n",
      "gs: 0.4764880952380954\n",
      "gs: 0.4723723723723724\n",
      "gs: 0.4626633305532387\n",
      "gs: 0.45725308641975304\n",
      "gs: 0.4515456506110712\n",
      "gs: 0.4455750224618149\n",
      "gs: 0.4262897391966494\n",
      "gs: 0.4241052631578947\n",
      "gs: 0.416912165848336\n",
      "gs: 0.40186335403726703\n",
      "gs: 0.3799721351445489\n",
      "gs: 0.3562499999999999\n",
      "gs: 0.45676206509539846\n",
      "gs: 0.3823863636363637\n",
      "gs: 0.38782771535580524\n",
      "gs: 0.39314814814814814\n",
      "gs: 0.46462264150943394\n",
      "gs: 0.5085434173669467\n",
      "gs: 0.48714689265536715\n",
      "gs: 0.47464387464387464\n",
      "gs: 0.4823099415204679\n",
      "gs: 0.46772727272727277\n",
      "gs: 0.4455750224618149\n",
      "gs: 0.4262897391966494\n",
      "gs: 0.49360143275805934\n",
      "gs: 0.5108452950558213\n",
      "gs: 0.472067901234568\n",
      "gs: 0.4827327327327327\n",
      "gs: 0.49608695652173923\n",
      "gs: 0.49928160919540227\n",
      "gs: 0.5024216524216524\n",
      "gs: 0.5055084745762712\n",
      "gs: 0.5085434173669467\n",
      "sasgf\n",
      "gs: 0.6161064425770308\n",
      "gs: 0.6072033898305085\n",
      "gs: 0.5981481481481481\n",
      "gs: 0.5700292397660818\n",
      "gs: 0.5893429487179487\n",
      "gs: 0.5734848484848485\n",
      "gs: 0.5480350877192981\n",
      "gs: 0.5402867247694834\n",
      "gs: 0.5305555555555557\n",
      "gs: 0.5204240666908301\n",
      "gs: 0.5253314393939394\n",
      "gs: 0.5373949579831934\n",
      "gs: 0.5362139917695474\n",
      "gs: 0.538125\n",
      "gs: 0.54375\n",
      "gs: 0.5772222222222222\n",
      "gs: 0.5863612836438924\n",
      "gs: 0.6178374262326289\n",
      "gs: 0.6175480769230769\n",
      "gs: 0.6115744069015098\n",
      "gs: 0.5931818181818181\n",
      "gs: 0.5822822822822823\n",
      "gs: 0.5873511904761906\n",
      "gs: 0.6056060606060605\n",
      "gs: 0.6109813733666944\n",
      "gs: 0.5887445887445887\n",
      "gs: 0.5278882575757575\n",
      "gs: 0.5830555555555555\n",
      "gs: 0.5474053724053723\n",
      "gs: 0.5331300813008131\n",
      "gs: 0.4501924001924002\n",
      "gs: 0.46427015250544656\n",
      "gs: 0.476031746031746\n",
      "gs: 0.4872641509433962\n",
      "gs: 0.5403903903903904\n",
      "gs: 0.5700292397660818\n",
      "gs: 0.5889367816091954\n",
      "gs: 0.6072033898305085\n",
      "gs: 0.6161064425770308\n",
      "gs: 0.6161064425770308\n",
      "gs: 0.5889367816091954\n",
      "gs: 0.5403903903903904\n",
      "gs: 0.476031746031746\n",
      "gs: 0.46458333333333335\n",
      "gs: 0.4410130718954248\n",
      "gs: 0.41650000000000004\n",
      "gs: 0.4465710443747198\n",
      "gs: 0.4539930555555555\n",
      "gs: 0.4702907208283553\n",
      "gs: 0.4794444444444445\n",
      "gs: 0.48303726924416585\n",
      "gs: 0.4820767195767196\n",
      "gs: 0.534962962962963\n",
      "gs: 0.533891500195848\n",
      "gs: 0.528533103382198\n",
      "gs: 0.5401864801864802\n",
      "gs: 0.5644993979809205\n",
      "gs: 0.5936666666666666\n",
      "gs: 0.5198333333333333\n",
      "gs: 0.5822822822822823\n",
      "gs: 0.5981481481481481\n",
      "gs: 0.476031746031746\n",
      "gs: 0.4410130718954248\n",
      "gs: 0.4288778877887789\n",
      "gs: 0.41650000000000004\n",
      "gs: 0.4465710443747198\n",
      "gs: 0.4539930555555555\n",
      "gs: 0.4657528641571196\n",
      "gs: 0.48303726924416585\n",
      "gs: 0.5505892255892256\n",
      "gs: 0.596875\n",
      "gs: 0.5440705128205129\n",
      "gs: 0.5822822822822823\n",
      "gs: 0.6159604519774011\n",
      "gs: 0.6204481792717087\n",
      "sasgf\n",
      "gs: 0.612005649717514\n",
      "gs: 0.6034188034188034\n",
      "gs: 0.594683908045977\n",
      "gs: 0.567551622418879\n",
      "gs: 0.5802540138988739\n",
      "gs: 0.5581666666666666\n",
      "gs: 0.5306738383385627\n",
      "gs: 0.552100381887616\n",
      "gs: 0.5094223484848484\n",
      "gs: 0.5105322128851542\n",
      "gs: 0.5322916666666666\n",
      "gs: 0.5259462759462759\n",
      "gs: 0.5317384370015948\n",
      "gs: 0.5201647411210953\n",
      "gs: 0.5526322967812329\n",
      "gs: 0.5450877192982456\n",
      "gs: 0.6103324348607368\n",
      "gs: 0.6062784567457465\n",
      "gs: 0.6109567901234567\n",
      "gs: 0.5998484848484847\n",
      "gs: 0.5796130952380952\n",
      "gs: 0.62296918767507\n",
      "gs: 0.6250700280112045\n",
      "gs: 0.6035187526337968\n",
      "gs: 0.6148148148148147\n",
      "gs: 0.6193939393939393\n",
      "gs: 0.5951075575861412\n",
      "gs: 0.5834035087719297\n",
      "gs: 0.5732400932400932\n",
      "gs: 0.5588052122257354\n",
      "gs: 0.5497916666666666\n",
      "gs: 0.5387152777777778\n",
      "gs: 0.529700061842919\n",
      "gs: 0.527219037693243\n",
      "gs: 0.5369825708061002\n",
      "gs: 0.5290519877675841\n",
      "gs: 0.5581845238095239\n",
      "gs: 0.5767543859649122\n",
      "gs: 0.594683908045977\n",
      "gs: 0.6034188034188034\n",
      "gs: 0.612005649717514\n",
      "gs: 0.6204481792717087\n",
      "gs: 0.6034188034188034\n",
      "gs: 0.5087227414330219\n",
      "gs: 0.4544117647058823\n",
      "gs: 0.43116666666666664\n",
      "gs: 0.4069727891156462\n",
      "gs: 0.39450171821305846\n",
      "gs: 0.40746527777777775\n",
      "gs: 0.4190877192982456\n",
      "gs: 0.447256728778468\n",
      "gs: 0.4547682202854616\n",
      "gs: 0.4774642981539533\n",
      "gs: 0.4851540616246498\n",
      "gs: 0.48822751322751323\n",
      "gs: 0.49082817757516556\n",
      "gs: 0.5348902941458273\n",
      "gs: 0.5350952380952381\n",
      "gs: 0.5300595238095238\n",
      "gs: 0.537037037037037\n",
      "gs: 0.5636532738095238\n",
      "gs: 0.5868095238095238\n",
      "gs: 0.5378930817610063\n",
      "gs: 0.572972972972973\n",
      "gs: 0.62296918767507\n",
      "gs: 0.612005649717514\n",
      "gs: 0.49827044025157224\n",
      "gs: 0.47676282051282054\n",
      "gs: 0.43116666666666664\n",
      "gs: 0.4191919191919192\n",
      "gs: 0.39450171821305846\n",
      "gs: 0.4190877192982456\n",
      "gs: 0.43886897650338513\n",
      "gs: 0.4774642981539533\n",
      "gs: 0.49298245614035086\n",
      "gs: 0.5348902941458273\n",
      "gs: 0.5873809523809523\n",
      "gs: 0.545171339563863\n",
      "gs: 0.5796130952380952\n",
      "gs: 0.62296918767507\n",
      "sasgf\n",
      "gs: 0.5795518207282914\n",
      "gs: 0.5592592592592593\n",
      "gs: 0.5488505747126435\n",
      "gs: 0.5691349149293076\n",
      "gs: 0.550400641025641\n",
      "gs: 0.5361231361231361\n",
      "gs: 0.5399196042053185\n",
      "gs: 0.511859649122807\n",
      "gs: 0.5181999203504579\n",
      "gs: 0.5295880149812735\n",
      "gs: 0.5308238636363635\n",
      "gs: 0.5227905655015629\n",
      "gs: 0.5425\n",
      "gs: 0.5410997004706889\n",
      "gs: 0.5821273964131106\n",
      "gs: 0.582\n",
      "gs: 0.5720159908623643\n",
      "gs: 0.5664262820512821\n",
      "gs: 0.5593650793650794\n",
      "gs: 0.539563862928349\n",
      "gs: 0.5438271604938272\n",
      "gs: 0.5631626235399819\n",
      "gs: 0.5698412698412698\n",
      "gs: 0.5750000000000001\n",
      "gs: 0.5673486088379706\n",
      "gs: 0.5476972184222526\n",
      "gs: 0.5596588802373008\n",
      "gs: 0.5449935815147624\n",
      "gs: 0.531577747378021\n",
      "gs: 0.489760348583878\n",
      "gs: 0.4869841269841269\n",
      "gs: 0.5095614665708125\n",
      "gs: 0.5142618849040866\n",
      "gs: 0.5588405797101448\n",
      "gs: 0.5683908045977012\n",
      "gs: 0.5694915254237288\n",
      "gs: 0.5795518207282914\n",
      "gs: 0.5795518207282914\n",
      "gs: 0.5694915254237288\n",
      "gs: 0.5592592592592593\n",
      "gs: 0.5165191740412979\n",
      "gs: 0.48242424242424253\n",
      "gs: 0.47064220183486233\n",
      "gs: 0.44641744548286616\n",
      "gs: 0.4339622641509434\n",
      "gs: 0.4522435897435898\n",
      "gs: 0.45865219874357516\n",
      "gs: 0.4636165577342048\n",
      "gs: 0.47000000000000003\n",
      "gs: 0.47274764679515907\n",
      "gs: 0.4722222222222222\n",
      "gs: 0.47108771929824556\n",
      "gs: 0.4644927536231884\n",
      "gs: 0.5119597774925118\n",
      "gs: 0.49700854700854696\n",
      "gs: 0.48740031897926633\n",
      "gs: 0.4913194444444444\n",
      "gs: 0.5220595137519948\n",
      "gs: 0.5661072261072261\n",
      "gs: 0.5073333333333334\n",
      "gs: 0.5438271604938272\n",
      "gs: 0.5859943977591037\n",
      "gs: 0.5694915254237288\n",
      "gs: 0.5165191740412979\n",
      "gs: 0.48242424242424253\n",
      "gs: 0.44641744548286616\n",
      "gs: 0.4339622641509434\n",
      "gs: 0.4522435897435898\n",
      "gs: 0.4673441028313358\n",
      "gs: 0.4717171717171717\n",
      "gs: 0.46939443535188213\n",
      "gs: 0.5087685976574865\n",
      "gs: 0.5742256322818982\n",
      "gs: 0.5216828478964401\n",
      "gs: 0.5521212121212121\n",
      "gs: 0.5715942028985507\n",
      "gs: 0.5789173789173789\n",
      "gs: 0.5824858757062146\n",
      "gs: 0.5859943977591037\n",
      "sasgf\n",
      "gs: 0.592156862745098\n",
      "gs: 0.572934472934473\n",
      "gs: 0.5630747126436781\n",
      "gs: 0.5218750000000001\n",
      "gs: 0.5434636118598384\n",
      "gs: 0.5359041167274623\n",
      "gs: 0.5268333333333334\n",
      "gs: 0.4944912280701754\n",
      "gs: 0.5140630663283798\n",
      "gs: 0.5209160571229537\n",
      "gs: 0.5248176014591883\n",
      "gs: 0.542743830787309\n",
      "gs: 0.5575409265064438\n",
      "gs: 0.5538888888888889\n",
      "gs: 0.5838333333333333\n",
      "gs: 0.5804498870939725\n",
      "gs: 0.5763071895424836\n",
      "gs: 0.5888888888888888\n",
      "gs: 0.5584848484848485\n",
      "gs: 0.5462462462462463\n",
      "gs: 0.594774011299435\n",
      "gs: 0.5856481481481481\n",
      "gs: 0.5894440450515218\n",
      "gs: 0.5941269841269841\n",
      "gs: 0.5708053189899897\n",
      "gs: 0.5362268938021022\n",
      "gs: 0.5713756025213201\n",
      "gs: 0.5379166666666666\n",
      "gs: 0.520436507936508\n",
      "gs: 0.5133680555555555\n",
      "gs: 0.5287638287638289\n",
      "gs: 0.5431372549019607\n",
      "gs: 0.5190026954177897\n",
      "gs: 0.5111111111111111\n",
      "gs: 0.5530434782608695\n",
      "gs: 0.572934472934473\n",
      "gs: 0.592156862745098\n",
      "gs: 0.5826271186440678\n",
      "gs: 0.5218750000000001\n",
      "gs: 0.47762345679012347\n",
      "gs: 0.4660436137071651\n",
      "gs: 0.4422222222222223\n",
      "gs: 0.41747572815533984\n",
      "gs: 0.4413757165190203\n",
      "gs: 0.4510000000000001\n",
      "gs: 0.466604823747681\n",
      "gs: 0.49526335733232285\n",
      "gs: 0.49703703703703705\n",
      "gs: 0.49836897426603843\n",
      "gs: 0.5369565217391304\n",
      "gs: 0.5360244826581172\n",
      "gs: 0.5297811878374536\n",
      "gs: 0.5235171261487052\n",
      "gs: 0.5336111111111111\n",
      "gs: 0.5708333333333333\n",
      "gs: 0.47326732673267324\n",
      "gs: 0.5257716049382716\n",
      "gs: 0.5395454545454546\n",
      "gs: 0.592156862745098\n",
      "gs: 0.5111111111111111\n",
      "gs: 0.45424528301886785\n",
      "gs: 0.41747572815533984\n",
      "gs: 0.4510000000000001\n",
      "gs: 0.466604823747681\n",
      "gs: 0.4829473684210525\n",
      "gs: 0.5\n",
      "gs: 0.5425283979631805\n",
      "gs: 0.577526395173454\n",
      "gs: 0.5257716049382716\n",
      "gs: 0.5656432748538012\n",
      "gs: 0.5840455840455842\n",
      "gs: 0.5899717514124294\n",
      "sasgf\n",
      "gs: 0.5446327683615818\n",
      "gs: 0.5336182336182337\n",
      "gs: 0.5224137931034484\n",
      "gs: 0.5237472766884531\n",
      "gs: 0.5354978354978355\n",
      "gs: 0.5257283729269386\n",
      "gs: 0.5196180555555556\n",
      "gs: 0.5274093722369585\n",
      "gs: 0.5304924242424243\n",
      "gs: 0.5297457331940091\n",
      "gs: 0.5394179894179894\n",
      "gs: 0.5213333333333333\n",
      "gs: 0.5491913600347336\n",
      "gs: 0.5447788417692658\n",
      "gs: 0.5630718954248365\n",
      "gs: 0.5641157433847326\n",
      "gs: 0.5644444444444444\n",
      "gs: 0.5475475475475475\n",
      "gs: 0.5375000000000001\n",
      "gs: 0.555367231638418\n",
      "gs: 0.5604840642223821\n",
      "gs: 0.5174031642116749\n",
      "gs: 0.4789785681714547\n",
      "gs: 0.5545808966861598\n",
      "gs: 0.5300411522633746\n",
      "gs: 0.5364705882352941\n",
      "gs: 0.5059394631639064\n",
      "gs: 0.5325892857142859\n",
      "gs: 0.5195906432748536\n",
      "gs: 0.5110144927536232\n",
      "gs: 0.5224137931034484\n",
      "gs: 0.5336182336182337\n",
      "gs: 0.5554621848739495\n",
      "gs: 0.5446327683615818\n",
      "gs: 0.5336182336182337\n",
      "gs: 0.4876106194690266\n",
      "gs: 0.475595238095238\n",
      "gs: 0.45090909090909087\n",
      "gs: 0.4679012345679013\n",
      "gs: 0.4765498652291105\n",
      "gs: 0.4787301587301588\n",
      "gs: 0.4779572694111517\n",
      "gs: 0.476\n",
      "gs: 0.4734968734968735\n",
      "gs: 0.5388888888888889\n",
      "gs: 0.5335416666666666\n",
      "gs: 0.5346764346764347\n",
      "gs: 0.5486095661846496\n",
      "gs: 0.5576738946775557\n",
      "gs: 0.49999999999999994\n",
      "gs: 0.5336336336336337\n",
      "gs: 0.5627450980392157\n",
      "gs: 0.5336182336182337\n",
      "gs: 0.4876106194690266\n",
      "gs: 0.45090909090909087\n",
      "gs: 0.47304097771387493\n",
      "gs: 0.4787301587301588\n",
      "gs: 0.4779572694111517\n",
      "gs: 0.5227497885707382\n",
      "gs: 0.5554135407983699\n",
      "gs: 0.4553191489361702\n",
      "gs: 0.5174454828660436\n",
      "gs: 0.548695652173913\n",
      "gs: 0.5522988505747126\n",
      "gs: 0.5627450980392157\n",
      "sasgf\n",
      "gs: 0.5085434173669467\n",
      "gs: 0.49646892655367225\n",
      "gs: 0.4864583333333334\n",
      "gs: 0.50806227411732\n",
      "gs: 0.5037037037037038\n",
      "gs: 0.5077042894799905\n",
      "gs: 0.5156939378148342\n",
      "gs: 0.5169953682952338\n",
      "gs: 0.5137543859649123\n",
      "gs: 0.5129430505774593\n",
      "gs: 0.45859619210002844\n",
      "gs: 0.46337448559670785\n",
      "gs: 0.44398148148148137\n",
      "gs: 0.4909847244953628\n",
      "gs: 0.49079861111111106\n",
      "gs: 0.48528313163006126\n",
      "gs: 0.4719095719095719\n",
      "gs: 0.46383333333333326\n",
      "gs: 0.45109461260232236\n",
      "gs: 0.4393429487179487\n",
      "gs: 0.42571428571428577\n",
      "gs: 0.4328616352201258\n",
      "gs: 0.5111111111111111\n",
      "gs: 0.507659478885894\n",
      "gs: 0.5143429487179488\n",
      "gs: 0.5181372549019608\n",
      "gs: 0.5071876704855429\n",
      "gs: 0.4945098039215687\n",
      "gs: 0.49783950617283956\n",
      "gs: 0.5133072831868012\n",
      "gs: 0.4748636115657392\n",
      "gs: 0.49270833333333336\n",
      "gs: 0.49811594202898557\n",
      "gs: 0.4936781609195402\n",
      "gs: 0.49646892655367225\n",
      "gs: 0.5085434173669467\n",
      "gs: 0.49646892655367225\n",
      "gs: 0.48418803418803424\n",
      "gs: 0.47169540229885054\n",
      "gs: 0.49110830172777076\n",
      "gs: 0.49270833333333336\n",
      "gs: 0.49287878787878786\n",
      "gs: 0.4905864197530864\n",
      "gs: 0.4664261664261664\n",
      "gs: 0.46287878787878783\n",
      "gs: 0.44736770321876707\n",
      "gs: 0.43439434129089305\n",
      "gs: 0.47781395853685005\n",
      "gs: 0.4741869918699187\n",
      "gs: 0.4442962962962962\n",
      "gs: 0.434236860001943\n",
      "gs: 0.4533958510940608\n",
      "gs: 0.5019444444444444\n",
      "gs: 0.36323024054982816\n",
      "gs: 0.3715986394557823\n",
      "gs: 0.42571428571428577\n",
      "gs: 0.5148459383753502\n",
      "gs: 0.49646892655367225\n",
      "gs: 0.48418803418803424\n",
      "gs: 0.47169540229885054\n",
      "gs: 0.49319319319319327\n",
      "gs: 0.4905864197530864\n",
      "gs: 0.4818910256410257\n",
      "gs: 0.44317005177220237\n",
      "gs: 0.458089133089133\n",
      "gs: 0.5162574404761905\n",
      "gs: 0.44675925925925924\n",
      "gs: 0.48552631578947364\n",
      "gs: 0.4915942028985508\n",
      "gs: 0.5034188034188035\n",
      "gs: 0.5148459383753502\n",
      "sasgf\n",
      "gs: 0.5264705882352941\n",
      "gs: 0.4810144927536233\n",
      "gs: 0.5090413943355121\n",
      "gs: 0.5013333333333333\n",
      "gs: 0.49600769600769595\n",
      "gs: 0.48982683982683983\n",
      "gs: 0.488888888888889\n",
      "gs: 0.4947829549980088\n",
      "gs: 0.5033949498610608\n",
      "gs: 0.49023809523809525\n",
      "gs: 0.5199105956264347\n",
      "gs: 0.5152130625248905\n",
      "gs: 0.5307307307307307\n",
      "gs: 0.534077380952381\n",
      "gs: 0.5315212810788031\n",
      "gs: 0.5349415204678364\n",
      "gs: 0.5301994301994302\n",
      "gs: 0.5034267912772586\n",
      "gs: 0.5129604672057502\n",
      "gs: 0.46371100164203605\n",
      "gs: 0.41035298960584543\n",
      "gs: 0.5240871116117525\n",
      "gs: 0.4942592592592593\n",
      "gs: 0.4979689366786141\n",
      "gs: 0.47021604938271605\n",
      "gs: 0.4793939393939394\n",
      "gs: 0.4758258258258259\n",
      "gs: 0.46473214285714287\n",
      "gs: 0.4926724137931035\n",
      "gs: 0.5041310541310542\n",
      "gs: 0.5153954802259887\n",
      "gs: 0.5264705882352941\n",
      "gs: 0.5264705882352941\n",
      "gs: 0.4926724137931035\n",
      "gs: 0.46915204678362565\n",
      "gs: 0.4195454545454545\n",
      "gs: 0.42768974145120936\n",
      "gs: 0.4365923795830339\n",
      "gs: 0.43873015873015864\n",
      "gs: 0.5055213241512625\n",
      "gs: 0.5040404040404041\n",
      "gs: 0.5003921568627451\n",
      "gs: 0.5020582484305856\n",
      "gs: 0.5198907732106928\n",
      "gs: 0.5215401785714286\n",
      "gs: 0.4912621359223301\n",
      "gs: 0.49439102564102566\n",
      "gs: 0.5277298850574712\n",
      "gs: 0.5350140056022409\n",
      "gs: 0.5041310541310542\n",
      "gs: 0.43228228228228227\n",
      "gs: 0.4195454545454545\n",
      "gs: 0.42768974145120936\n",
      "gs: 0.4365923795830339\n",
      "gs: 0.4379807692307693\n",
      "gs: 0.4993033786137234\n",
      "gs: 0.5171130952380952\n",
      "gs: 0.45235507246376816\n",
      "gs: 0.4974603174603175\n",
      "gs: 0.5226608187134503\n",
      "gs: 0.5350140056022409\n",
      "sasgf\n",
      "gs: 0.5142655367231639\n",
      "gs: 0.5024216524216524\n",
      "gs: 0.5095352564102563\n",
      "gs: 0.5167535174570088\n",
      "gs: 0.5154401154401155\n",
      "gs: 0.520708202599731\n",
      "gs: 0.5231481481481481\n",
      "gs: 0.521819499818775\n",
      "gs: 0.5200757575757575\n",
      "gs: 0.49730036942313155\n",
      "gs: 0.5160978556327394\n",
      "gs: 0.513516935739158\n",
      "gs: 0.5364709397878381\n",
      "gs: 0.5360021786492374\n",
      "gs: 0.5322916666666666\n",
      "gs: 0.528888888888889\n",
      "gs: 0.5311096136567834\n",
      "gs: 0.5269350587107597\n",
      "gs: 0.5211419753086419\n",
      "gs: 0.5028787878787879\n",
      "gs: 0.5066066066066066\n",
      "gs: 0.526412429378531\n",
      "gs: 0.5181891025641026\n",
      "gs: 0.5237959261374452\n",
      "gs: 0.5281590413943356\n",
      "gs: 0.5163706025774991\n",
      "gs: 0.501875\n",
      "gs: 0.5238505747126438\n",
      "gs: 0.5273148148148148\n",
      "gs: 0.5206012378426171\n",
      "gs: 0.5075757575757576\n",
      "gs: 0.5160160160160161\n",
      "gs: 0.512094395280236\n",
      "gs: 0.49037356321839076\n",
      "gs: 0.5259103641456584\n",
      "gs: 0.49037356321839076\n",
      "gs: 0.47811594202898555\n",
      "gs: 0.46564327485380114\n",
      "gs: 0.4744627054361568\n",
      "gs: 0.4822822822822823\n",
      "gs: 0.4827634139560745\n",
      "gs: 0.47616801437556155\n",
      "gs: 0.4643822577574719\n",
      "gs: 0.45964052287581697\n",
      "gs: 0.4375231910946197\n",
      "gs: 0.43130135962946364\n",
      "gs: 0.49734071420818404\n",
      "gs: 0.488034188034188\n",
      "gs: 0.46573039363737034\n",
      "gs: 0.48774552074350874\n",
      "gs: 0.4962733502299822\n",
      "gs: 0.531060606060606\n",
      "gs: 0.4615000000000001\n",
      "gs: 0.4952160493827161\n",
      "gs: 0.49908256880733937\n",
      "gs: 0.5309322033898305\n",
      "gs: 0.5341736694677872\n",
      "gs: 0.49037356321839076\n",
      "gs: 0.46564327485380114\n",
      "gs: 0.4822822822822823\n",
      "gs: 0.4813271604938272\n",
      "gs: 0.47616801437556155\n",
      "gs: 0.43130135962946364\n",
      "gs: 0.46192964123998603\n",
      "gs: 0.5323076923076923\n",
      "gs: 0.44253472222222234\n",
      "gs: 0.4912772585669782\n",
      "gs: 0.5173976608187133\n",
      "gs: 0.5208695652173914\n",
      "gs: 0.5242816091954022\n",
      "gs: 0.5309322033898305\n",
      "sasgf\n",
      "gs: 0.6201680672268908\n",
      "gs: 0.6114406779661016\n",
      "gs: 0.6025641025641025\n",
      "gs: 0.5935344827586206\n",
      "gs: 0.5654867256637168\n",
      "gs: 0.5749869723814487\n",
      "gs: 0.5695\n",
      "gs: 0.5418402777777778\n",
      "gs: 0.5426759410801965\n",
      "gs: 0.5409259259259259\n",
      "gs: 0.531823124320406\n",
      "gs: 0.544757924068269\n",
      "gs: 0.5271181001283697\n",
      "gs: 0.5319404874960432\n",
      "gs: 0.5547545538746527\n",
      "gs: 0.5869731800766284\n",
      "gs: 0.5967741935483871\n",
      "gs: 0.6153127246585192\n",
      "gs: 0.6183641975308642\n",
      "gs: 0.6210175145954961\n",
      "gs: 0.6247247247247247\n",
      "gs: 0.6229532163742689\n",
      "gs: 0.6090517241379311\n",
      "gs: 0.6068181818181818\n",
      "gs: 0.6112593828190158\n",
      "gs: 0.5670222634508348\n",
      "gs: 0.5420212765957447\n",
      "gs: 0.587132867132867\n",
      "gs: 0.5572966507177033\n",
      "gs: 0.5626273541216424\n",
      "gs: 0.5118657937806873\n",
      "gs: 0.5319172113289761\n",
      "gs: 0.5365229110512129\n",
      "gs: 0.5368827160493828\n",
      "gs: 0.5256880733944954\n",
      "gs: 0.5558035714285715\n",
      "gs: 0.5654867256637168\n",
      "gs: 0.6025641025641025\n",
      "gs: 0.6201680672268908\n",
      "gs: 0.6201680672268908\n",
      "gs: 0.6114406779661016\n",
      "gs: 0.5843478260869566\n",
      "gs: 0.5359090909090909\n",
      "gs: 0.5046728971962617\n",
      "gs: 0.4828571428571428\n",
      "gs: 0.4245000000000001\n",
      "gs: 0.41212121212121205\n",
      "gs: 0.4345584939489019\n",
      "gs: 0.4517894736842106\n",
      "gs: 0.5488636363636363\n",
      "gs: 0.5526733254994125\n",
      "gs: 0.555590686979017\n",
      "gs: 0.5744419642857144\n",
      "gs: 0.5841212458286986\n",
      "gs: 0.5927142857142857\n",
      "gs: 0.5824324324324325\n",
      "gs: 0.6039130434782609\n",
      "gs: 0.6239495798319328\n",
      "gs: 0.6114406779661016\n",
      "gs: 0.5256880733944954\n",
      "gs: 0.46019417475728147\n",
      "gs: 0.4245000000000001\n",
      "gs: 0.41212121212121205\n",
      "gs: 0.42407235621521333\n",
      "gs: 0.4345584939489019\n",
      "gs: 0.44375000000000003\n",
      "gs: 0.509610123119015\n",
      "gs: 0.590497737556561\n",
      "gs: 0.5212871287128713\n",
      "gs: 0.5824324324324325\n",
      "gs: 0.6239495798319328\n",
      "sasgf\n",
      "gs: 0.5943977591036415\n",
      "gs: 0.5752136752136753\n",
      "gs: 0.5653735632183908\n",
      "gs: 0.5451754385964912\n",
      "gs: 0.5135135135135135\n",
      "gs: 0.5574952232065312\n",
      "gs: 0.5511666666666667\n",
      "gs: 0.5366573902288188\n",
      "gs: 0.5013229678123294\n",
      "gs: 0.5212138436276368\n",
      "gs: 0.5273200757575758\n",
      "gs: 0.5183733890630442\n",
      "gs: 0.5510298260954046\n",
      "gs: 0.5728047324432867\n",
      "gs: 0.5755927952576378\n",
      "gs: 0.5950359389038634\n",
      "gs: 0.5995370370370371\n",
      "gs: 0.6006533222129552\n",
      "gs: 0.6007575757575758\n",
      "gs: 0.5911293721028233\n",
      "gs: 0.5820175438596491\n",
      "gs: 0.600280112044818\n",
      "gs: 0.5952321378926884\n",
      "gs: 0.5984567901234569\n",
      "gs: 0.5725832959808755\n",
      "gs: 0.5571496212121212\n",
      "gs: 0.5696800595238095\n",
      "gs: 0.5460416666666666\n",
      "gs: 0.5414682539682539\n",
      "gs: 0.49456895263708356\n",
      "gs: 0.502414451971513\n",
      "gs: 0.5075320512820514\n",
      "gs: 0.49759658580413296\n",
      "gs: 0.5242559523809524\n",
      "gs: 0.5451754385964912\n",
      "gs: 0.5553623188405797\n",
      "gs: 0.5653735632183908\n",
      "gs: 0.5943977591036415\n",
      "gs: 0.5943977591036415\n",
      "gs: 0.5848870056497175\n",
      "gs: 0.5242559523809524\n",
      "gs: 0.5135135135135135\n",
      "gs: 0.4685358255451715\n",
      "gs: 0.4567610062893082\n",
      "gs: 0.4447619047619047\n",
      "gs: 0.4200647249190938\n",
      "gs: 0.4456666666666667\n",
      "gs: 0.4512746512746513\n",
      "gs: 0.4591364111758554\n",
      "gs: 0.5334830979888746\n",
      "gs: 0.5335416666666666\n",
      "gs: 0.5283891547049442\n",
      "gs: 0.5530548451264563\n",
      "gs: 0.5650690006483282\n",
      "gs: 0.5799164554585563\n",
      "gs: 0.553582554517134\n",
      "gs: 0.5820175438596491\n",
      "gs: 0.600280112044818\n",
      "gs: 0.5943977591036415\n",
      "gs: 0.4914373088685015\n",
      "gs: 0.4685358255451715\n",
      "gs: 0.4200647249190938\n",
      "gs: 0.4387354524926177\n",
      "gs: 0.4512746512746513\n",
      "gs: 0.4591364111758554\n",
      "gs: 0.5186160510715915\n",
      "gs: 0.5727272727272728\n",
      "gs: 0.5110544217687075\n",
      "gs: 0.5578703703703705\n",
      "gs: 0.5931623931623932\n",
      "sasgf\n",
      "gs: 0.5847338935574231\n",
      "gs: 0.5755649717514124\n",
      "gs: 0.5662393162393162\n",
      "gs: 0.5633333333333332\n",
      "gs: 0.5597557204700062\n",
      "gs: 0.5480087288597928\n",
      "gs: 0.5396299103195655\n",
      "gs: 0.4869187675070028\n",
      "gs: 0.495093383982273\n",
      "gs: 0.5141447699587235\n",
      "gs: 0.518042264752791\n",
      "gs: 0.5279316040027202\n",
      "gs: 0.5413246694026447\n",
      "gs: 0.5553703703703704\n",
      "gs: 0.5561403508771929\n",
      "gs: 0.5841194968553458\n",
      "gs: 0.5821950635034748\n",
      "gs: 0.5794753086419753\n",
      "gs: 0.5757575757575757\n",
      "gs: 0.5640640640640642\n",
      "gs: 0.5550595238095237\n",
      "gs: 0.5427728613569321\n",
      "gs: 0.5910364145658263\n",
      "gs: 0.5807575757575758\n",
      "gs: 0.5864197530864197\n",
      "gs: 0.5909455128205129\n",
      "gs: 0.5665113871635612\n",
      "gs: 0.5599635202918376\n",
      "gs: 0.5322679057542476\n",
      "gs: 0.5210416666666666\n",
      "gs: 0.503968253968254\n",
      "gs: 0.4661458333333333\n",
      "gs: 0.45029375386518233\n",
      "gs: 0.4642174743790169\n",
      "gs: 0.44047619047619047\n",
      "gs: 0.5067567567567568\n",
      "gs: 0.5471014492753624\n",
      "gs: 0.5567528735632185\n",
      "gs: 0.5662393162393162\n",
      "gs: 0.5847338935574231\n",
      "gs: 0.5847338935574231\n",
      "gs: 0.5471014492753624\n",
      "gs: 0.5171130952380951\n",
      "gs: 0.4633956386292836\n",
      "gs: 0.44047619047619047\n",
      "gs: 0.3919141914191419\n",
      "gs: 0.37916666666666665\n",
      "gs: 0.39297739297739287\n",
      "gs: 0.4350877192982456\n",
      "gs: 0.4430578286961266\n",
      "gs: 0.4502190362405416\n",
      "gs: 0.45665113871635615\n",
      "gs: 0.46242263483642787\n",
      "gs: 0.4859943977591037\n",
      "gs: 0.4884259259259259\n",
      "gs: 0.49224433033804027\n",
      "gs: 0.5419472502805837\n",
      "gs: 0.543030303030303\n",
      "gs: 0.5452777777777778\n",
      "gs: 0.5522693452380952\n",
      "gs: 0.5554152637485971\n",
      "gs: 0.5653935185185185\n",
      "gs: 0.5189393939393939\n",
      "gs: 0.5427728613569321\n",
      "gs: 0.5847338935574231\n",
      "gs: 0.49621212121212105\n",
      "gs: 0.4633956386292836\n",
      "gs: 0.3919141914191419\n",
      "gs: 0.37916666666666665\n",
      "gs: 0.4430578286961266\n",
      "gs: 0.45665113871635615\n",
      "gs: 0.472212154162136\n",
      "gs: 0.4884259259259259\n",
      "gs: 0.516108499804152\n",
      "gs: 0.560358584436309\n",
      "gs: 0.5023148148148149\n",
      "gs: 0.5726495726495726\n",
      "sasgf\n",
      "gs: 0.5521186440677965\n",
      "gs: 0.5399146451033243\n",
      "gs: 0.5323434228060155\n",
      "gs: 0.5400871459694989\n",
      "gs: 0.5415\n",
      "gs: 0.5356902356902357\n",
      "gs: 0.5272569444444445\n",
      "gs: 0.5317894736842106\n",
      "gs: 0.5448471668478918\n",
      "gs: 0.5399338209683038\n",
      "gs: 0.5410168718650251\n",
      "gs: 0.49077821403402805\n",
      "gs: 0.49583197655486816\n",
      "gs: 0.47845643939393934\n",
      "gs: 0.5541269841269842\n",
      "gs: 0.5573674752920035\n",
      "gs: 0.5509465612269351\n",
      "gs: 0.5425925925925926\n",
      "gs: 0.5359090909090909\n",
      "gs: 0.5509465612269351\n",
      "gs: 0.522545008183306\n",
      "gs: 0.5124018364141597\n",
      "gs: 0.5344354913401872\n",
      "gs: 0.5420448179271709\n",
      "gs: 0.5425925925925926\n",
      "gs: 0.5108715184186883\n",
      "gs: 0.5120239088128996\n",
      "gs: 0.5042424242424243\n",
      "gs: 0.5296783625730994\n",
      "gs: 0.5402898550724637\n",
      "gs: 0.5413105413105412\n",
      "gs: 0.5521186440677965\n",
      "gs: 0.5627450980392157\n",
      "gs: 0.5521186440677965\n",
      "gs: 0.5191304347826087\n",
      "gs: 0.496165191740413\n",
      "gs: 0.4843750000000001\n",
      "gs: 0.44770642201834865\n",
      "gs: 0.45771604938271615\n",
      "gs: 0.46527677929547095\n",
      "gs: 0.47492063492063485\n",
      "gs: 0.47764423076923085\n",
      "gs: 0.47995642701525054\n",
      "gs: 0.47983324648254305\n",
      "gs: 0.5344784543579724\n",
      "gs: 0.5326593923833975\n",
      "gs: 0.5279166666666666\n",
      "gs: 0.5371610845295055\n",
      "gs: 0.5415719420965704\n",
      "gs: 0.5596496106785318\n",
      "gs: 0.5100961538461538\n",
      "gs: 0.5146031746031746\n",
      "gs: 0.5318042813455657\n",
      "gs: 0.5359090909090909\n",
      "gs: 0.5697478991596638\n",
      "gs: 0.5627450980392157\n",
      "gs: 0.496165191740413\n",
      "gs: 0.4723723723723723\n",
      "gs: 0.44770642201834865\n",
      "gs: 0.45771604938271615\n",
      "gs: 0.47088948787062\n",
      "gs: 0.47764423076923085\n",
      "gs: 0.479\n",
      "gs: 0.5060135404221426\n",
      "gs: 0.5559139784946238\n",
      "gs: 0.4911666666666667\n",
      "gs: 0.5399399399399399\n",
      "gs: 0.5553623188405797\n",
      "gs: 0.5590517241379311\n",
      "gs: 0.5662429378531072\n",
      "gs: 0.5697478991596638\n",
      "sasgf\n",
      "gs: 0.49496802891298297\n",
      "gs: 0.4923091566723776\n",
      "gs: 0.48578431372549025\n",
      "gs: 0.5008333333333334\n",
      "gs: 0.523298245614035\n",
      "gs: 0.529787784679089\n",
      "gs: 0.5306632837984777\n",
      "gs: 0.4734074074074074\n",
      "gs: 0.4732280473244329\n",
      "gs: 0.44772727272727275\n",
      "gs: 0.5331524842946888\n",
      "gs: 0.5364330637915543\n",
      "gs: 0.5262345679012345\n",
      "gs: 0.5188629413400057\n",
      "gs: 0.503125\n",
      "gs: 0.546141975308642\n",
      "gs: 0.5460317460317461\n",
      "gs: 0.5300463720676487\n",
      "gs: 0.5222222222222224\n",
      "gs: 0.5057109557109557\n",
      "gs: 0.5332859848484849\n",
      "gs: 0.5184912280701754\n",
      "gs: 0.502935537982267\n",
      "gs: 0.5003003003003004\n",
      "gs: 0.5064053940160135\n",
      "gs: 0.5012931034482759\n",
      "gs: 0.5247175141242937\n",
      "gs: 0.5361344537815126\n",
      "gs: 0.5361344537815126\n",
      "gs: 0.5131054131054131\n",
      "gs: 0.5012931034482759\n",
      "gs: 0.4892753623188406\n",
      "gs: 0.47704678362573094\n",
      "gs: 0.4646017699115045\n",
      "gs: 0.4758928571428571\n",
      "gs: 0.4934945788156797\n",
      "gs: 0.49614197530864185\n",
      "gs: 0.4961328976034859\n",
      "gs: 0.4902356902356902\n",
      "gs: 0.5271921494143716\n",
      "gs: 0.5234228671400638\n",
      "gs: 0.5161682615629984\n",
      "gs: 0.5220926843485865\n",
      "gs: 0.5248650184711565\n",
      "gs: 0.5396825396825397\n",
      "gs: 0.4655660377358491\n",
      "gs: 0.4721183800623054\n",
      "gs: 0.503125\n",
      "gs: 0.5361344537815126\n",
      "gs: 0.5131054131054131\n",
      "gs: 0.4892753623188406\n",
      "gs: 0.4646017699115045\n",
      "gs: 0.4895454545454546\n",
      "gs: 0.4934945788156797\n",
      "gs: 0.4985849056603774\n",
      "gs: 0.487708719851577\n",
      "gs: 0.5027950310559006\n",
      "gs: 0.5416759362254356\n",
      "gs: 0.4308580858085808\n",
      "gs: 0.49714714714714725\n",
      "gs: 0.5204347826086957\n",
      "gs: 0.5260057471264368\n",
      "gs: 0.536864406779661\n",
      "gs: 0.542156862745098\n",
      "sasgf\n",
      "gs: 0.59593837535014\n",
      "gs: 0.5862994350282486\n",
      "gs: 0.581969696969697\n",
      "gs: 0.5713141025641026\n",
      "gs: 0.5637445269369883\n",
      "gs: 0.5607434427653292\n",
      "gs: 0.5434343434343434\n",
      "gs: 0.5193333333333333\n",
      "gs: 0.5174171842650104\n",
      "gs: 0.5324099407204742\n",
      "gs: 0.5321459135643989\n",
      "gs: 0.4863029269678887\n",
      "gs: 0.5178571428571429\n",
      "gs: 0.510270659668622\n",
      "gs: 0.5980936819172112\n",
      "gs: 0.5931891025641025\n",
      "gs: 0.5892063492063492\n",
      "gs: 0.5571865443425076\n",
      "gs: 0.5817817817817819\n",
      "gs: 0.5979938271604938\n",
      "gs: 0.5740837241618898\n",
      "gs: 0.5489048187972919\n",
      "gs: 0.5712099877968648\n",
      "gs: 0.5499446290143966\n",
      "gs: 0.5486546375435265\n",
      "gs: 0.4805967841682127\n",
      "gs: 0.49000000000000005\n",
      "gs: 0.49077402348430393\n",
      "gs: 0.4800925925925925\n",
      "gs: 0.5248511904761906\n",
      "gs: 0.5355457227138642\n",
      "gs: 0.5563768115942029\n",
      "gs: 0.5764957264957264\n",
      "gs: 0.5862994350282486\n",
      "gs: 0.59593837535014\n",
      "gs: 0.59593837535014\n",
      "gs: 0.5862994350282486\n",
      "gs: 0.5460526315789473\n",
      "gs: 0.4564465408805031\n",
      "gs: 0.4442857142857143\n",
      "gs: 0.4318910256410257\n",
      "gs: 0.45201525054466235\n",
      "gs: 0.4708994708994709\n",
      "gs: 0.4749381570810143\n",
      "gs: 0.4803819444444445\n",
      "gs: 0.4829451345755694\n",
      "gs: 0.4792195239821191\n",
      "gs: 0.5190696717093752\n",
      "gs: 0.5163308913308913\n",
      "gs: 0.5132487667371388\n",
      "gs: 0.5098285486443381\n",
      "gs: 0.5080476190476191\n",
      "gs: 0.5580068537556728\n",
      "gs: 0.5210784313725491\n",
      "gs: 0.5571865443425076\n",
      "gs: 0.6014005602240897\n",
      "gs: 0.5764957264957264\n",
      "gs: 0.46838006230529605\n",
      "gs: 0.4564465408805031\n",
      "gs: 0.4318910256410257\n",
      "gs: 0.45201525054466235\n",
      "gs: 0.4708994708994709\n",
      "gs: 0.4821775925224201\n",
      "gs: 0.5277451888442934\n",
      "gs: 0.5842304625199362\n",
      "gs: 0.5619696969696969\n",
      "gs: 0.5889367816091954\n",
      "gs: 0.5973163841807909\n",
      "gs: 0.6014005602240897\n",
      "sasgf\n",
      "gs: 0.5925770308123249\n",
      "gs: 0.5647988505747127\n",
      "gs: 0.5355457227138642\n",
      "gs: 0.5046969696969696\n",
      "gs: 0.5080144679230916\n",
      "gs: 0.5332096474953618\n",
      "gs: 0.5114975450081833\n",
      "gs: 0.5013739545997611\n",
      "gs: 0.479973474801061\n",
      "gs: 0.48867990247300586\n",
      "gs: 0.4968067226890757\n",
      "gs: 0.5019179894179894\n",
      "gs: 0.5108151476251604\n",
      "gs: 0.530534115467737\n",
      "gs: 0.5800876144962167\n",
      "gs: 0.5780823786142936\n",
      "gs: 0.5931917211328975\n",
      "gs: 0.592594707786027\n",
      "gs: 0.5912293314162475\n",
      "gs: 0.5739739739739739\n",
      "gs: 0.5651785714285714\n",
      "gs: 0.5530973451327434\n",
      "gs: 0.5895114942528736\n",
      "gs: 0.5956060606060606\n",
      "gs: 0.5967592592592594\n",
      "gs: 0.5964533908459143\n",
      "gs: 0.5868681605002605\n",
      "gs: 0.5487037037037037\n",
      "gs: 0.5375382901698692\n",
      "gs: 0.5164703513540724\n",
      "gs: 0.5040436456996149\n",
      "gs: 0.5220486111111111\n",
      "gs: 0.5388621794871795\n",
      "gs: 0.5242138364779875\n",
      "gs: 0.5046969696969696\n",
      "gs: 0.5355457227138642\n",
      "gs: 0.5454678362573099\n",
      "gs: 0.5552173913043479\n",
      "gs: 0.5742165242165242\n",
      "gs: 0.5834745762711864\n",
      "gs: 0.5925770308123249\n",
      "gs: 0.5925770308123249\n",
      "gs: 0.5834745762711864\n",
      "gs: 0.5647988505747127\n",
      "gs: 0.5254464285714285\n",
      "gs: 0.4940366972477064\n",
      "gs: 0.4257281553398058\n",
      "gs: 0.38849999999999996\n",
      "gs: 0.4020683020683021\n",
      "gs: 0.4431578947368421\n",
      "gs: 0.45085924713584286\n",
      "gs: 0.45774591796097175\n",
      "gs: 0.4823863636363635\n",
      "gs: 0.48571926158133055\n",
      "gs: 0.49109243697479\n",
      "gs: 0.5248666062619551\n",
      "gs: 0.5311904761904762\n",
      "gs: 0.535104895104895\n",
      "gs: 0.549069185884968\n",
      "gs: 0.5644345238095239\n",
      "gs: 0.47249190938511326\n",
      "gs: 0.5456845238095238\n",
      "gs: 0.5530973451327434\n",
      "gs: 0.5742165242165242\n",
      "gs: 0.4376602564102564\n",
      "gs: 0.4257281553398058\n",
      "gs: 0.4135620915032679\n",
      "gs: 0.40115511551155114\n",
      "gs: 0.38849999999999996\n",
      "gs: 0.4431578947368421\n",
      "gs: 0.45774591796097175\n",
      "gs: 0.4785791953606379\n",
      "gs: 0.4931878306878306\n",
      "gs: 0.5471134891579836\n",
      "gs: 0.5632541478129713\n",
      "gs: 0.5304545454545454\n",
      "gs: 0.567536231884058\n",
      "gs: 0.5882768361581919\n",
      "gs: 0.5949579831932773\n",
      "sasgf\n",
      "gs: 0.55406162464986\n",
      "gs: 0.5430790960451977\n",
      "gs: 0.5385416666666667\n",
      "gs: 0.521969696969697\n",
      "gs: 0.5124548234639978\n",
      "gs: 0.5023148148148148\n",
      "gs: 0.49937106918238994\n",
      "gs: 0.5100160256410258\n",
      "gs: 0.5174917491749175\n",
      "gs: 0.5215\n",
      "gs: 0.5130291936543697\n",
      "gs: 0.5190778214034029\n",
      "gs: 0.503299685227396\n",
      "gs: 0.5611666666666667\n",
      "gs: 0.5593277748827514\n",
      "gs: 0.5566993464052288\n",
      "gs: 0.5517460317460318\n",
      "gs: 0.5192660550458715\n",
      "gs: 0.5610644257703081\n",
      "gs: 0.5436666666666667\n",
      "gs: 0.5400436442989635\n",
      "gs: 0.5263257575757576\n",
      "gs: 0.52890625\n",
      "gs: 0.5450396825396825\n",
      "gs: 0.5397248345524208\n",
      "gs: 0.5239583333333333\n",
      "gs: 0.5221721721721722\n",
      "gs: 0.5147321428571429\n",
      "gs: 0.5077960387694901\n",
      "gs: 0.5301449275362319\n",
      "gs: 0.5406609195402299\n",
      "gs: 0.5430790960451977\n",
      "gs: 0.55406162464986\n",
      "gs: 0.55406162464986\n",
      "gs: 0.5430790960451977\n",
      "gs: 0.5089855072463767\n",
      "gs: 0.4730654761904762\n",
      "gs: 0.4606606606606607\n",
      "gs: 0.47090909090909094\n",
      "gs: 0.47834306366416457\n",
      "gs: 0.48727534148094903\n",
      "gs: 0.4895777178796047\n",
      "gs: 0.49110576923076926\n",
      "gs: 0.5091198725607328\n",
      "gs: 0.5028672476948339\n",
      "gs: 0.4866597004528039\n",
      "gs: 0.5132964289590796\n",
      "gs: 0.5191111111111111\n",
      "gs: 0.5563888888888888\n",
      "gs: 0.4902912621359222\n",
      "gs: 0.5146604938271605\n",
      "gs: 0.5192660550458715\n",
      "gs: 0.5572033898305084\n",
      "gs: 0.5610644257703081\n",
      "gs: 0.55406162464986\n",
      "gs: 0.4972222222222222\n",
      "gs: 0.48525073746312686\n",
      "gs: 0.4730654761904762\n",
      "gs: 0.4606606606606607\n",
      "gs: 0.47090909090909094\n",
      "gs: 0.47834306366416457\n",
      "gs: 0.4895777178796047\n",
      "gs: 0.4906529602132115\n",
      "gs: 0.5210877192982456\n",
      "gs: 0.5529616070590444\n",
      "gs: 0.4953525641025642\n",
      "gs: 0.5146604938271605\n",
      "gs: 0.5492816091954023\n",
      "gs: 0.5572033898305084\n",
      "gs: 0.5610644257703081\n",
      "sasgf\n",
      "gs: 0.5103641456582633\n",
      "gs: 0.4981638418079095\n",
      "gs: 0.4936011904761905\n",
      "gs: 0.48948948948948956\n",
      "gs: 0.49787878787878787\n",
      "gs: 0.508641975308642\n",
      "gs: 0.5119098969566259\n",
      "gs: 0.5175805675805676\n",
      "gs: 0.5183101748095025\n",
      "gs: 0.43299999999999994\n",
      "gs: 0.45802808302808296\n",
      "gs: 0.469375\n",
      "gs: 0.5201666666666667\n",
      "gs: 0.5219498910675382\n",
      "gs: 0.5219551282051282\n",
      "gs: 0.5211111111111111\n",
      "gs: 0.5171699017493411\n",
      "gs: 0.493943943943944\n",
      "gs: 0.4822916666666666\n",
      "gs: 0.5176470588235293\n",
      "gs: 0.4713636363636363\n",
      "gs: 0.48350013900472616\n",
      "gs: 0.470323084940073\n",
      "gs: 0.43399999999999994\n",
      "gs: 0.5115869484612532\n",
      "gs: 0.5098148148148148\n",
      "gs: 0.5157986111111111\n",
      "gs: 0.5031818181818182\n",
      "gs: 0.5056547619047619\n",
      "gs: 0.5106321839080461\n",
      "gs: 0.4981638418079095\n",
      "gs: 0.5103641456582633\n",
      "gs: 0.5103641456582633\n",
      "gs: 0.4981638418079095\n",
      "gs: 0.48575498575498577\n",
      "gs: 0.49454022988505747\n",
      "gs: 0.4994152046783626\n",
      "gs: 0.49907290349768224\n",
      "gs: 0.4977678571428572\n",
      "gs: 0.49579579579579586\n",
      "gs: 0.4873456790123456\n",
      "gs: 0.49420289855072463\n",
      "gs: 0.48317023075993715\n",
      "gs: 0.4748867990247301\n",
      "gs: 0.4637248609328199\n",
      "gs: 0.49762759170653903\n",
      "gs: 0.5207351290684624\n",
      "gs: 0.44222222222222224\n",
      "gs: 0.44827044025157226\n",
      "gs: 0.4822916666666666\n",
      "gs: 0.5176470588235293\n",
      "gs: 0.4981638418079095\n",
      "gs: 0.48575498575498577\n",
      "gs: 0.4994152046783626\n",
      "gs: 0.49907290349768224\n",
      "gs: 0.4933333333333333\n",
      "gs: 0.5005728314238953\n",
      "gs: 0.5084795321637428\n",
      "gs: 0.44222222222222224\n",
      "gs: 0.4768768768768771\n",
      "gs: 0.5030172413793105\n",
      "gs: 0.5176470588235293\n",
      "sasgf\n",
      "gs: 0.5477591036414566\n",
      "gs: 0.5365819209039547\n",
      "gs: 0.5361989043404972\n",
      "gs: 0.5401901901901902\n",
      "gs: 0.5333333333333333\n",
      "gs: 0.5220634920634921\n",
      "gs: 0.5290595849990481\n",
      "gs: 0.5374458874458874\n",
      "gs: 0.5327719298245613\n",
      "gs: 0.5331560283687944\n",
      "gs: 0.5277351520318272\n",
      "gs: 0.5423839726165308\n",
      "gs: 0.5397916666666667\n",
      "gs: 0.5516852195423624\n",
      "gs: 0.5572440087145969\n",
      "gs: 0.5572916666666667\n",
      "gs: 0.552821795941062\n",
      "gs: 0.5402444163506109\n",
      "gs: 0.5293859649122806\n",
      "gs: 0.5317005384749001\n",
      "gs: 0.5417508417508416\n",
      "gs: 0.5455627705627706\n",
      "gs: 0.5272577996715928\n",
      "gs: 0.5025421796625626\n",
      "gs: 0.5450037078235076\n",
      "gs: 0.5361894810170673\n",
      "gs: 0.5357626443647949\n",
      "gs: 0.5154647435897436\n",
      "gs: 0.5120370370370371\n",
      "gs: 0.49604604604604613\n",
      "gs: 0.4777286135693215\n",
      "gs: 0.5018840579710145\n",
      "gs: 0.5136494252873564\n",
      "gs: 0.5365819209039547\n",
      "gs: 0.5252136752136752\n",
      "gs: 0.4899122807017543\n",
      "gs: 0.4777286135693215\n",
      "gs: 0.46532738095238096\n",
      "gs: 0.47637637637637636\n",
      "gs: 0.4843939393939394\n",
      "gs: 0.4901862663330553\n",
      "gs: 0.4942901234567901\n",
      "gs: 0.4998397435897436\n",
      "gs: 0.4969949626541602\n",
      "gs: 0.49046072974644395\n",
      "gs: 0.510135841170324\n",
      "gs: 0.4874643874643876\n",
      "gs: 0.4788582060321191\n",
      "gs: 0.4920116399136394\n",
      "gs: 0.548981507556557\n",
      "gs: 0.4909657320872276\n",
      "gs: 0.5188988095238095\n",
      "gs: 0.5241887905604719\n",
      "gs: 0.5477591036414566\n",
      "gs: 0.5018840579710145\n",
      "gs: 0.46532738095238096\n",
      "gs: 0.4942901234567901\n",
      "gs: 0.4988095238095238\n",
      "gs: 0.4876438069624982\n",
      "gs: 0.4998015873015874\n",
      "gs: 0.5498335291813553\n",
      "gs: 0.4909657320872276\n",
      "gs: 0.5188988095238095\n",
      "gs: 0.5444444444444444\n",
      "gs: 0.55406162464986\n",
      "sasgf\n",
      "gs: 0.5693502824858756\n",
      "gs: 0.5592592592592592\n",
      "gs: 0.5385507246376812\n",
      "gs: 0.5279239766081871\n",
      "gs: 0.49489489489489497\n",
      "gs: 0.5398412698412698\n",
      "gs: 0.5264705882352941\n",
      "gs: 0.5166145561924614\n",
      "gs: 0.5063333333333334\n",
      "gs: 0.49564694564694556\n",
      "gs: 0.5004638218923934\n",
      "gs: 0.504250709696698\n",
      "gs: 0.5332910474809713\n",
      "gs: 0.5245738636363635\n",
      "gs: 0.5269766631835597\n",
      "gs: 0.53\n",
      "gs: 0.5476525821596244\n",
      "gs: 0.5679396662387676\n",
      "gs: 0.5722754217966256\n",
      "gs: 0.5721132897603486\n",
      "gs: 0.5759090043784504\n",
      "gs: 0.5792467948717949\n",
      "gs: 0.5820634920634921\n",
      "gs: 0.5864197530864197\n",
      "gs: 0.5748511904761905\n",
      "gs: 0.5656342182890856\n",
      "gs: 0.5831932773109244\n",
      "gs: 0.5548162954502189\n",
      "gs: 0.5596949891067539\n",
      "gs: 0.566484280006948\n",
      "gs: 0.5486111111111112\n",
      "gs: 0.5305403556771546\n",
      "gs: 0.5748568478362903\n",
      "gs: 0.5220164609053498\n",
      "gs: 0.5393465909090909\n",
      "gs: 0.49003267973856207\n",
      "gs: 0.49282735613010836\n",
      "gs: 0.5061011904761904\n",
      "gs: 0.5385507246376812\n",
      "gs: 0.5489942528735633\n",
      "gs: 0.5592592592592592\n",
      "gs: 0.5693502824858756\n",
      "gs: 0.5792717086834733\n",
      "gs: 0.5592592592592592\n",
      "gs: 0.5061011904761904\n",
      "gs: 0.4718654434250764\n",
      "gs: 0.4231746031746032\n",
      "gs: 0.4329326923076923\n",
      "gs: 0.44073862554730636\n",
      "gs: 0.45164147993746745\n",
      "gs: 0.4576238576238576\n",
      "gs: 0.4598087554161064\n",
      "gs: 0.45894736842105255\n",
      "gs: 0.45754228041462086\n",
      "gs: 0.49677812608847094\n",
      "gs: 0.4915126050420169\n",
      "gs: 0.48042362002567396\n",
      "gs: 0.49577777777777776\n",
      "gs: 0.522765246449457\n",
      "gs: 0.5706018518518519\n",
      "gs: 0.5350793650793652\n",
      "gs: 0.5656342182890856\n",
      "gs: 0.5693502824858756\n",
      "gs: 0.48348484848484846\n",
      "gs: 0.44797507788162005\n",
      "gs: 0.4231746031746032\n",
      "gs: 0.44689542483660133\n",
      "gs: 0.45516666666666666\n",
      "gs: 0.45972222222222214\n",
      "gs: 0.5112644415917843\n",
      "gs: 0.5763431013431014\n",
      "gs: 0.5266990291262136\n",
      "gs: 0.5470679012345679\n",
      "gs: 0.5691520467836257\n",
      "sasgf\n",
      "gs: 0.5257703081232493\n",
      "gs: 0.5139830508474577\n",
      "gs: 0.48537023723939615\n",
      "gs: 0.5176946506758043\n",
      "gs: 0.5264981761334029\n",
      "gs: 0.5243333333333333\n",
      "gs: 0.5269161810847154\n",
      "gs: 0.530561403508772\n",
      "gs: 0.5328407224958949\n",
      "gs: 0.529971590909091\n",
      "gs: 0.5297457331940091\n",
      "gs: 0.5348684210526315\n",
      "gs: 0.5300925925925926\n",
      "gs: 0.5225008707767329\n",
      "gs: 0.5219551282051282\n",
      "gs: 0.5279649595687331\n",
      "gs: 0.5305775221663073\n",
      "gs: 0.5328703703703703\n",
      "gs: 0.5347789824854046\n",
      "gs: 0.5362121212121213\n",
      "gs: 0.5359460598398651\n",
      "gs: 0.5179597701149425\n",
      "gs: 0.5336134453781514\n",
      "gs: 0.4505411255411256\n",
      "gs: 0.47964912280701755\n",
      "gs: 0.4364824534321293\n",
      "gs: 0.43032296650717705\n",
      "gs: 0.5255331088664421\n",
      "gs: 0.4823728404011115\n",
      "gs: 0.4976503385105536\n",
      "gs: 0.49396825396825406\n",
      "gs: 0.5095454545454545\n",
      "gs: 0.5156432748538011\n",
      "gs: 0.5144927536231884\n",
      "gs: 0.5139830508474577\n",
      "gs: 0.5257703081232493\n",
      "gs: 0.5257703081232493\n",
      "gs: 0.5139830508474577\n",
      "gs: 0.501994301994302\n",
      "gs: 0.4773913043478261\n",
      "gs: 0.49372102823430253\n",
      "gs: 0.4998484848484849\n",
      "gs: 0.49958298582151794\n",
      "gs: 0.4986111111111111\n",
      "gs: 0.49269841269841275\n",
      "gs: 0.4869788692175899\n",
      "gs: 0.4764999999999999\n",
      "gs: 0.46418646346929626\n",
      "gs: 0.5052099063726971\n",
      "gs: 0.5017145135566188\n",
      "gs: 0.4657239819004525\n",
      "gs: 0.4601755374073032\n",
      "gs: 0.45702632554690403\n",
      "gs: 0.5357106102706596\n",
      "gs: 0.507748538011696\n",
      "gs: 0.5179597701149425\n",
      "gs: 0.5139830508474577\n",
      "gs: 0.48979885057471273\n",
      "gs: 0.4773913043478261\n",
      "gs: 0.49919919919919925\n",
      "gs: 0.4986111111111111\n",
      "gs: 0.4869788692175899\n",
      "gs: 0.45972222222222225\n",
      "gs: 0.44562870080111455\n",
      "gs: 0.534035409035409\n",
      "gs: 0.46305031446540884\n",
      "gs: 0.5229344729344728\n",
      "gs: 0.5278248587570622\n",
      "gs: 0.5326330532212886\n",
      "sasgf\n",
      "gs: 0.5789915966386555\n",
      "gs: 0.5687853107344633\n",
      "gs: 0.5478448275862069\n",
      "gs: 0.5660493827160494\n",
      "gs: 0.5652964959568734\n",
      "gs: 0.5473856209150327\n",
      "gs: 0.551155115511551\n",
      "gs: 0.5245670995670996\n",
      "gs: 0.5216842105263159\n",
      "gs: 0.5475291375291375\n",
      "gs: 0.5387407407407407\n",
      "gs: 0.5264382010908716\n",
      "gs: 0.5856209150326797\n",
      "gs: 0.5861222158766418\n",
      "gs: 0.5864779874213837\n",
      "gs: 0.5864197530864197\n",
      "gs: 0.5706845238095237\n",
      "gs: 0.56047197640118\n",
      "gs: 0.5841736694677871\n",
      "gs: 0.5746031746031746\n",
      "gs: 0.5786057692307692\n",
      "gs: 0.5815343613173424\n",
      "gs: 0.5694353518821604\n",
      "gs: 0.5628105590062112\n",
      "gs: 0.541108630952381\n",
      "gs: 0.5509413778348309\n",
      "gs: 0.5465608465608466\n",
      "gs: 0.5133663366336633\n",
      "gs: 0.5192901234567903\n",
      "gs: 0.5236363636363637\n",
      "gs: 0.5150442477876106\n",
      "gs: 0.5371014492753623\n",
      "gs: 0.5478448275862069\n",
      "gs: 0.5584045584045584\n",
      "gs: 0.5789915966386555\n",
      "gs: 0.5789915966386555\n",
      "gs: 0.5687853107344633\n",
      "gs: 0.5261695906432748\n",
      "gs: 0.5150442477876106\n",
      "gs: 0.49219219219219223\n",
      "gs: 0.4439252336448599\n",
      "gs: 0.4546271338724169\n",
      "gs: 0.46317460317460324\n",
      "gs: 0.46995192307692313\n",
      "gs: 0.4752522368170569\n",
      "gs: 0.48228243877019283\n",
      "gs: 0.5270186335403727\n",
      "gs: 0.5274374773468649\n",
      "gs: 0.513885323063757\n",
      "gs: 0.5360740740740741\n",
      "gs: 0.5426405707312494\n",
      "gs: 0.5725818452380952\n",
      "gs: 0.5425076452599388\n",
      "gs: 0.5516516516516516\n",
      "gs: 0.5561011904761904\n",
      "gs: 0.5687853107344633\n",
      "gs: 0.5150442477876106\n",
      "gs: 0.4804545454545454\n",
      "gs: 0.4563271604938271\n",
      "gs: 0.4439252336448599\n",
      "gs: 0.46317460317460324\n",
      "gs: 0.46995192307692313\n",
      "gs: 0.4793028322440087\n",
      "gs: 0.4843333333333334\n",
      "gs: 0.4959259259259258\n",
      "gs: 0.573\n",
      "gs: 0.5129449838187702\n",
      "gs: 0.5425076452599388\n",
      "gs: 0.5851540616246498\n",
      "The f1 score of your AdaBoost implementation on the IRIS dataset is 0.933\n",
      "The f1 score of the sklearn AdaBoost implementation on the IRIS dataset is 0.933\n"
     ]
    }
   ],
   "source": [
    "my_adaboost = AdaBoostTreeClassifier().fit(X_train, y_train)\n",
    "sk_adaboost = AdaBoostClassifier().fit(X_train, y_train)\n",
    "\n",
    "my_y_pred = my_adaboost.predict(X_test)\n",
    "sk_y_pred = sk_adaboost.predict(X_test)\n",
    "\n",
    "my_f1 = f1_score(y_test, my_y_pred, average='macro')\n",
    "sk_f1 = f1_score(y_test, sk_y_pred, average='macro')\n",
    "\n",
    "print('The f1 score of your AdaBoost implementation on the IRIS dataset is {:.3f}'.format(my_f1))\n",
    "print('The f1 score of the sklearn AdaBoost implementation on the IRIS dataset is {:.3f}'.format(sk_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f14028b-e090-47cf-870a-2b11e3645b8e",
   "metadata": {},
   "source": [
    "You should see an f1 score of **0.933** using both your implementation as well as the one from scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6649193e-eb67-4fac-88d0-b7450aff1a17",
   "metadata": {},
   "source": [
    "### 2.3 Questions about AdaBoost (6 Points)\n",
    "\n",
    "Assume you use your implementation of `AdaBoostTreeClassifier` to train a binary classifier of the dataset shown in 1.1 b).\n",
    "\n",
    "**2.3 a) Question (2 Points):** Will the binary classifier be able to achieve a training error (not test error!) of 0? Explain your answer! (Your explanation is more important than a simple Yes/No answer)\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a596b-add5-4acf-aa83-3755f6c4583c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "678d899b-c099-4f05-b769-2d3d5c66fd59",
   "metadata": {},
   "source": [
    "**2.3 b) True/False Questions (4 Points):** In the table below are 4 statement that are either *True* or *False*. Complete the table to specify whether a statement is *True* or *False*, and provide a brief explanation for your answer (Your explanation is more important than a simple True/False answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b080cf-91f9-477c-95cf-3827fbf96294",
   "metadata": {},
   "source": [
    "This is a markdown cell. Please fill in your answers for (1)~(4).\n",
    "\n",
    "| No. | Statement                                                                                               \t| True or False?   \t| Brief Explanation |\n",
    "|-----|------------------------------------------------------------------------------------------------------------|--------------| ------- |\n",
    "| (1)  | AdaBoost usually performs better than Random Forests when the dataset contains mislabeled data points | ??? | ??? |\n",
    "| (2)  | The error rate $\\epsilon_m$ of the Adaboost classifier always decreases from one iteration to the next. | ??? | ??? |\n",
    "| (3)  | Assume an error rate of $\\epsilon_m \\leq 0.2$ in iteration $m$. This means that up to 20% of the data samples have been misclassified | ??? | ??? |\n",
    "| (4)  | If after running AdaBoost the last Weak Learner does not misclassify and training samples, additional iterations could still help reduce errors on unseen data  | ??? | ??? |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b77b56-8212-47a9-85e4-dae447301bbe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2ac1ea-25f6-4d20-84d3-b3cec2f15d23",
   "metadata": {},
   "source": [
    "## 3 Evaluating Tree-Based Models\n",
    "\n",
    "In this last part, we look into evaluation different tree-based models using k-fold cross validationa. K-fold cross-validation is a technique used in machine learning to assess the performance and generalization ability of a model. It involves dividing the dataset into K subsets (or \"folds\") of equal size. The model is trained on K-1 of these folds and tested on the remaining one. This process is repeated K times, with each fold used as the test set exactly once. The final performance metric is computed by averaging the results from each iteration. K-fold cross-validation helps ensure that the model's performance is consistent across different subsets of the data, reducing the risk of overfitting or underfitting.\n",
    "\n",
    "### Prepare Dataset\n",
    "\n",
    "#### Load Dataset from File\n",
    "\n",
    "We use a [WHO Life Expectancy](https://www.kaggle.com/kumarajarshi/life-expectancy-who) dataset for this task. Note that we cleaned the dataset for you (i.e., there are no dirty records in there)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "643c9a75-171d-40e5-b060-eb6da2a5e5cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:36:56.857206200Z",
     "start_time": "2023-10-15T13:36:56.747174100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Year      Status  Adult Mortality  infant deaths  Alcohol  \\\n0  2015  Developing            263.0             62     0.01   \n1  2014  Developing            271.0             64     0.01   \n2  2013  Developing            268.0             66     0.01   \n3  2012  Developing            272.0             69     0.01   \n4  2011  Developing            275.0             71     0.01   \n\n   percentage expenditure  Hepatitis B  Measles   BMI  under-five deaths  ...  \\\n0               71.279624         65.0     1154  19.1                 83  ...   \n1               73.523582         62.0      492  18.6                 86  ...   \n2               73.219243         64.0      430  18.1                 89  ...   \n3               78.184215         67.0     2787  17.6                 93  ...   \n4                7.097109         68.0     3013  17.2                 97  ...   \n\n   Total expenditure  Diphtheria  HIV/AIDS         GDP  Population  \\\n0               8.16        65.0       0.1  584.259210  33736494.0   \n1               8.18        62.0       0.1  612.696514    327582.0   \n2               8.13        64.0       0.1  631.744976  31731688.0   \n3               8.52        67.0       0.1  669.959000   3696958.0   \n4               7.87        68.0       0.1   63.537231   2978599.0   \n\n   thinness  1-19 years  thinness 5-9 years  Income composition of resources  \\\n0                  17.2                17.3                            0.479   \n1                  17.5                17.5                            0.476   \n2                  17.7                17.7                            0.470   \n3                  17.9                18.0                            0.463   \n4                  18.2                18.2                            0.454   \n\n   Schooling  Life expectancy  \n0       10.1             65.0  \n1       10.0             59.9  \n2        9.9             59.9  \n3        9.8             59.5  \n4        9.5             59.2  \n\n[5 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Year</th>\n      <th>Status</th>\n      <th>Adult Mortality</th>\n      <th>infant deaths</th>\n      <th>Alcohol</th>\n      <th>percentage expenditure</th>\n      <th>Hepatitis B</th>\n      <th>Measles</th>\n      <th>BMI</th>\n      <th>under-five deaths</th>\n      <th>...</th>\n      <th>Total expenditure</th>\n      <th>Diphtheria</th>\n      <th>HIV/AIDS</th>\n      <th>GDP</th>\n      <th>Population</th>\n      <th>thinness  1-19 years</th>\n      <th>thinness 5-9 years</th>\n      <th>Income composition of resources</th>\n      <th>Schooling</th>\n      <th>Life expectancy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2015</td>\n      <td>Developing</td>\n      <td>263.0</td>\n      <td>62</td>\n      <td>0.01</td>\n      <td>71.279624</td>\n      <td>65.0</td>\n      <td>1154</td>\n      <td>19.1</td>\n      <td>83</td>\n      <td>...</td>\n      <td>8.16</td>\n      <td>65.0</td>\n      <td>0.1</td>\n      <td>584.259210</td>\n      <td>33736494.0</td>\n      <td>17.2</td>\n      <td>17.3</td>\n      <td>0.479</td>\n      <td>10.1</td>\n      <td>65.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2014</td>\n      <td>Developing</td>\n      <td>271.0</td>\n      <td>64</td>\n      <td>0.01</td>\n      <td>73.523582</td>\n      <td>62.0</td>\n      <td>492</td>\n      <td>18.6</td>\n      <td>86</td>\n      <td>...</td>\n      <td>8.18</td>\n      <td>62.0</td>\n      <td>0.1</td>\n      <td>612.696514</td>\n      <td>327582.0</td>\n      <td>17.5</td>\n      <td>17.5</td>\n      <td>0.476</td>\n      <td>10.0</td>\n      <td>59.9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2013</td>\n      <td>Developing</td>\n      <td>268.0</td>\n      <td>66</td>\n      <td>0.01</td>\n      <td>73.219243</td>\n      <td>64.0</td>\n      <td>430</td>\n      <td>18.1</td>\n      <td>89</td>\n      <td>...</td>\n      <td>8.13</td>\n      <td>64.0</td>\n      <td>0.1</td>\n      <td>631.744976</td>\n      <td>31731688.0</td>\n      <td>17.7</td>\n      <td>17.7</td>\n      <td>0.470</td>\n      <td>9.9</td>\n      <td>59.9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2012</td>\n      <td>Developing</td>\n      <td>272.0</td>\n      <td>69</td>\n      <td>0.01</td>\n      <td>78.184215</td>\n      <td>67.0</td>\n      <td>2787</td>\n      <td>17.6</td>\n      <td>93</td>\n      <td>...</td>\n      <td>8.52</td>\n      <td>67.0</td>\n      <td>0.1</td>\n      <td>669.959000</td>\n      <td>3696958.0</td>\n      <td>17.9</td>\n      <td>18.0</td>\n      <td>0.463</td>\n      <td>9.8</td>\n      <td>59.5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2011</td>\n      <td>Developing</td>\n      <td>275.0</td>\n      <td>71</td>\n      <td>0.01</td>\n      <td>7.097109</td>\n      <td>68.0</td>\n      <td>3013</td>\n      <td>17.2</td>\n      <td>97</td>\n      <td>...</td>\n      <td>7.87</td>\n      <td>68.0</td>\n      <td>0.1</td>\n      <td>63.537231</td>\n      <td>2978599.0</td>\n      <td>18.2</td>\n      <td>18.2</td>\n      <td>0.454</td>\n      <td>9.5</td>\n      <td>59.2</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows  21 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/a3-life-expectancy-cleaned.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acef995-4e5c-43d4-91d7-2d3ce24a73b0",
   "metadata": {},
   "source": [
    "#### Separate Features & Target\n",
    "\n",
    "For your convenience, we split the dataframe into two, one containing the input features, the other containing the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6b13268-9ac1-4990-b96e-89b3c4589487",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:36:58.224688800Z",
     "start_time": "2023-10-15T13:36:58.146337500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 1649 samples with 20 features\n"
     ]
    }
   ],
   "source": [
    "df_X = df.iloc[:,0:-1]\n",
    "df_y = df.iloc[:,-1]\n",
    "\n",
    "num_samples, num_features = df_X.shape\n",
    "\n",
    "print('The dataset contains {} samples with {} features'.format(num_samples, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b40b6b-0113-4838-8f31-06a30d2ee54a",
   "metadata": {},
   "source": [
    "### 3.1 Data Preprocessing (2 Points)\n",
    "\n",
    "As usual, the first step is data preprocessing (informed by an EDA). As mentioned above, there's not much to do as this dataset does not contain any \"dirty\" records, particularly, there are no NA values in any of the columns/features. As such, there should be no need to remove any samples.\n",
    "\n",
    "**Perform and data preprocessing/transformation steps you deem appropriate!** As it might affect your decision, the data will be used to train different tree-based models (recall: the tree-based classifiers of sklearn do not support categorical features!). Note that some preprocessing steps might be easier to perform on the pandas dataframe while others on the NumPy arrays. This is why we provide 2 code cells, but it's up to which one to use.\n",
    "\n",
    "**Note:** Perform only preprocessing steps that are indeed needed, and briefly(!) explain your decision by commenting your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da0cabed-075b-4289-a503-102b6b384a4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:37:03.358234800Z",
     "start_time": "2023-10-15T13:37:03.282207400Z"
    }
   },
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb2fbae2-44cb-44ee-9868-9978dcd155dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:37:08.942647100Z",
     "start_time": "2023-10-15T13:37:08.832532200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert dataframes to numpy arrays\n",
    "X, y = df_X.to_numpy(), df_y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dec6ae36-908c-46af-839e-39c0946f1d14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:37:09.578051900Z",
     "start_time": "2023-10-15T13:37:09.499474800Z"
    }
   },
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbf90b6-e701-4c0a-8acd-c0067c1a4ecc",
   "metadata": {},
   "source": [
    "### 3.2 Basic K-Fold Cross Validation\n",
    "\n",
    "The code cell below performs K-Fold Cross Validation. Note that we use `X` and `y` here, and assume our true test data for the final evaluation of the model(s) is a separate dataset. Since we only perform validation here, we can ignore the test data.\n",
    "\n",
    "The code cell below allows you to train a `DecisionTreeRegressor`, a `RandomForestRegressor`, or a `GradientBoostingRegressor` (all `sklearn` implementations). You only need to remove the comment before the regressor of choice, and comment the 2 other regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91afc3b3-73ea-4004-bbed-e40b73d1b01b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T13:37:12.201714Z",
     "start_time": "2023-10-15T13:37:11.444553700Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\paras\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\paras\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1247, in fit\n    super().fit(\n  File \"C:\\Users\\paras\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 186, in fit\n    X, y = self._validate_data(\n  File \"C:\\Users\\paras\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 560, in _validate_data\n    X = check_array(X, input_name=\"X\", **check_X_params)\n  File \"C:\\Users\\paras\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"C:\\Users\\paras\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'Developing'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[1;32m<timed exec>:15\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:285\u001B[0m, in \u001B[0;36mcross_validate\u001B[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001B[0m\n\u001B[0;32m    265\u001B[0m parallel \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39mn_jobs, verbose\u001B[38;5;241m=\u001B[39mverbose, pre_dispatch\u001B[38;5;241m=\u001B[39mpre_dispatch)\n\u001B[0;32m    266\u001B[0m results \u001B[38;5;241m=\u001B[39m parallel(\n\u001B[0;32m    267\u001B[0m     delayed(_fit_and_score)(\n\u001B[0;32m    268\u001B[0m         clone(estimator),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    282\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m train, test \u001B[38;5;129;01min\u001B[39;00m cv\u001B[38;5;241m.\u001B[39msplit(X, y, groups)\n\u001B[0;32m    283\u001B[0m )\n\u001B[1;32m--> 285\u001B[0m \u001B[43m_warn_or_raise_about_fit_failures\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresults\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror_score\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001B[39;00m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001B[39;00m\n\u001B[0;32m    289\u001B[0m \u001B[38;5;66;03m# the correct key.\u001B[39;00m\n\u001B[0;32m    290\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(scoring):\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001B[0m, in \u001B[0;36m_warn_or_raise_about_fit_failures\u001B[1;34m(results, error_score)\u001B[0m\n\u001B[0;32m    360\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_failed_fits \u001B[38;5;241m==\u001B[39m num_fits:\n\u001B[0;32m    361\u001B[0m     all_fits_failed_message \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    362\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mAll the \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m fits failed.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    363\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIt is very likely that your model is misconfigured.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    364\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou can try to debug the error by setting error_score=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    365\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBelow are more details about the failures:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfit_errors_summary\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    366\u001B[0m     )\n\u001B[1;32m--> 367\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(all_fits_failed_message)\n\u001B[0;32m    369\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    370\u001B[0m     some_fits_failed_message \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    371\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mnum_failed_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m fits failed out of a total of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    372\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe score on these train-test partitions for these parameters\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    376\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBelow are more details about the failures:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfit_errors_summary\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    377\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: \nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\paras\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\paras\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1247, in fit\n    super().fit(\n  File \"C:\\Users\\paras\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 186, in fit\n    X, y = self._validate_data(\n  File \"C:\\Users\\paras\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 560, in _validate_data\n    X = check_array(X, input_name=\"X\", **check_X_params)\n  File \"C:\\Users\\paras\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"C:\\Users\\paras\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'Developing'\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Only considered hyperparameter: max depth of trees\n",
    "param_choices = [1, 2, 3, 5, 8, 10, 12, 15, 20, 25, 50]\n",
    "\n",
    "# Keep track of results for visualization\n",
    "param_to_scores = {}\n",
    "\n",
    "for param in param_choices:\n",
    "\n",
    "    # Train regressor with the current parameter setting\n",
    "    regressor = DecisionTreeRegressor(max_depth=param)\n",
    "    #regressor = RandomForestRegressor(max_depth=param)\n",
    "    #regressor = GradientBoostingRegressor(max_depth=param)\n",
    "    \n",
    "    # Perform 10-fold cross_validations\n",
    "    scores = cross_validate(regressor, X, y, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "    \n",
    "    # Extract the 10 RSME scores (training scores and validation scores) for each run/fold\n",
    "    # The (-1) is only needed since we get the negative root mean squared errors (it's a sklearn thing)\n",
    "    rsme_train = scores['train_score'] * (-1)\n",
    "    rsme_valid = scores['test_score'] * (-1)\n",
    "    \n",
    "    ## Keep track of all num_folds f1 scores for current param (for plotting)\n",
    "    param_to_scores[param] = (rsme_train, rsme_valid)\n",
    "    \n",
    "    ## Print statement for some immediate feedback (values in parenthesis represent the Standard Deviation)\n",
    "    print('param = {}, RSME training = {:.1f} ({:.1f}), RSME validation = {:.1f} ({:.1f})'\n",
    "          .format(param, np.mean(rsme_train), np.std(rsme_train), np.mean(rsme_valid), np.std(rsme_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95be1c2-ea9f-4d24-8611-90e220798a37",
   "metadata": {},
   "source": [
    "**Visualization of Results.** We provide you with 2 methods to visualize the results:\n",
    "* `plot_validation_results()` shows all `num_folds` scores for each parameter setting together with the means and standard deviations of the validation scores.\n",
    "* `plot_scores()` shows the training and validation scores for each parameter setting.\n",
    "\n",
    "Just run the code cell below to plot both figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86510a8f-530d-4027-8507-8f2e11416909",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_validation_results(param_to_scores)\n",
    "\n",
    "plot_scores(param_to_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0fbca4-ff7a-475a-b104-0fc212e4a5b7",
   "metadata": {},
   "source": [
    "#### 3.2 a) Comparing Tree-Based Regression Models (5 Points)\n",
    "\n",
    "Run the k-fold cross validation for all 3 regressors and compare and discuss the results! You should see quite a number of differences regarding runtimes, issues of overfitting and underfitting, overall performance, effects of parameter values, etc. You can use the code cells above for cross validation and visualization.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b75e07-30d4-47cc-ae35-30e583a85658",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fe03c5b-dde9-4a18-8b6f-7de32d977d69",
   "metadata": {},
   "source": [
    "#### 3.2 b) Assessing the Evaluation (3 Points)\n",
    "\n",
    "Discuss if we found the regressor with the cross-validation result from above! There is no need to implement anything here.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dec8767-9a8d-4450-a2e1-cb006eb1c57b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e0f5c9-d95d-4cbd-a642-794ddbf64d30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
