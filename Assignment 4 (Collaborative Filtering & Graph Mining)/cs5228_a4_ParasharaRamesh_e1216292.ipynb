{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763f0fa0-6584-44d4-b163-a891252769f9",
   "metadata": {},
   "source": [
    "# CS5228 Assignment 4 - Recommender Systems & Graph Mining\n",
    "\n",
    "Hello everyone, this assignment notebook covers Recommender Systems & Graph Mining. There are some code-completion tasks and question-answering tasks in this answer sheet. For code completion tasks, please write down your answer (i.e., your lines of code) between sentences that \"Your code starts here\" and \"Your code ends here\". The space between these two lines does not reflect the required or expected lines of code. For answers in plain text, you can refer to [this Markdown guide](https://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd) to customize the layout (although it shouldn't be needed).\n",
    "\n",
    "When you work on this notebook, you can insert additional code cells (e.g., for testing) or markdown cells (e.g., to keep track of your thoughts). However, before the submission, please remove all those additional cells again. Thanks!\n",
    "\n",
    "**Important:**\n",
    "* Rename and save this Jupyter notebook as **cs5228_a4_YourName_YourNUSNETID.ipynb** (e.g., **cs5228_a4_BobSmith_e12345678.ipynb**) before submission!\n",
    "* Rename and save the script file *cs5228_a4.py* as **cs5228_a4_YourName_YourNUSNETID.py** (e.g., **cs5228_a4_BobSmith_e12345678.py**) before submission!\n",
    "* Submission deadline is Nov XX, 11.59 pm. Late submissions will be penalized by 10% for each additional day. Failure to appropriately rename both files will yield a penalty of 1 Point. There is no need to use your full name if it's rather long; it's just  important to easily identify you in Canvas etc.\n",
    "\n",
    "Please also add your NUSNET and student id in the code cell below. This is just to make any identification of your notebook doubly sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "119a3b10-7afe-4814-9a79-32310842f1c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:28:14.405112300Z",
     "start_time": "2023-11-09T04:28:14.321470600Z"
    }
   },
   "outputs": [],
   "source": [
    "student_id = 'A0285647M'\n",
    "nusnet_id = 'e1216292'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7455d1c-31ad-4620-8d8a-ad2adc363f56",
   "metadata": {},
   "source": [
    "Here is an overview over the tasks to be solved and the points associated with each task. The notebook can appear very long and verbose, but note that a lot of parts provide additional explanations, documentation, or some discussion. The code and markdown cells you are supposed to complete are well, but you can use the overview below to double-check that you covered everything.\n",
    "\n",
    "* **1 Recommender Systems (30 Points)**\n",
    "    * 1.1 Content-based (User-Item Similarities) (7 Points)\n",
    "        * 1.1 a) Calculate User Profile (5 Points)\n",
    "        * 1.1 b) Calculate User-Item Similarities (2 Points)\n",
    "    * 1.2 User-based Collaborative Filtering (7 Points)\n",
    "        * 1.2 a) Calculate User-User Similarities (5 Points)\n",
    "        * 1.2 b) Calculate Estimated Rating (2 Points)\n",
    "    * 1.3 Matrix Factorization (16 Points)\n",
    "        * 1.3 a) Implement Non-Negative Matrix Factorization (8 Points)\n",
    "        * 1.3 b) Hyperparameter Exploration (3 Points)\n",
    "        * 1.3 c) Matrix Factorization & Updates (5 Points)\n",
    "* **2 Graph Mining (20 Points)**\n",
    "    * 2.1 Implementing Closeness Centrality (4 Points)\n",
    "    * 2.2 Implementing PageRank Centrality (8 Points)\n",
    "    * 2.3 Comparing Centrality Measures (8 Points)\n",
    "        * 2.3 a) Run Off-The-Shelf Centrality Algorithms (3 Points)\n",
    "        * 2.3 b) Discussion of Results (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a4e6d6-30f8-4722-9750-4986c33fcccd",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Enable Auto-Reload\n",
    "\n",
    "This ensures that any saved changes to your `.py` file gets automatically reloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65641534-c0db-491c-8f9a-475ba94a5c6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:28:14.410916Z",
     "start_time": "2023-11-09T04:28:14.324976300Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad660cb-6a44-4906-8756-d18ec238bf09",
   "metadata": {},
   "source": [
    "### Enable \"Inline Plotting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8067a9c2-d8d6-4cb5-b994-07fdc2c51b17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:28:15.131327500Z",
     "start_time": "2023-11-09T04:28:14.356969200Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bdd634-a20d-491b-9339-49eab3ab7205",
   "metadata": {},
   "source": [
    "### Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ca2bb72-8aa3-4d53-9d2a-e4953aa02592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:28:16.048248800Z",
     "start_time": "2023-11-09T04:28:15.131327500Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from src.utils import plot_mrt_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befecf0d-9ea3-4355-abb3-6cc7b1daa7ec",
   "metadata": {},
   "source": [
    "**Important:** This notebook also requires you to complete in a separate `.py` script file. This keeps this notebook cleaner and simplifies testing your implementations for us. As you need to rename the file `cs5228_a4.py`, you also need to edit the import statement below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c0827e5-fcc6-42d4-93df-96258a07cec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:28:16.092698400Z",
     "start_time": "2023-11-09T04:28:16.048248800Z"
    }
   },
   "outputs": [],
   "source": [
    "from cs5228_a4_ParasharaRamesh_e1216292 import *\n",
    "#from cs5228_a4_BobSmith_e12345678 import get_noise_dbscan # <-- you will need to rename this accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ccaa8-0df8-4b2c-9676-0f872ec645d8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e65ca2e-36ac-4d5e-aa5d-b5c5e39ba1a4",
   "metadata": {},
   "source": [
    "## 1 Recommender Systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce33383-b661-4357-b37e-3a27e2aa3b06",
   "metadata": {},
   "source": [
    "### 1.1 Content-based (User-Item Similarities) (7 Points)\n",
    "\n",
    "The [Spotify Dataset 1921-2020](https://www.kaggle.com/yamaerenay/spotify-dataset-19212020-160k-tracks) contains over 175,000 songs with both [audio features](https://developer.spotify.com/documentation/web-api/reference/#endpoint-get-audio-features) and [track features](https://developer.spotify.com/documentation/web-api/reference/#endpoint-get-track). For this task, we look at 6 different songs an individual user $u$ has rated, and limit ourselves to 4 audio features, all ranging from 0 to 1. Note that this is somewhat different to the example from the lecture where we had only 0 or 1 as feature values -- recall that our features were only binary indicating whether a movie belonged to a certain genre. However, this does not change the calculation.\n",
    "\n",
    "**Important:** The ratings are not part of the dataset but manually added for this task. The range of the ratings is from 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a44e018-246e-426b-8461-8611d94e4112",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:28:16.163841300Z",
     "start_time": "2023-11-09T04:28:16.092698400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   acousticness  danceability  energy  liveness  rating\n0          0.93          0.32    0.14      0.18       9\n1          0.11          0.85    0.82      0.09       2\n2          0.75          0.36    0.39      0.12       7\n3          0.84          0.50    0.24      0.13       8\n4          0.88          0.33    0.18      0.09       8\n5          0.11          0.76    0.69      0.09       2",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>acousticness</th>\n      <th>danceability</th>\n      <th>energy</th>\n      <th>liveness</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.93</td>\n      <td>0.32</td>\n      <td>0.14</td>\n      <td>0.18</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.11</td>\n      <td>0.85</td>\n      <td>0.82</td>\n      <td>0.09</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.75</td>\n      <td>0.36</td>\n      <td>0.39</td>\n      <td>0.12</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.84</td>\n      <td>0.50</td>\n      <td>0.24</td>\n      <td>0.13</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.88</td>\n      <td>0.33</td>\n      <td>0.18</td>\n      <td>0.09</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.11</td>\n      <td>0.76</td>\n      <td>0.69</td>\n      <td>0.09</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/a4-spotify-sample.csv')\n",
    "\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87902a7b-b94b-4867-b14c-5c10aa280d3a",
   "metadata": {},
   "source": [
    "#### 1.1 a) Calculate User Profile (5 Points)\n",
    "\n",
    "Calculate the user profile vector $v_u$ based on $u$'s rating history! Please complete the equation by adding the profile vector in the markdown cell below; use a precision of 2 decimals for all vector values.\n",
    "\n",
    "**Important:** Show at least for one element in the profile vector how you calculated the value in detail. You can use an additional code or markdown cell.\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Find the mean of all the ratings\n",
    "$$\\mu_r = \\frac{\\Sigma_{i=1}^{6} r_i}{6}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg rating is 6.0\n"
     ]
    }
   ],
   "source": [
    "mu_rating = df[\"rating\"].mean()\n",
    "print(f\"avg rating is {mu_rating}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T05:03:30.508908500Z",
     "start_time": "2023-11-09T05:03:30.445985300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Subtract each rating from the mean of the ratings\n",
    "$$r'_i = r_i  - \\mu_r$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New rating values\n"
     ]
    },
    {
     "data": {
      "text/plain": "0    3.0\n1   -4.0\n2    1.0\n3    2.0\n4    2.0\nName: rating, dtype: float64"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"rating\"] = df[\"rating\"] - mu_rating\n",
    "print(\"New rating values\")\n",
    "df[\"rating\"].head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T05:04:24.827769800Z",
     "start_time": "2023-11-09T05:04:24.774143900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Find the new value for acousticness\n",
    "$$a' = \\frac{\\Sigma_{i=1}^{6} r'_i.a_i}{6}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user's acousticness value is 1.0166666666666666\n"
     ]
    }
   ],
   "source": [
    "df[\"acousticness\"] = df[\"acousticness\"] * df[\"rating\"]\n",
    "user_acousticness_val = df[\"acousticness\"].mean()\n",
    "print(f\"user's acousticness value is {user_acousticness_val}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T05:07:22.061310Z",
     "start_time": "2023-11-09T05:07:22.009305500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Find the new value for danceability\n",
    "$$d' = \\frac{\\Sigma_{i=1}^{6} r'_i.d_i}{6}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user's danceability value is -0.5766666666666667\n"
     ]
    }
   ],
   "source": [
    "df[\"danceability\"] = df[\"danceability\"] * df[\"rating\"]\n",
    "user_danceability_val = df[\"danceability\"].mean()\n",
    "print(f\"user's danceability value is {user_danceability_val}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T05:07:50.536214100Z",
     "start_time": "2023-11-09T05:07:50.482174600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Find the new value for energy\n",
    "$$e' = \\frac{\\Sigma_{i=1}^{6} r'_i.e_i}{6}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user's energy value is -0.7316666666666666\n"
     ]
    }
   ],
   "source": [
    "df[\"energy\"] = df[\"energy\"] * df[\"rating\"]\n",
    "user_energy_val = df[\"energy\"].mean()\n",
    "print(f\"user's energy value is {user_energy_val}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T05:08:11.663167600Z",
     "start_time": "2023-11-09T05:08:11.618621500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "00abe218-eff0-4314-9047-d0a37f847404",
   "metadata": {},
   "source": [
    "6. Find the new value for liveness\n",
    "$$l' = \\frac{\\Sigma_{i=1}^{6} r'_i.l_i}{6}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user's liveness value is 0.06333333333333334\n"
     ]
    }
   ],
   "source": [
    "df[\"liveness\"] = df[\"liveness\"] * df[\"rating\"]\n",
    "user_liveness_val = df[\"liveness\"].mean()\n",
    "print(f\"user's liveness value is {user_liveness_val}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T05:08:28.580372100Z",
     "start_time": "2023-11-09T05:08:28.528864900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "7. Final vector\n",
    "$$v_u = [a', d', e', l']$$\n",
    "$$v_u = [1.0166666666666666, -0.5766666666666667, -0.7316666666666666, 0.06333333333333334]$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "184f37f4-e917-43ec-811d-7e9ce0932515",
   "metadata": {},
   "source": [
    "#### 1.1 b) Calculate User-Item Similarities (2 Points)\n",
    "\n",
    "Calculate all cosine similarities between user $u$ and 2 new songs as defined by their feature values! Please complete the table and the statement below; use a precision of 2 decimals for the similarity values. Based on your results, which of the 2 songs should be recommended to the user?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f363aeae-bd91-40a9-b4a2-3a7a33d98707",
   "metadata": {},
   "source": [
    "|      | acousticness | danceability | energy | liveness | cosine similarity            |\n",
    "| ---  | ---          | ---          | ---    | ---      | ---                          |\n",
    "| A    | 0.24         | 0.72         | 0.43   | 0.02     | **???** |\n",
    "| B    | 0.79         | 0.32         | 0.12   | 0.09     | **???** |\n",
    "\n",
    "The song we should recommend to user $u$ is: **???**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f6ea0e-c1e7-4de9-a7b5-ec23d065c0b6",
   "metadata": {},
   "source": [
    "### 1.2 User-based Collaborative Filtering (7 Points)\n",
    "\n",
    "Given to you is a simple rating dataset containing 6 users $u_1, u_2, \\dots, u_6$, 8 songs $s_1, s_2, \\dots, s_8$, and the rating matrix $R$:\n",
    "\n",
    "$$\n",
    "R = \n",
    "\\begin{bmatrix} \n",
    "    4 & 0 & 0 & 3 & 5 & 0 & 1 & 4 \\\\\n",
    "    3 & 0 & 0 & 3 & 4 & 1 & 2 & 0 \\\\\n",
    "    1 & 0 & 0 & 2 & 0 & 5 & 4 & 2 \\\\\n",
    "    4 & 0 & 0 & 4 & 0 & 2 & 1 & 5 \\\\\n",
    "    3 & 3 & 0 & 2 & 4 & \\mathbf{\\color{red} ?}  & 1 & 2 \\\\\n",
    "    0 & 1 & 0 & 3 & 4 & 2 & 1 & 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In this example, the range of the ratings are from 1 to 5.\n",
    "\n",
    "Your overall task is to find the best estimate for rating $R_{u_5,s_6}$ of user $u_5$ for song $s_6$, indicated by the red question mark in rating matrix $R$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c62f72-16a3-4845-8bde-ebcd53068a9b",
   "metadata": {},
   "source": [
    "#### 1.2 a) Calculate User-User Similarities (5 Points)\n",
    "\n",
    "Calculate all cosine similarities between user $u_5$ and all other users! (5 Points)! Please complete the list of equations in the markdown below; please use a precision of 2 decimals.\n",
    "\n",
    "**Important:** Show at least for one equation how you calculate the similarity in detail. You can use an additional code or markdown cell.\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197a9e69-6310-48cb-8815-77276f47172a",
   "metadata": {},
   "source": [
    "* sim($u_5$, $u_2$) = **???**\n",
    "* sim($u_5$, $u_3$) = **???**\n",
    "* sim($u_5$, $u_4$) = **???**\n",
    "* sim($u_5$, $u_5$) = **???**\n",
    "* sim($u_5$, $u_6$) = **???**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf5a5e-ea62-4c57-acee-5b13af2a7048",
   "metadata": {},
   "source": [
    "#### Calculate Estimated Rating (2 Points)\n",
    "\n",
    "Calculate the estimated rating $R_{u_5,s_6}$! Consider the 2 most similar users for this calculation. Show how you arrived at this result! You can use an additional code or markdown cell.\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba2c5b4-9eaf-4d2d-8cd4-4166f153813d",
   "metadata": {},
   "source": [
    "* $R_{u_5,s_6}$ = **???**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f032ee14-481e-4cfa-95a9-c4b748c8d617",
   "metadata": {},
   "source": [
    "### 1.3 Matrix Factorization (16 Points)\n",
    "\n",
    "Matrix Factorization -- and here more specifically: non-negative Matrix Factorization -- is a class of algorithms where a matrix $M$ is factorized into (usually) two matrices $W$ and $H$, with the property that all three matrices have no negative elements. Matrix Factorization is popular techniques applied in recommender systems, where $W$ and $H$ contain a latent representation of all users and all items, respectively, and $M$ represents the rating matrix.\n",
    "\n",
    "In this task, you will implement (non-negative) Matrix Factorization from scratch using Gradient Descent as covered in the lecture. In fact, we use the rating matrix $M$ which was used as an example in the lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdca2b7e-dcd6-428b-a8d5-2a40881286d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4. 0. 0. 5. 1. 0. 0.]\n",
      " [5. 5. 4. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 2. 4. 5. 0.]\n",
      " [0. 3. 0. 0. 0. 0. 3.]]\n"
     ]
    }
   ],
   "source": [
    "M = np.array([\n",
    "    [4, 0, 0, 5, 1, 0, 0],\n",
    "    [5, 5, 4, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 2, 4, 5, 0],\n",
    "    [0, 3, 0, 0, 0, 0, 3]\n",
    "], dtype=np.float16)\n",
    "\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d88d1f-f2b7-4e5f-949d-cb7c4724292f",
   "metadata": {},
   "source": [
    "We provide you with the skeleton code for class `NMF` (short for Non-Negative Matrix Factorization). The code includes the initialization of matrices `W` and `H`, as well as of Matrix `Z`. Matrix `Z` is an auxiliary matrix containing the indices of all non-zero entries of Matrix `M`. Recall from the lecture that we need to compute the Gradient Descent based only on the non-zero entries in the rating matrix.\n",
    "\n",
    "The code cell below shows an example using the default parameter (`k=100`). The shapes of `W` and `H` reflect the number of users and items, as well as the size $k$ of the latent representations. The shape of `Z` is `(num_nonzero, 2)`. For example matrix `M`, the shape should be `(11, 2)` since `M` has 11 non-zero entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5ace682-51ed-4e17-92c0-544d07d3b38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W.shape = (4, 100)\n",
      "H.shape = (100, 7)\n",
      "Z.shape = (11, 2)\n",
      "\n",
      "Z containing all the indices of all non-zero entries in M (first 5 entries only)\n",
      "[[0 0]\n",
      " [0 3]\n",
      " [0 4]\n",
      " [1 0]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "nmf = NMF(M)\n",
    "\n",
    "print('W.shape = {}'.format(nmf.W.shape))\n",
    "print('H.shape = {}'.format(nmf.H.shape))\n",
    "print('Z.shape = {}'.format(nmf.Z.shape))\n",
    "print()\n",
    "print('Z containing all the indices of all non-zero entries in M (first 5 entries only)')\n",
    "print(nmf.Z[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70d7cfe-22e4-4439-9ce6-9c7d3635acd7",
   "metadata": {},
   "source": [
    "We also provide you with the method `calc_loss()` which calculates the loss w.r.t. the current values of matrices `W` and `H`. **Important:** Note that method implements the loss without regularization! Since we need this method only to print the loss and so to see its trend over time, this simplified calculation is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4160d0a6-a8e4-4007-b0b6-e68321288732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 4879.6\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "nmf = NMF(M)\n",
    "\n",
    "loss = nmf.calc_loss()\n",
    "\n",
    "print('Initial loss: {:.1f}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afae2f59-53b6-4f7e-9a03-ff9e1996fa44",
   "metadata": {},
   "source": [
    "#### 1.3 a) Implement Non-Negative Matrix Factorization (8 Points)\n",
    "\n",
    "Implement method `fit()` to perform matrix factorization using Gradient Descent! The complete algorithm together with the required gradients is available as pseudo code in the lecture slides, and you are already familiar with the basic concept of Gradient Descent. Here, consider the regularization terms when calculating the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c1ba6d2-9483-4f10-bdaa-79eb36663faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4638.72147 \t 0%\n",
      "Loss: 2911.73392 \t 10%\n",
      "Loss: 1931.49005 \t 20%\n",
      "Loss: 1330.81092 \t 30%\n",
      "Loss: 942.49529 \t 40%\n",
      "Loss: 681.48014 \t 50%\n",
      "Loss: 500.80400 \t 60%\n",
      "Loss: 372.86762 \t 70%\n",
      "Loss: 280.63586 \t 80%\n",
      "Loss: 213.17450 \t 90%\n",
      "Loss: 167.59880 \t 100%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "nmf = NMF(M)\n",
    "\n",
    "nmf.fit(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c90fbd3-e836-4bdf-9f9a-6c71600bc4b3",
   "metadata": {},
   "source": [
    "With the default values for all parameters  (`k=100`, `learning_rate=0.0001`, `lambda_reg=0.1`, `num_iter=100`), you should see a loss around **167.6** at the end of the training.\n",
    "\n",
    "**Important:** There are 2 different but equally fine solutions to implement Gradient Descent. Either\n",
    "* Calculate gradient with respect to w_u\n",
    "* Calculate gradient with respect to h_v\n",
    "* Update w_u\n",
    "* Update h_v\n",
    "\n",
    "or:\n",
    "\n",
    "* Calculate gradient with respect to w_u\n",
    "* Update w_u\n",
    "* Calculate gradient with respect to h_v\n",
    "* Update h_v\n",
    "\n",
    "Both solutions are correct, but you should appreciate the subtle difference. The reference solution follows the first approach, the algorithm on the lecture slides follows the second approach. So if you use the second approach, your output will be slighly different, the loss at the end won't be exactly 167.6 (but very similar)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea656c2-7f48-4deb-a0e5-3d08e70203c0",
   "metadata": {},
   "source": [
    "**Predicting unknown ratings (nothing for you to do here).** With our learned estimates for `W` and `H`, we can simply calculate matrix `P` as the product of `W` and `H`, representing the matrix of predicted ratings. We encapsulate this simple computation in method `predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab78c45-ce8e-480e-82ad-e8ae7488da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = nmf.predict()\n",
    "\n",
    "print(np.around(P, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10183f47-e076-42df-ab03-a0f1f93e2c2d",
   "metadata": {},
   "source": [
    "With the default values for all parameters  (`k=100`, `learning_rate=0.0001`, `lambda_reg=0.1`, `num_iter=100`), the result should look something like this:\n",
    "\n",
    "```\n",
    "[[ 7.02 10.17 11.97  7.85  5.61 10.61 12.52]\n",
    " [ 7.75  6.9   8.05 11.22  9.09 14.9  13.09]\n",
    " [ 9.65  8.96 10.37  7.02  6.81  8.33 10.76]\n",
    " [ 9.11  7.25 10.69 11.67  9.07 12.4   9.27]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca350f89-76f6-4fec-94f3-6c257f692afc",
   "metadata": {},
   "source": [
    "#### 1.3 b) Hyperparameter Exploration (3 Points)\n",
    "\n",
    "Explore different hyperparameter settings and briefly explain your observations! You can use the code cell below for that; you can simply set different values for `k`, `learning_rate`, `lambda_reg`, and `num_iter`.\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32207795-e0ad-4881-b051-6eceffedeecb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7013a58e-1d94-4a63-815a-5f771528db26",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "k, learning_rate, lambda_reg, num_iter = 100, 0.0001, 0.1, 100\n",
    "\n",
    "nmf = NMF(M, k=k)\n",
    "\n",
    "nmf.fit(learning_rate=learning_rate, lambda_reg=lambda_reg, num_iter=num_iter, verbose=True)\n",
    "\n",
    "P = nmf.predict()\n",
    "\n",
    "print('\\nReconstructed rating matrix:')\n",
    "print(np.around(P, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4374dd6-c24d-44b0-897d-0824f9a2781e",
   "metadata": {},
   "source": [
    "#### 1.3 c) Matrix Factorization & Updates (5 Points)\n",
    "\n",
    "You have now implemented a basic model-based recommender system using (non-negative) Matrix Factorization. Since we used only a toy rating matrix, performance was not an issue here. In real-world recommendations with many users and items, Matrix Factorization can be quite time consuming. The problem is that online platforms are very dynamic: users are joining and leaving, new items are added, users add new or update previous ratings. All of those cases change the rating matrix.\n",
    "\n",
    "**How do different cases (e.g., new user/item/rating) affect a current result of a Matrix Factorization for a recommender system? (3 Points)** Outline the different problems, and discuss meaningful approaches to mitigate them. For example, a new user or item refers to the *Cold-Start Problem*. What are good practical strategies to address the Cold-Start Problem and other changes to the rating matrix using Matrix Factorization?\n",
    "\n",
    "(Note: When you're discussing challenges regarding runtime/performance, please **exclude** any solutions relying on bigger clusters and parallel computing :). While those are valid points, in principle, here we want to focus on conceptual solutions).\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc088706-bde1-49d8-900b-307865647ad0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4e0fbf0-64cf-4238-8d47-39289e2a4d09",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e4110c-546f-4d15-8658-b0d3f1ddb7fe",
   "metadata": {},
   "source": [
    "## Graph Mining (20 Points)\n",
    "\n",
    "### Load and Prepare Data\n",
    "\n",
    "Throughout this section we work the MRT train network as our underlying graph. The MRT stations mark the nodes, and there is an edge (directed or undirected; see below) if there is a direct train connection between the respective MRT stations.\n",
    "\n",
    "**Load data from files.** We first load the information about the MRT stations. We only need this information to have access to the latitude and longitude of the stations, so we can plot the MRT graph and preserve the relative geographic locations of the MRT stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa810fa-f19d-4dcc-bb2f-535a46eda405",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrt_stations = pd.read_csv('data/a4-mrt-stations.csv')\n",
    "\n",
    "df_mrt_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3525a7d5-8b81-4589-b881-f8e75351ba3e",
   "metadata": {},
   "source": [
    "The following file contains the main information: Which MRT stations are directly connected with by a train. Note that the file contains each connection twice for both directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f56e12a-e1a7-4fa4-97dc-26a446263d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrt = pd.read_csv('data/a4-mrt-connections.csv')\n",
    "\n",
    "df_mrt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f67d28-8223-42e2-9b7d-f23e9add76b5",
   "metadata": {},
   "source": [
    "### Create Graphs\n",
    "\n",
    "From this data, we can easily create our NetworkX graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8becef58-46d8-4380-86b8-eb9a1669063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an \"empty\" directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "for idx, row in df_mrt.iterrows():\n",
    "    try:\n",
    "        df_mrt_stations.loc[df_mrt_stations.name.str.lower() == row['source']].iloc[0]\n",
    "        df_mrt_stations.loc[df_mrt_stations.name.str.lower() == row['destination']].iloc[0]\n",
    "        G.add_edge(row['source'], row['destination'])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04ada3d-f46e-4303-9bfe-e5ac8223bbec",
   "metadata": {},
   "source": [
    "We provide you with the method `plot_mrt_graph()` to visualize the train network. As mentioned before, we can utilize the information about the geo-coordinates of MRT stations to preserve their relative location. Of course the connections between the nodes / MRT stations are still just straight lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa46e78-0d1d-446f-af78-8d8dfbdcf096",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mrt_graph(G, df_mrt_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec3fe93-3ede-47d7-a7d0-7a652065f408",
   "metadata": {},
   "source": [
    "### 2.1 Implementing Closeness Centrality (4 Points)\n",
    "\n",
    "The Closeness Centrality of a node $v$ is defined as\n",
    "\n",
    "$$\n",
    "closeness(v) = \\frac{N}{\\sum_{w\\in V}d(v,w)}\n",
    "$$\n",
    "\n",
    "where $N$ is the number of nodes that can be reached from $v$, and $d(v,w)$ is the length of the shortest path between node $v$ and a node $w$.\n",
    "\n",
    "We saw that both distance-based centrality measure Closeness and Betweenness require the to solve the All-Pairs Shortest Paths (APSP) problem. Since this is not a \"programming\" or \"algorithms and data structures\" module, we don't expect you to come up with your own solution for the problem from scratch. For this task, you can utilize any method from [`nx.algorithms.shortest_paths`](https://networkx.org/documentation/stable/reference/algorithms/shortest_paths.html). Using a method to compute the shortest path between two nodes will make the computation of Closeness Centrality pretty straightforward.\n",
    "\n",
    "**Implement method `closeness()` to compute the Closeness Centrality of a Graph G.** You can assume the input Graph G being strongly connected, undirected, and unweighted.\n",
    "\n",
    "You can use the code cell below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a2efe7-8867-4588-865d-6458f3cca000",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_closeness_scores = closeness(G)\n",
    "\n",
    "for station, score in sorted(my_closeness_scores.items(), key=lambda kv: kv[1], reverse=True)[:5]:\n",
    "    print('{} ({:.5f})'.format(station, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac84638-c96f-46bf-ba1a-15d13e7437cd",
   "metadata": {},
   "source": [
    "**Compare your implementation with the one from NetworkX**. The code cell below computes the Closeness Centrality over the *undirected* MRT graph using the implementation from NetworkX, and again shows the 5 MRT stations with the highest scores. *Important:* The NetworkX implementation uses a slightly different definition of the Closeness Centrality. The exact values (typically from the 3rd decimal position onward) will differ a bit. However, the values should be very similar and the ranking of the top-5 MRT stations should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35769187-1912-4250-ad9e-128995b82ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_closeness_scores = nx.algorithms.centrality.closeness_centrality(G, wf_improved=False)\n",
    "\n",
    "for station, score in sorted(nx_closeness_scores.items(), key=lambda kv: kv[1], reverse=True)[:5]:\n",
    "    print('{} ({:.5f})'.format(station, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f263d87-54b9-4052-827e-cb613fe1fb25",
   "metadata": {},
   "source": [
    "### 2.2 Implementing PageRank Centrality (8 Points)\n",
    "\n",
    "In this task, you will implement the basic PageRank algorithm using the Power Iteration methods as introduced in the lecture.\n",
    "\n",
    "$$\n",
    "c_{PR} = \\alpha M c_{PR} + (1-\\alpha)E\n",
    "$$\n",
    "\n",
    "where $E = (1/n, 1/n, ..., 1/n)^T$ with $n$ being the number of nodes.\n",
    "\n",
    "Recall from the lecture that PageRank requires the **transition matrix** of a graph is input. For this, we provide you with the method `create_transition_matrix(A)` that converts the adjacency matrix of a Graph G into an transition matrix. Check out also the given code in method `pagerank()` where we use a numpy method to convert the Graph G to its adjacency matrix and then call `create_transition_matrix(A)`.\n",
    "\n",
    "**Implement method `pagerank()` to compute the PageRank Centrality of a Graph G**.  You can assume the input Graph G being strongly connected, directed, and unweighted.\n",
    "\n",
    "You can use the code cell below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e8814b-09d9-486b-9477-ab2e8fef0335",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pagerank_scores = pagerank(G)\n",
    "\n",
    "for station, score in sorted(my_pagerank_scores.items(), key=lambda kv: kv[1], reverse=True)[:5]:\n",
    "    print('{} ({:.5f})'.format(station, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a30bd2-0b19-49b9-b9d9-85730351563b",
   "metadata": {},
   "source": [
    "**Compare your implementation with the one from NetworkX**. The code cell below computes the PageRank Centrality over the *directed* MRT graph using the implementation from NetworkX, and again shows the 5 MRT stations with the highest scores. Apart from minor precision issues, the NetworkX result and your result should match, of course. Note that your implementation and the one of NetworkX are using the same default value for `alpha` and `eps` (called `tol` in case of NetworkX)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed90d12-aad2-4dfe-a7cc-9a8eeb6fefab",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_pagerank_scores = nx.pagerank(G)\n",
    "\n",
    "for station, score in sorted(nx_pagerank_scores.items(), key=lambda kv: kv[1], reverse=True)[:5]:\n",
    "    print('{} ({:.5f})'.format(station, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd463ce-f855-4c1e-b1de-b6ed8c1b5225",
   "metadata": {},
   "source": [
    "### 2.3 Comparing Centrality Measures (8 Points)\n",
    "\n",
    "We saw in the lecture that different centrality measures look at different topological features of a graph to quantify the importance of nodes. This task compares different measures, using the following implementations provided by `networkX`:\n",
    "\n",
    "* [nx.algorithms.link_analysis.pagerank](https://networkx.org/documentation/networkx-1.10/reference/generated/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html)\n",
    "* [nx.centrality.in_degree_centrality](https://networkx.org/documentation/networkx-1.10/reference/generated/networkx.algorithms.centrality.in_degree_centrality.html#networkx.algorithms.centrality.in_degree_centrality)\n",
    "* [nx.centrality.out_degree_centrality](https://networkx.org/documentation/networkx-1.10/reference/generated/networkx.algorithms.centrality.out_degree_centrality.html#networkx.algorithms.centrality.out_degree_centrality)\n",
    "* [nx.centrality.closeness_centrality](https://networkx.org/documentation/networkx-1.10/reference/generated/networkx.algorithms.centrality.closeness_centrality.html#networkx.algorithms.centrality.closeness_centrality)\n",
    "* [nx.centrality.betweenness_centrality](https://networkx.org/documentation/networkx-1.10/reference/generated/networkx.algorithms.centrality.betweenness_centrality.html)\n",
    "\n",
    "(**Note:** [nx.algorithms.link_analysis.pagerank](https://networkx.org/documentation/networkx-1.10/reference/generated/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html) might give a (slightly) different ranking than your own implementation of PageRank since this implementation is a bit modified. So don't take this as a 1:1 reference to check your PageRank implementation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a5f107-237f-48c0-8e59-a61db5d51354",
   "metadata": {},
   "source": [
    "#### 2.3 a) Run Off-The-Shelf Centrality Algorithms (3 Points)\n",
    "\n",
    "Run the 5 centrality measures on the MRT train network graph! Find the top-5 MRT stations with respect to their centrality scores and complete the table below. You only need to add the name of the MRT stations, not the scores. Keep in mind that different centrality measures assume a directed or undirected graph. While most algorithms will work on directed graphs, we consider the basic implementations covered in the lecture. In short, use `G_undirected` for Closeness and Betweenness.\n",
    "\n",
    "Use the code cell below to show how you get to the results; use the default values for all 5 implementations of the centrality measures.\n",
    "\n",
    "\n",
    "| Rank | PageRank | InDegree | OutDegree | Closeness | Betweenness |\n",
    "| ---  | ---      | ---      | ---       | ---         |  --- |\n",
    "| 1    | ??? | ??? | ??? | ??? | ??? |\n",
    "| 2    | ??? | ??? | ??? | ??? | ??? |\n",
    "| 3    | ??? | ??? | ??? | ??? | ??? |\n",
    "| 4    | ??? | ??? | ??? | ??? | ??? |\n",
    "| 5    | ??? | ??? | ??? | ??? | ??? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f8a789-d15c-49a1-8088-e0d90b23c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "\n",
    "\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0148221-6a5a-4b84-aebb-17ae5425c07c",
   "metadata": {},
   "source": [
    "#### 2.3 b) Discussion of Results (5 Points)\n",
    "\n",
    "Discuss the results and your observations! Based on the definitions and intuitions behind these 5 different centrality measures, discuss the results of 2.2 a): For each centrality, briefly describe what it means for a MRT station to have the highest score!\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944177f5-b9a1-4b00-9294-cbe0b772fae0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d95e73a-7d51-45e1-bca1-34b096f27496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
